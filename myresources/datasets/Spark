SPARK-12177,  KafkaDStreams to new Kafka 0 10 Consumer API,  0 9 already released and it introduce new consumer API that not compatible with old one  So  I added new consumer api  I made separate classes in package org apache spark streaming kafka v09 with changed API  I didn t remove old classes for more backward compatibility  User will not need to change his old spark applications when he uprgade to new Spark version  Please rewiew my changes,
SPARK-13671,  different physical plan for existing RDD and data sources,  now  we use PhysicalRDD for both existing RDD and data sources  they are becoming much different  we should use different physical plans for them ,
SPARK-9774,  Python API for ml regression IsotonicRegression,  Python API  user guide and example for ml regression IsotonicRegression,
SPARK-15887,  back the hive site xml support for Spark 2 0,  now  Spark 2 0 does not load hive site xml  Based on users  feedback  it seems make sense to still load this conf file  Originally  this file was loaded when we load HiveConf class and all settings can be retrieved after we create a HiveConf instances  Let s avoid of using this way to load hive site xml  Instead  since hive site xml is a normal hadoop conf file  we can first find its url using the classloader and then use Hadoop Configuration s addResource  or add hive site xml as a default resource through Configuration addDefaultResource  to load confs  Please note that hive site xml needs to be loaded into the hadoop conf used to create metadataHive ,
SPARK-7440,  physical Distinct operator in favor of Aggregate,  can just rewrite distinct using groupby  i e  aggregate operator  ,
SPARK-13805,  consume ColumnVector in generated code when ColumnarBatch is used,  generated code accesses a   ColumnarBatch   object  it is possible to get values of each column from   ColumnVector   instead of calling   getRow     ,
SPARK-13075,  database table system catalog,  of Spark 1 6  Spark SQL internally has only a limited catalog  and does not support any of the DDLs  This is an umbrella ticket to introduce an internal API for a system catalog  and the associated DDL implementations using this API   ,
SPARK-14831,  ML APIs in SparkR consistent,  current master  we have 4 ML methods in SparkR    code none  glm formula  family  data       kmeans data  centers       naiveBayes formula  data       survreg formula  data        code   We tried to keep the signatures similar to existing ones in R  However  if we put them together  they are not consistent  One example is k means  which doesn t accept a formula  Instead of looking at each method independently  we might want to update the signature of kmeans to   code none  kmeans formula  data  centers        code   We can also discuss possible global changes here  For example   glm  puts  family  before  data  while  kmeans  puts  centers  after  data   This is not consistent  And logically  the formula doesn t mean anything without associating with a DataFrame  So it makes more sense to me to have the following signature    code none  algorithm df  formula   required params    optional params    code   If we make this change  we might want to avoid name collisions because they have different signature  We can use  ml kmeans    ml glm   etc   Sorry for discussing API changes in the last minute  But I think it would be better to have consistent signatures in SparkR   cc    shivaram    josephkb    yanboliang ,
SPARK-14866,  SQLQuerySuite out into smaller test suites,,
SPARK-14776,  HiveSqlAstBuilder and SparkSqlAstBuilder,  Spark 2 0 we shouldn t have two parsers anymore  There should be only a single one  ,
SPARK-17661,Consolidate various listLeafFiles implementations,  are 4 listLeafFiles related functions in Spark     ListingFileCatalog listLeafFiles  which calls HadoopFsRelation listLeafFilesInParallel if the number of paths passed in is greater than a threshold  if it is lower  then it has its own serial version implemented    HadoopFsRelation listLeafFiles  called only by HadoopFsRelation listLeafFilesInParallel    HadoopFsRelation listLeafFilesInParallel  called only by ListingFileCatalog listLeafFiles   It is actually very confusing and error prone because there are effectively two distinct implementations for the serial version of listing leaf files  This code can be improved by     Move all file listing code into ListingFileCatalog  since it is the only class that needs this    Keep only one function for listing files in serial  ,
SPARK-14375,  test for spark ml KMeansSummary,  is no unit test for KMeansSummary in spark ml   Other items which could be fixed here    Add Since version to KMeansSummary class   Modify clusterSizes method to match GMM method  to be robust to empty clusters  in case we support that sometime    See PR for  SPARK 13538  ,
SPARK-1714,  advantage of AMRMClient APIs to simplify logic in YarnAllocationHandler,,
SPARK-14345,  deserializer expression resolution from ObjectOperator,,
SPARK-18850,  StreamExecution and progress classes serializable,  StreamExecution and progress classes serializable because it is too easy for it to get captured with normal usage ,
SPARK-11730,  Importance for GBT,  Forests have feature importance  but GBT do not  It would be great if we can add feature importance to GBT as well  Perhaps the code in Random Forests can be refactored to apply to both types of ensembles   See https   issues apache org jira browse SPARK 5133,
SPARK-18754,  recentProgresses to recentProgress,  informal poll of a bunch of users found this name to be more clear ,
SPARK-14042,  support for custom coalescers,  our discussion on the mailing list  please see  here http   mail archives apache org mod mbox  spark dev 201602 mbox  3CCA g63F7aVRBH WyyK3nvBSLCMPtSdUuL Ge9 WW4DnmnvY4SXg mail gmail com 3E   it would be nice to specify a custom coalescing policy as the current   coalesce     method only allows the user to specify the number of partitions and we cannot really control much  The need for this feature popped up when I wanted to merge small files by coalescing them by size ,
SPARK-9867,  utilities for binary data into ByteArray,  utilities such as Substring substringBinarySQL and BinaryPrefixComparator computePrefix for binary data are put together in ByteArray for easy to read ,
SPARK-11485,  DataFrameHolder and DatasetHolder public,  two classes should be public  since they are used in public code  ,
SPARK-12510,  ActorReceiver to support Java,  now the Java users cannot use ActorHelper because it uses special Scala syntax   This patch just refactored the codes to provide Java API and add an example ,
SPARK-12144, Support more external data source API in SparkR,  we support read df  write df  jsonFile  parquetFile in SQLContext  we should support more external data source API such as read json  read parquet  read orc  read jdbc  read csv and so on   Some of the exist API is deprecated and will remove at Spark 2 0  we should also deprecate them at SparkR  Note  we should refer the DataFrameReader and DataFrameWriter API at Spark SQL but defined with R like style   DataFrameReader API  http   spark apache org docs latest api scala index html org apache spark sql DataFrameReader DataFrameWriter API  http   spark apache org docs latest api scala index html org apache spark sql DataFrameWriter,
SPARK-9738,  FromUnsafe and add its codegen version to GenerateSafe,  https   github com apache spark pull 7752 we added  FromUnsafe  to convert nexted unsafe data like array map struct to safe versions  It s a quick solution and we already have  GenerateSafe  to do the conversion which is codegened  So we should remove  FromUnsafe  and implement its codegen version in  GenerateSafe  ,
SPARK-9698,  feature interaction as a transformer,  feature interaction as a transformer  which takes a list of vector double columns  and generate a single vector column that contains the interactions  multiplication  among them with proper handling of feature names ,
SPARK-11532,  implicit conversion from Expression to Column,,
SPARK-10682,  Bagel test suites,  has been deprecated and we haven t done any changes to it  There is no need to run those tests    ,
SPARK-12538,  table support,    nongli    please attach the design doc ,
SPARK-16640,  codegen for Elt function,  function doesn t support codegen execution  It is better to provide the support  ,
SPARK-13329,Considering output for statistics of logical plan,  current implementation of statistics of UnaryNode does not considering output  for example  Project   we should considering it to have a better guess ,
SPARK-15110,    Implement repartitionByColumn on DataFrame,  repartitionByColumn on DataFrame   This will allow us to run R functions on each partition identified by column groups with dapply   method ,
SPARK-13027,  API for updateStateByKey to provide batch time as input,  StateDStream currently does not provide the batch time as input to the state update function  This is required in cases where the behavior depends on the batch start time   We  Conviva  have been patching it manually for the past several Spark versions but we thought it might be useful for others as well ,
SPARK-13485, Dataset oriented  API evolution in Spark 2 0,  part of Spark 2 0  we want to create a stable API foundation for Dataset to become the main user facing API in Spark  This ticket tracks various tasks related to that   The main high level changes are   1  Merge Dataset DataFrame 2  Create a more natural entry point for Dataset  SQLContext HiveContext are not ideal because of the name  SQL   Hive   and  SparkContext  is not ideal because of its heavy dependency on RDDs  3  First class support for sessions 4  First class support for some system catalog   See the design doc for more details   ,
SPARK-14709,  ml API for linear SVM,  API for SVM algorithm for DataFrames   I would recommend using OWL QN  rather than wrapping spark mllib s SGD based implementation   The API should mimic existing spark ml classification APIs ,
SPARK-13930,  fast serialization on collect limit,  the fast serialization has been introduced to collecting DataFrame Dataset  The same technology can be used on collect limit operator too ,
SPARK-14334,  toLocalIterator for Dataset,  toLocalIterator of RDD is super slow  we should have a optimized implementation for Dataset DataFrame ,
SPARK-9674,  GeneratedAggregate,  is subsumed by the new aggregate implementation  ,
SPARK-17428,  executors workers support virtualenv,  users have requirements to use third party R packages in executors workers  but SparkR can not satisfy this requirements elegantly  For example  you should to mess with the IT administrators of the cluster to deploy these R packages on each executors workers node which is very inflexible   I think we should support third party R packages for SparkR users as what we do for jar packages in the following two scenarios  1  Users can install R packages from CRAN or custom CRAN like repository for each executors  2  Users can load their local R packages and install them on each executors   To achieve this goal  the first thing is to make SparkR executors support virtualenv like Python conda  I have investigated and found packrat http   rstudio github io packrat   is one of the candidates to support virtualenv for R  Packrat is a dependency management system for R and can isolate the dependent R packages in its own private package space  Then SparkR users can install third party packages in the application scope destroy after the application exit  and don t need to bother IT administrators to install these packages manually   I would like to know whether it make sense ,
SPARK-16858,  of TestHiveSharedState,  TestHiveSharedState  Otherwise  we are not really testing the reflection logic based on the setting of we are not really testing the reflection logic based on the setting of CATALOG IMPLEMENTATION ,
SPARK-11410,  a DataFrame API that provides functionality similar to HiveQL s DISTRIBUTE BY,  BY allows the user to control the partitioning and ordering of a data set which can be very useful for some applications ,
SPARK-13598,  LeftSemiJoinBNL,  left semi join without joining keys is already supported in BroadcastNestedLoopJoin  it has the same implementation as LeftSemiJoinBNL  we should remove that ,
SPARK-15362,  spark ml KMeansModel load backwards compatible,  14646 makes KMeansModel store the clusters one per row  KMeansModel load   method needs to be updated in order to load models saved with Spark 1 6 ,
SPARK-19029,  databaseName  from SimpleCatalogRelation,  useless  databaseName   from  SimpleCatalogRelation    ,
SPARK-9767,  ConnectionManager,  introduced the Netty network module for shuffle in Spark 1 2  and has turned it on by default for 3 releases  The old ConnectionManager is difficult to maintain  It s time to remove it ,
SPARK-12992,  parquet decoding using ColumnarBatch,  files benefit from vectorized decoding  ColumnarBatches have been designed to support this  This means that a single encoded parquet column is decoded to a single ColumnVector  ,
SPARK-9769,  Python API for ml feature CountVectorizer,  Python API  user guide and example for ml feature CountVectorizerModel,
SPARK-16074,  VectorUDT MatrixUDT in a public API,  VectorUDT and MatrixUDT are private APIs  because UserDefinedType itself is private in Spark  However  in order to let developers implement their own transformers and estimators  we should expose both types in a public API to simply the implementation of transformSchema  transform  etc  Otherwise  they need to get the data types using reflection   Note that this doesn t mean to expose VectorUDT MatrixUDT classes  We can just have a method or a static value that returns VectorUDT MatrixUDT instance with DataType as the return type  There are two ways to implement this  1  following DataTypes java in SQL  so Java users doesn t need the extra       2  Define DataTypes in Scala ,
SPARK-11256,  all Stage ResultStage ShuffleMapStage internal state as private ,,
SPARK-12537,  option to accept quoting of all character backslash quoting mechanism,  can provides the option to choose JSON parser can be enabled to accept quoting of all character or not   For example  if JSON file that includes not listed by JSON backslash quoting specification  it returns corrupt record   code title JSON File borderStyle solid    name    Cazen Lee    price     10     name    John Doe    price      20     name    Tracy    price     10    code   corrupt record returns null   code  scala  df show                                               corrupt record      name price                                                          null Cazen Lee    10     name    John Do         null  null                   null     Tracy    10                                          code   And after apply this patch  we can enable allowBackslashEscapingAnyCharacter option like below   code  scala  val df   sqlContext read option  allowBackslashEscapingAnyCharacter    true   json   user Cazen test test2 txt   df  org apache spark sql DataFrame    name  string  price  string   scala  df show                         name price                     Cazen Lee    10    John Doe    20       Tracy    10                     code   This issue similar to HIVE 11825  HIVE 12717  ,
SPARK-9667,  SparkSqlSerializer2 in favor of Unsafe exchange,GenerateUnsafeProjection can be used directly as a code generated serializer  We no longer need SparkSqlSerializer2   ,
SPARK-12333,  shuffle spill encryption in Spark,  shuffle file encryption in SPARK 5682  spills data should also be encrypted ,
SPARK-16405,  metrics and source for external shuffle service,ExternalShuffleService is essential for spark  In order to better monitor shuffle service  we added various metrics in shuffle service and  ExternalShuffleServiceSource for metric system ,
SPARK-14978,  TrainValidationSplitModel should support validationMetrics,validationMetrics in TrainValidationSplitModel should also be supported in pyspark ml tuning,
SPARK-11194,  a single URLClassLoader for jars added through SQL s  ADD JAR  command,  now  we stack a new URLClassLoader when a user add a jar through SQL s add jar command  This approach can introduce issues caused by the ordering of added jars when a class of a jar depends on another class of another jar   For example   code  ClassLoader1 for Jar1 jar  A class                 ClassLoader2 for Jar2 jar  B class depending on A class   code  In this case  when we lookup class B  we will not be able to find class A because Jar2 is the parent of Jar1 ,
SPARK-14577,  sql codegen maxCaseBranches config option,  currently disable codegen for CaseWhen if the number of branches is greater than 20  in CaseWhen MAX NUM CASES FOR CODEGEN   It would be better if this value is a non public config defined in SQLConf ,
SPARK-13011,  means wrapper in SparkR,  a simple wrapper in SparkR to support k means ,
SPARK-19290,  a new extending interface in Analyzer for post hoc resolution,  implement DDL commands  we added several analyzer rules in sql hive module to analyze DDL related plans  However  our Analyzer currently only have one extending interface  extendedResolutionRules  which defines extra rules that will be run together with other rules in the resolution batch  and doesn t fit DDL rules well  because   1  DDL rules may do some checking and normalization  but we may do it many times as the resolution batch will run rules again and again  until fixed point  and it s hard to tell if a DDL rule has already done its checking and normalization  It s fine because DDL rules are idempotent  but it s bad for analysis performance 2  some DDL rules may depend on others  and it s pretty hard to write if conditions to guarantee the dependencies  It will be good if we have a batch which run rules in one pass  so that we can guarantee the dependencies by rules order ,
SPARK-10459,  could process UnsafeRow,   There will be ConvertToSafe for PythonUDF  that s not needed actually ,
SPARK-1707,  3 second sleep before starting app on YARN,  acquiring allocations from YARN and launching containers  Spark currently waits for 3 seconds for executors to connect to the driver   On Spark standalone  nothing like this happens   I m wondering whether we can just remove this sleep entirely   Is there a reason I m missing why YARN is different than standalone in this regard   At the least we could do something smarter like wait until all executors have registered ,
SPARK-12757,  reference counting to prevent blocks from being evicted during reads,  a pre requisite to off heap caching of blocks  we need a mechanism to prevent pages   blocks from being evicted while they are being read  With on heap objects  evicting a block while it is being read merely leads to memory accounting problems  because we assume that an evicted block is a candidate for garbage collection  which will not be true during a read   but with off heap memory this will lead to either data corruption or segmentation faults   To address this  we should add a reference counting mechanism to track which blocks pages are being read in order to prevent them from being evicted prematurely  I propose to do this in two phases  first  add a safe  conservative approach in which all BlockManager get    calls implicitly increment the reference count of blocks and where tasks  references are automatically freed upon task completion  This will be correct but may have adverse performance impacts because it will prevent legitimate block evictions  In phase two  we should incrementally add release   calls in order to fix the eviction of unreferenced blocks  The latter change may need to touch many different components  which is why I propose to do it separately in order to make the changes easier to reason about and review ,
SPARK-15426,  2 0 SQL API audit,  is an umbrella ticket to list issues I found with APIs for the 2 0 release  ,
SPARK-12542,  intersect except in Hive SQL,,
SPARK-12735,  spark ec2 scripts to AMPLab,  would be easier to fix bugs and maintain the ec2 script separately from Spark releases   For more information  see https   issues apache org jira browse SPARK 9562  ,
SPARK-9736,  anyNull should delegate to the underlying rows,  anyNull currently loops through every field to check for null  which is inefficient if the underlying rows are UnsafeRows  It should just delegate to the underlying implementation  ,
SPARK-17490,  SerializeFromObject for primitive array,  logical plan    SerializeFromObject   for an array always use   GenericArrayData   as a destination    UnsafeArrayData   could be used for an primitive array  This is a simple approach to solve issues that are addressed by SPARK 16043   Here is a motivating example    code  sparkContext parallelize Seq Array 1    1  toDS map e    e  show  code ,
SPARK-8745,  GenerateProjection,  on discussion offline with   marmbrus   we should remove GenerateProjection ,
SPARK-13526,   Move SQLContext HiveContext per session state to separate class,  is just a clean up task  Today there are all these fields in SQLContext that are not organized in any particular way  However  since each SQLContext is a session  many of these fields are actually isolated per session  To minimize the size of these context files and provide a logical grouping that makes more sense  I propose that we move these fields into its own class  called SessionState ,
SPARK-13348,  duplicated broadcasts,  broadcasted table could be used multiple times in a query  we should cache them ,
SPARK-7887,  EvaluatedType from SQL Expression,  s not a very useful type to use  We can just remove it to simplify expressions slightly  ,
SPARK-19336,  Python API,  a Python wrapper for spark ml classification LinearSVC,
SPARK-18590,    Include package vignettes and help pages  build source package in Spark distribution,  should include in Spark distribution the built source package for SparkR  This will enable help and vignettes when the package is used  Also this source package is what we would release to CRAN  ,
SPARK-18216,  Column expr public,  expr is private sql   but it s an actually really useful field to have for debugging  We should open it up  similar to how we use QueryExecution  ,
SPARK-9903,  local processing in PrefixSpan if there are no small prefixes,  exists a chance that the prefixes keep growing to the maximum pattern length  Then the final local processing step becomes unnecessary ,
SPARK-17150,  SQL generation for inline tables,  tables currently do not support SQL generation  and as a result a view that depends on inline tables would fail ,
SPARK-8876,  InternalRow type alias in expressions package,  type alias was there because initially when I moved Row around  I didn t want to do massive changes to the expression code  But now it should be pretty easy to just remove it  One less concept to worry about  ,
SPARK-12585,  numFields of UnsafeRow should not changed by pointTo  ,  now  numFields will be passed in by pointTo    then bitSetWidthInBytes is calculated  making pointTo   a little bit heavy   It should be part of constructor of UnsafeRow ,
SPARK-9613,  use of JavaConversions and migrate all existing uses to JavaConverters,  s style checker should ban the use of Scala s JavaConversions  which provides implicit conversions between Java and Scala collections types   Instead  we should be performing these conversions explicitly using JavaConverters  or forgoing the conversions altogether if they re occurring inside of performance critical code  ,
SPARK-12645,  support hash function ,  hash function for SparkR,
SPARK-14157,  Drop Function DDL command,  only parse create function command  In order to support native drop function command  we need to parse it too ,
SPARK-11513,  the internal implicit conversion from LogicalPlan to DataFrame,  has an internal implicit conversion that turns a LogicalPlan into a DataFrame  This has been fairly confusing to a few new contributors  Since it doesn t buy us much  we should just remove that implicit conversion  ,
SPARK-15721,  DefaultParamsReadable Writable public APIs,  DefaultParamsWritable and DefaultParamsReadable public will help users who have their own transformers save and load Pipelines   Making them public should be safe  even if we change internal formats ,
SPARK-8154,  Term Code type aliases in code generation,  my perspective as a code reviewer  I find them more confusing than using String directly  ,
SPARK-15416,  a better message for not finding classes removed in Spark 2 0,  removed some classes in Spark 2 0  If the user uses an incompatible library  he may see ClassNotFoundException  It s better to give an instruction to ask people using a correct version ,
SPARK-4444,  VD type parameter from EdgeRDD,  to vertex attribute caching  EdgeRDD previously took two type parameters  ED and VD  However  this is an implementation detail that should not be exposed in the interface  so this PR drops the VD type parameter   This requires removing the filter method from the EdgeRDD interface  because it depends on vertex attribute caching ,
SPARK-8596,  and configure RStudio server on Spark EC2,  will make it convenient for R users to use SparkR from their browsers ,
SPARK-11042,  a mechanism to ban creating new root SQLContexts in a JVM,  some use cases  it will be useful to explicitly ban creating multiple root SQLContexts HiveContexts  At here root SQLContext means the first SQLContext that gets created  ,
SPARK-16270,  xpath user defined functions,  SQL currently falls back to Hive for xpath related functions ,
SPARK-11113,  DeveloperApi annotation from private classes,  a variety of reasons  we tagged a bunch of internal classes in the execution package in SQL as DeveloperApi  ,
SPARK-14939,  FoldablePropagation optimizer,  issues aims to add new FoldablePropagation optimizer that propagates foldable expressions by replacing all attributes with the aliases of original foldable expression  Other optimizations will take advantage of the propagated foldable expressions  e g  EliminateSorts optimizer now can handle the following Case 2 and 3   Case 1 is the previous implementation    1  Literals and foldable expression  e g   ORDER BY 1 0   abc   Now    2  Foldable ordinals  e g   SELECT 1 0   abc   Now   ORDER BY 1  2  3  3  Foldable aliases  e g   SELECT 1 0 x   abc  y  Now   z ORDER BY x  y  z     Before    code  scala  sql  SELECT 1 0  Now   x ORDER BY 1  x   explain    Physical Plan    WholeStageCodegen       Sort  1 0 5 ASC x 0 ASC   true  0          INPUT    Exchange rangepartitioning 1 0 5 ASC  x 0 ASC  200   None       WholeStageCodegen             Project  1 0 AS 1 0 5 1461873043577000 AS x 0                 INPUT          Scan OneRowRelation    code     After    code  scala  sql  SELECT 1 0  Now   x ORDER BY 1  x   explain    Physical Plan    WholeStageCodegen       Project  1 0 AS 1 0 5 1461873079484000 AS x 0           INPUT    Scan OneRowRelation    code ,
SPARK-13168,  adjacent Repartition operations,  SQL should collapse adjacent   Repartition   operators and only keep the last one ,
SPARK-13694,  expressions should always include all expressions,,
SPARK-13890,  some internal classes  dependency on SQLContext,  general it is better for internal classes to not depend on the external class  in this case SQLContext  to reduce coupling between user facing APIs and the internal implementations   ,
SPARK-16051,   read orc write orc  to SparkR,  issue adds  read orc write orc  to SparkR for API parity ,
SPARK-12476,  JdbcRelation unhandledFilters for removing unnecessary Spark Filter,   SELECT   FROM jdbcTable WHERE col0    xxx   Current plan   code     Optimized Logical Plan    Project  col0 0 col1 1     Filter  col0 0   xxx        Relation col0 0 col1 1  JDBCRelation jdbc postgresql postgres testRel  Lorg apache spark Partition  2ac7c683  user maropu  password   driver org postgresql Driver       Physical Plan       Filter  col0 0   xxx        Scan JDBCRelation jdbc postgresql postgres testRel  Lorg apache spark Partition  2ac7c683  user maropu  password   driver org postgresql Driver   col0 0 col1 1  PushedFilters   EqualTo col0 xxx    code   This patch enables a plan below   code     Optimized Logical Plan    Project  col0 0 col1 1     Filter  col0 0   xxx        Relation col0 0 col1 1  JDBCRelation jdbc postgresql postgres testRel  Lorg apache spark Partition  2ac7c683  user maropu  password   driver org postgresql Driver       Physical Plan    Scan JDBCRelation jdbc postgresql postgres testRel  Lorg apache spark Partition  2ac7c683  user maropu  password   driver org postgresql Driver   col0 0 col1 1  PushedFilters   EqualTo col0 xxx    code   ,
SPARK-14039,  SubqueryHolder an inner class,,
SPARK-17186,  catalog table type INDEX,,
SPARK-13597,  API for GeneralizedLinearRegression,  SPARK 12811  we should add Python API for generalized linear regression ,
SPARK-12463,  spark deploy mesos recoveryMode and use spark deploy recoveryMode,  spark deploy mesos recoveryMode and use spark deploy recoveryMode configuration for cluster mode ,
SPARK-15181,  API for Generalized Linear Regression Summary,  should add an interface to the GLR summaries in Python for feature parity ,
SPARK-11774,   struct    encode   decode  in SparkR,   struct    encode   decode  in SparkR as documented at http   spark apache org docs latest api scala index html org apache spark sql functions ,
SPARK-14435,  Kryo in our custom Hive 1 2 1 fork,  order to upgrade to Kryo 3  we need to shade Kryo in our custom Hive 1 2 1 fork ,
SPARK-18834,  event time time stats through StreamingQueryProgress,,
SPARK-10104,Consolidate different forms of table identifiers,  now  we have QualifiedTableName  TableIdentifier  and Seq String  to represent table identifiers  We should only have one form and looks TableIdentifier is the best one because it provides methods to get table name  database name  return unquoted string  and return quoted string    There will be TODOs having  SPARK 10104  in it  Those places need to be updated ,
SPARK-15827,  Spark s forked sbt pom reader to Maven Central,  s SBT build currently uses a fork of the sbt pom reader plugin but depends on that fork via a SBT subproject which is cloned from https   github com scrapcodes sbt pom reader tree ignore artifact id  This unnecessarily slows down the initial build on fresh machines and is also risky because it risks a build breakage in case that GitHub repository ever changes or is deleted   In order to address these issues  I propose to publish a pre built binary of our forked sbt pom reader plugin to Maven Central under the org spark project namespace ,
SPARK-17420,  rmarkdown R package on Jenkins machines,  building SparkR vignettes on Jenkins machines  we need the rmarkdown R   The package is available at  https   cran r project org web packages rmarkdown index html   I think running something like Rscript  e  install packages  rmarkdown   repos  http   cran stat ucla edu      should work,
SPARK-13732,  projectList from Windows,projectList is useless  Remove it from the class Window  It simplifies the codes in Analyzer and Optimizer  ,
SPARK-10373,   since annotator to pyspark to be shared by all components,  s   since  is defined under  pyspark sql   It would be nice to move it under  pyspark  to be shared by all components ,
SPARK-4447,  layers of abstraction in YARN code no longer needed after dropping yarn alpha,  example  YarnRMClient and YarnRMClientImpl can be merged YarnAllocator and YarnAllocationHandler can be merged,
SPARK-9703,EnsureRequirements should not add unnecessary shuffles when only ordering requirements are unsatisfied,  SortMergeJoin  which requires a sorted  clustered distribution of its input rows  Say that both of SMJ s children produce unsorted output but are both single partition  In this case  we will need to inject sort operators but should not need to inject exchanges  Unfortunately  it looks like the Exchange unnecessarily repartitions using a hash partitioning   We should update Exchange so that it does not unnecessarily repartition children when only the ordering requirements are unsatisfied   I d like to fix this for Spark 1 5 since it makes certain types of unit tests easier to write ,
SPARK-15908,  varargs type dropDuplicates   function in SparkR,  is for API parity of Scala API  Refer to https   issues apache org jira browse SPARK 15807,
SPARK-14825,  functionality in Hive module into SQL core module,  is an umbrella ticket to reduce the difference between sql core and sql hive  Ultimately the only difference should just be the ability to run Hive serdes as well as UDFs  ,
SPARK-12115,  numPartitions   in RDD to be  getNumPartitions  to be consistent with pyspark scala,,
SPARK-9489,  compatibleWith  meetsRequirements  and needsAnySort checks from Exchange,  reviewing   yhuai  s patch for SPARK 2205  I noticed that Exchange s   compatible   check may be incorrectly returning   false   in many cases   As far as I know  this is not actually a problem because the   compatible      meetsRequirements    and   needsAnySort   checks are serving only as short circuit performance optimizations that are not necessary for correctness   In order to reduce code complexity  I think that we should remove these checks and unconditionally rewrite the operator s children   This should be safe because we rewrite the tree in a single bottom up pass ,
SPARK-13139,  native DDL commands,  currently delegate most DDLs directly to Hive  through NativePlaceholder in HiveQl scala  In Spark 2 0  we want to provide native implementations for DDLs for both SQLContext and HiveContext   The first step is to properly parse these DDLs  and then create logical commands that encapsulate them  The actual implementation can still delegate to HiveNativeCommand  As an example  we should define a command for RenameTable with the proper fields  and just delegate the implementation to HiveNativeCommand  we might need to track the original sql query in order to run HiveNativeCommand  but we can remove the sql query in the future once we do the next step    Once we flush out the internal persistent catalog API  we can then switch the implementation of these newly added commands to use the catalog API  ,
SPARK-14205,  trait Queryable,,
SPARK-15457,  MLlib 2 0 build warnings from deprecations,  classes and methods have been deprecated and are creating lots of build warnings in branch 2 0   This issue is to identify and fix those items     WithSGD classes  Change to make class not deprecated  object deprecated  and public class constructor deprecated   Any public use will require a deprecated API   We need to keep a non deprecated private API since we cannot eliminate certain uses  Python API  streaming algs  and examples     Use in PythonMLlibAPI  Change to using private constructors    Streaming algs  No warnings after we un deprecate the classes    Examples  Deprecate or change ones which use deprecated APIs   MulticlassMetrics fields  precision  etc   ,
SPARK-14541,  function  IFNULL  NULLIF  NVL and NVL2,  will be great to have these SQL functions   IFNULL  NULLIF  NVL  NVL2  The meaning of these functions could be found in oracle docs ,
SPARK-13882,  org apache spark sql execution local,  introduced some local operators in org apache spark sql execution local package but never fully wired the engine to actually use these  We still plan to implement a full local mode  but it s probably going to be fairly different from what the current iterator based local mode would look like   Let s just remove them for now  and we can always re introduced them in the future by looking at branch 1 6  ,
SPARK-17844,  API should simplify defining frame boundaries without partitioning ordering,  I was creating the example code for SPARK 10496  I realized it was pretty convoluted to define the frame boundaries for window functions when there is no partition column or ordering column  The reason is that we don t provide a way to create a WindowSpec directly with the frame boundaries  We can trivially improve this by adding rowsBetween and rangeBetween to Window object  ,
SPARK-13884,  DescribeCommand s dependency on LogicalPlan,DescribeCommand should just take a TableIdentifier  and ask the metadata catalog for table s information ,
SPARK-18826,  FileStream be able to start with most recent files,  starting a stream with a lot of backfill and maxFilesPerTrigger  the user could often want to start with most recent files first  This would let you keep low latency for recent data and slowly backfill historical data   It s better to add an option to control this behavior ,
SPARK-15413,   toBreeze  to  asBreeze  in Vector and Matrix,  re using  asML  to convert the mllib vector matrix to ml vector matrix now  Using  as  is more correct given that this conversion actually shares the same underline data structure  As a result  in this PR   toBreeze  will be changed to  asBreeze   This is a private API  as a result  it will not affect any user s application  ,
SPARK-16732,  unused codes in subexpressionEliminationForWholeStageCodegen,  codes in subexpressionEliminationForWholeStageCodegen are never used actually  Remove them using this jira ,
SPARK-18537,  a REST api to spark streaming,  to monitoring our streaming application using Spark REST interface and found out that there is no api for streaming  it let us no choice but to implement one for ourself   this api should cover exceptly the same amount of information as you can get from the web interface the implementation is base on the current REST implementation of spark core and will be available for running applications only  here is how you can use it   endpoint root   streaming api v1    Endpoint    Meaning      statistics Statistics information of stream    receivers A list of all receiver streams    receivers   stream id   Details of the given receiver stream    batches A list of all retained batches    batches   batch id   Details of the given batch    batches   batch id   operations A list of all output operations of the given batch    batches   batch id   operations   operation id   Details of the given operation  given batch ,
SPARK-9754,  TypeCheck in debug package,  no longer applies in the new Tungsten world  ,
SPARK-6956,  DataFrame API compatibility with Pandas,  is not always possible  but whenever possible we should remove or reduce the differences between Pandas and Spark DataFrames in Python  ,
SPARK-11015,  computeCost and clusterCenters to KMeansModel in spark ml package,  Transformer version of KMeansModel does not currently have methods to computeCost or get the centers of the cluster centroids  If there could be a way to get this either by exposing the parentModel or by adding these method it would make things easier ,
SPARK-7693,   import scala concurrent ExecutionContext Implicits global ,  a lesson from SPARK 7655  Spark should avoid to use  scala concurrent ExecutionContext Implicits global  because the user may submit blocking actions to  scala concurrent ExecutionContext Implicits global  and exhaust all threads in it  This could crash Spark   So Spark should always use its own thread pools for safety ,
SPARK-9693,  a page in all unsafe operators to avoid starving an operator,  g  currently we can do up to 3 sorts within a task    1  During the aggregation  2  During a sort on the same key  3  During the shuffle  In environments with tight memory restrictions  the first operator may acquire so much memory such that the subsequent ones in the same task are starved  A simple fix is to reserve at least a page in advance in each of these places  The reserved page size need not be the same as the normal page size   This is a sister problem to SPARK 4452 in Spark Core ,
SPARK-18361,  RDD localCheckpoint in PySpark,  of today  I could not access rdd localCheckpoint   in pyspark   This is an important issue for machine learning people  as we often have to iterate algorithms and perform operations like joins in each iteration    If the lineage is not truncated  the memory usage  the lineage  and computation time explode  rdd localCheckpoint    seems like the most straightforward way of truncating the lineage  but the python API does not expose it ,
SPARK-8948,  ExtractValueWithOrdinal abstract class,  is unnecessary and makes the type hierarchy slightly more complicated than needed ,
SPARK-10714,  PythonRDD to decouple iterator computation from PythonRDD,  idea is that most of the logic of calling Python actually has nothing to do with RDD  it is really just communicating with a socket    there is nothing distributed about it   and it is only currently depending on RDD because it was written this way   If we extract that functionality out  we can apply it to area of the code that doesn t depend on RDDs  and also make it easier to test  ,
SPARK-12608,  submitJobThreadPool since submitJob doesn t create a separate thread to wait for the job result,    9264 https   github com apache spark pull 9264   submitJob would create a separate thread to wait for the job result   submitJobThreadPool  was a workaround in  ReceiverTracker  to run these waiting job result threads  Now   9264 https   github com apache spark pull 9264  has been merged to master and resolved this blocking issue   submitJobThreadPool  can be removed now ,
SPARK-14562,  constraints propagation in Union,   Union only takes intersect of the constraints from it s children  all others are dropped  we should try to merge them together ,
SPARK-17653,  should remove unnecessary distincts  in multiple unions ,     code  select 1 a union select 2 b union select 3 c  code    Explain plan   code     Physical Plan     HashAggregate keys  a 13   functions        Exchange hashpartitioning a 13  200         HashAggregate keys  a 13   functions              Union              HashAggregate keys  a 13   functions                    Exchange hashpartitioning a 13  200                     HashAggregate keys  a 13   functions                          Union                          Project  1 AS a 13                             Scan OneRowRelation                            Project  2 AS b 14                             Scan OneRowRelation                Project  3 AS c 15                 Scan OneRowRelation    code   Only one distinct should be necessary  This makes a bunch of unions slower than a bunch of union alls followed by a distinct ,
SPARK-17324,  Direct Usage of HiveClient in InsertIntoHiveTable,  is another step to get rid of HiveClient from  HiveSessionState   All the metastore interactions should be through  ExternalCatalog  interface  However  the existing implementation of  InsertIntoHiveTable   still requires Hive clients  Thus  we can remove HiveClient by moving the metastore interactions into  ExternalCatalog   ,
SPARK-17144,  of useless CreateHiveTableAsSelectLogicalPlan,  CreateHiveTableAsSelectLogicalPlan   is a dead code after refactoring ,
SPARK-18714,SparkSession time   a simple timer function,  Spark developers often want to test the runtime of some function in interactive debugging and testing  It d be really useful to have a simple spark time method that can test the runtime  ,
SPARK-15629,  0 python converage pyspark ml linalg,  parent task SPARK 14813  ,
SPARK-13925,  R like summary statistics in SparkR  glm for more family and link functions,  continues the work of SPARK 11494  SPARK 9837  and SPARK 12566 to expose R like model summary in more family and link functions ,
SPARK-7279,  diffSum which is theoretical zero in LinearRegression and coding formating,,
SPARK-15214,  code generation for Generate,  Generate   currently does not support code generation  Lets add support for CG and for it and its most important generators    explode   and   json tuple   ,
SPARK-10599,  communication in BlockMatrix multiply and increase performance,  BlockMatrix multiply sends each block to all the corresponding columns of the right BlockMatrix  even though there might not be any corresponding block to multiply with   Some optimizations we can perform are     Simulate the multiplication on the driver  and figure out which blocks actually need to be shuffled    Send the block once to a partition  and join inside the partition rather than sending multiple copies to the same partition,
SPARK-11468,  R API for stddev variance,,
SPARK-10688,  API for AFTSurvivalRegression,  SPARK 10686  we should add Python API for AFTSurvivalRegression ,
SPARK-14410,SessionCatalog needs to check function existence ,  now  operations for an existing functions in SessionCatalog do not really check if the function exists  We should add this check and avoid of doing the check in command ,
SPARK-13659,  returnValues from BlockStore APIs,  preparation for larger refactorings  I think that we should remove the confusing returnValues   option from the BlockStore put   APIs  returning the value is only useful in one place  caching  and in other situations  such as block replication  it s simpler to put   and then get   ,
SPARK-9644,  update DecimalType with precision   18 in UnsafeRow,   we don t support using DecimalType with precision   18 in new unsafe aggregation  it s good to support it ,
SPARK-16498,  hive hack for data source table into HiveExternalCatalog,,
SPARK-19060,  the supportsPartial flag in AggregateFunction,,
SPARK-15753,  some Analyzer stuff to Analyzer from DataFrameWriter,DataFrameWriter insertInto includes some Analyzer stuff  We should move it to Analyzer ,
SPARK-9142,  unnecessary self types in Catalyst,  small change  based on code review and offline discussion with   dragos  ,
SPARK-13571,  current database in SQL HiveContext,  already have internal APIs for Hive to do this  We should do it for SQLContext too so we can merge these code paths one day ,
SPARK-13234,  duplicated SQL metrics,  lots of SQL operators  we have metrics for both of input and output  the number of input rows should be exactly the number of output rows of child  we could only have metrics for output rows   After we improve the performance using whole stage codegen  the overhead of SQL metrics are not trivial anymore  we should avoid that if it s not necessary   Some of the operator does not have SQL metrics  we should add that for them   For those operators that have the same number of rows from input and output  for example  Projection  we may don t need that  ,
SPARK-14626,  accumulators and task metrics,  goal of the ticket is to simplify both the external interface and the internal implementation for accumulators and metrics  They are unnecessarily convoluted and we should be able to simplify them quite a bit   This is an umbrella ticket and I will iteratively create new tasks as my investigation goes on  At a high level  I d would like to create better abstractions for internal implementations  as well as creating a simplified accumulator v2 external interface that doesn t involve a complex type hierarchy ,
SPARK-9085,  LeafNode  UnaryNode  BinaryNode from TreeNode,  are not very useful  and cause problems with toString due to the order they are mixed in  ,
SPARK-12811,  interface for generalized linear models  GLMs ,  Spark 1 6  MLlib provides logistic regression and linear regression with L1 L2 elastic net regularization  We want to expand the support of generalized linear models  GLMs  in 2 0  e g   Poisson Gamma families and more link functions  SPARK 9835 implements a GLM solver for the case when the number of features is small  We also need to design an interface for GLMs   In SparkR  we can simply follow glm or glmnet  On the Python Scala Java side  the interface should be consistent with LinearRegression and LogisticRegression  e g     code  val glm   new GeneralizedLinearModel      setFamily  poisson      setSolver  irls    code   It would be great if LinearRegression and LogisticRegression can reuse code from GeneralizedLinearModel ,
SPARK-17931,taskScheduler has some unneeded serialization,  the existing code  there are three layers of serialization involved in sending a task from the scheduler to an executor    A Task object is serialized   The Task object is copied to a byte buffer that also contains serialized information about any additional JARs  files  and Properties needed for the task to execute  This byte buffer is stored as the member variable serializedTask in the TaskDescription class    The TaskDescription is serialized  in addition to the serialized task   JARs  the TaskDescription class contains the task ID and other metadata  and sent in a LaunchTask message   While it is necessary to have two layers of serialization  so that the JAR  file  and Property info can be deserialized prior to deserializing the Task object  the third layer of deserialization is unnecessary  this is as a result of SPARK 2521   We should eliminate a layer of serialization by moving the JARs  files  and Properties into the TaskDescription class ,
SPARK-13873,  the copy in whole stage codegen when there is no joins,  we generate code for join  we copy the output row  because there could be multiple output row from single input row   We could avoid this copy when there is no join  or the join will not generate multiple output rows from single input row ,
SPARK-10938,  typeId in columnar cache,  is not needed in columnar cache  it s confusing to having them ,
SPARK-14655,   assert true  function,  issue aims to implement  assert true  function  It s  Hive Generic UDF Function https   github com apache hive blob master ql src java org apache hadoop hive ql udf generic GenericUDFAssertTrue java  since 1 2  The following is function description of Hive 1 2    code  hive  desc function assert true  OK assert true condition    Throw an exception if  condition  is not true   code    code  scala  sql  desc function assert true   collect   foreach println   Function  assert true   Class  org apache spark sql catalyst expressions AssertTrue   Usage  assert true condition    Throw an exception if  condition  is not true    code   Please note that Hive s false values are  false    0    null   and       empty string    But  in Spark  we intentionally designed to use implicit typecasting to BooleanType  ,
SPARK-18797,  spark logit in sparkr vignettes,  logit is added in 2 1  We need to update spark vignettes to reflect the changes  This is part of SparkR QA work ,
SPARK-18493,  withWatermark and checkpoint to python dataframe,  two methods were added to Scala Datasets  but are not available in Python yet ,
SPARK-9336,  all extra JoinedRows,  were added to improve performance  so JIT can inline the JoinedRow calls   However  we can also just improve it by projecting output out to UnsafeRow in Tungsten variant of the operators  ,
SPARK-15871,  assertNotPartitioned check in DataFrameWriter,  it doesn t make sense to specify partitioning parameters  e g  when  we write data out from Datasets DataFrames into jdbc tables or streaming   ForeachWriters    We probably should add checks against this in   DataFrameWriter   ,
SPARK-14850,  MatrixUDT should take primitive arrays without boxing,  SPARK 9390  we switched to use GenericArrayData to store indices and values in vector matrix UDTs  However  GenericArrayData is not specialized for primitive types  This might hurt MLlib performance badly  We should consider either specialize GenericArrayData or use a different container   cc    cloud fan    yhuai ,
SPARK-7131,  tree forest implementation from spark mllib to spark ml,  want to change and improve the spark ml API for trees and ensembles  but we cannot change the old API in spark mllib   To support the changes we want to make  we should move the implementation from spark mllib to spark ml   We will generalize and modify it  but will also ensure that we do not change the behavior of the old API   There are several steps to this  1  Copy the implementation over to spark ml and change the spark ml classes to use that implementation  rather than calling the spark mllib implementation   The current spark ml tests will ensure that the 2 implementations learn exactly the same models   Note  This should include performance testing to make sure the updated code does not have any regressions       UPDATE   I have run tests using spark perf  and there were no regressions  2  Remove the spark mllib implementation  and make the spark mllib APIs wrappers around the spark ml implementation   The spark ml tests will again ensure that we do not change any behavior  3  Move the unit tests to spark ml  and change the spark mllib unit tests to verify model equivalence   This JIRA is now for step 1 only   Steps 2 and 3 will be in separate JIRAs   After these updates  we can more safely generalize and improve the spark ml implementation ,
SPARK-12011,  Variance etc should support columnName as arguments,  SQL aggregate function stddev stddev pop stddev samp variance var pop var samp skewness kurtosis collect list collect set should support columnName as arguments like other aggregate function max min count sum   ,
SPARK-16343,  the PushDownPredicate rule to pushdown predicates currectly in non deterministic condition,  our Optimizer may reorder the predicates to run them more efficient  but in non deterministic condition  change the order between deterministic parts and non deterministic parts may change the number of input rows  For example   code sql  SELECT a FROM t WHERE rand     0 1 AND a   1  code  And  code sql  SELECT a FROM t WHERE a   1 AND rand     0 1  code  may call rand   for different times and therefore the output rows differ ,
SPARK-18949,  recoverPartitions API to Catalog,   we only have a SQL interface for recovering all the partitions in the directory of a table and update the catalog   MSCK REPAIR TABLE  or  ALTER TABLE table RECOVER PARTITIONS    Actually  very hard for me to remember  MSCK  and have no clue what it means   After the new  Scalable Partition Handling   the table repair becomes much more important for making visible the data in the created data source partitioned table   It is desriable to add it into the Catalog interface so that users can repair the table by  noformat  spark catalog recoverPartitions  testTable    noformat  ,
SPARK-18821,  k means wrapper in SparkR,  a wrapper in SparkR to support bisecting k means,
SPARK-18362,  TextFileFormat in implementation of CSVFileFormat,  s CSVFileFormat data source uses inefficient methods for reading files during schema inference and does not benefit from file listing   IO performance improvements made in Spark 2 0  In order to fix this performance problem  we should re implement those read paths in terms of TextFileFormat ,
SPARK-14951,Subexpression elimination in wholestage codegen version of TungstenAggregate,  wholestage codegen version of TungstenAggregate does not support subexpression elimination  We should support it ,
SPARK-5604,  setCheckpointDir from LDA and tree Strategy,  the discussion from the LDA PR  CheckpoingDir is a global Spark configuration  which should not be altered by an ML algorithm  We could check whether checkpointDir is set if checkpointInterval is positive ,
SPARK-11646,WholeTextFileRDD should return Text rather than String,  it returns Text  we can reuse this in Spark SQL to provide a WholeTextFile data source and directly convert the Text into UTF8String without extra string decoding and encoding ,
SPARK-15586,  2 0 QA  Scala APIs audit for evaluation  tuning,  containing JIRA for details   SPARK 14811 ,
SPARK-19183,  deleteWithJob hook to internal commit protocol API,  in SQL we implement overwrites by calling fs delete   directly on the original data  This is not ideal since we the original files end up deleted even if the job aborts  We should extend the commit protocol to allow file overwrites to be managed as well ,
SPARK-11114,  getOrCreate for SparkContext SQLContext for Python,  SQLContext newSession  ,
SPARK-15414,  the mllib ml linalg type conversion APIs public,  should open up the APIs for converting between new  old linear algebra types  in spark mllib linalg     Vector asML   Vectors fromML   same for Sparse Dense and for Matrices  I made these private originally  but they will be useful for users transitioning workloads ,
SPARK-14175,  whole stage codegen interface,   remove consumeChild 2  always create code for UnsafeRow and variables ,
SPARK-13992,  support for off heap caching,  should add support for caching serialized data off heap within the same process  i e  using direct buffers or sun misc unsafe    I ll expand this JIRA later with more detail  filing now as a placeholder  ,
SPARK-18296,  consistent naming for expression test suites,  think we have an undocumented naming convention to call expression unit tests ExpressionsSuite  and the end to end tests FunctionsSuite  It d be great to make all test suites consistent with this naming convention  ,
SPARK-13427,  USING clause in JOIN,  queries that JOIN tables with USING clause   SELECT   from table1 JOIN table2 USING  column list  ,
SPARK-13527,  Filters based on Constraints,  all the deterministic conditions in a   Filter   that are contained in the Child ,
SPARK-8949,  references to preferredNodeLocalityData in javadoc and print warning when used,  SparkContext constructor that takes preferredNodeLocalityData has not worked since before Spark 1 0  Also  the feature in SPARK 4352 is strictly better than a correct implementation of that feature   We should remove any documentation references to that feature and print a warning when it is used saying it doesn t work ,
SPARK-17761,  InternalRow hierarchy,  current InternalRow hierarchy makes a difference between immutable and mutable rows  In practice we cannot guarantee that an immutable internal row is immutable  you can always pass a mutable object as an one of its elements   Lets make all internal rows mutable  and reduce the complexity  ,
SPARK-10343,  nullability of expression in codegen,  codegen  we didn t consider nullability of expressions  Once considering this  we can avoid lots of null check  reduce the size of generated code  also improve performance    Before that  we should double check the correctness of nullablity of all expressions and schema  or we will hit NPE or wrong results ,
SPARK-15962,  additonal implementation with a dense format for UnsafeArrayData,  would be good to have an additional implementation  which uses dense format  for   UnsafeArrayData   to reduce memory footprint  Current   UnsafeArrayData   implementation uses only a sparse format  It is useful for an   UnsafeArrayData   that is created by a method   fromPrimitiveArray    which have no   null   value  ,
SPARK-10378,  HashJoinCompatibilitySuite,  don t bring much value since we now have better unit test coverage for hash joins  This will also help reduce the test time  ,
SPARK-11127,  Kinesis Client Library to the latest stable version,  use KCL 1 3 0 in the current master  KCL 1 4 0 added integration with Kinesis Producer Library  KPL  and support auto de aggregation  It would be great to upgrade KCL to the latest stable version   Note that the latest version is 1 6 1 and 1 6 0 restored compatibility with dynamodb streams kinesis adapter  which was broken in 1 4 0  See https   github com awslabs amazon kinesis client release notes     tdas    brkyvz  Please recommend a version for upgrade ,
SPARK-13098,  GenericInternalRowWithSchema,,
SPARK-16135,  hashCode and euqals in ArrayBasedMapData,  need  hashCode and euqals in UnsafeMapData because of the      behaivour of UnsafeMapData is different from that of ArrayBasedMapData ,
SPARK-15819,  KMeanSummary in KMeans of PySpark,  s no corresponding python api for KMeansSummary  it would be nice to have it  ,
SPARK-17486,  unused TaskMetricsUIData updatedBlockStatuses field,    TaskMetricsUIData updatedBlockStatuses   field is assigned to but never read  increasing the memory consumption of the web UI  We should remove this field ,
SPARK-9815,  PlatformDependent UNSAFE    Platform,PlatformDependent UNSAFE is way too verbose  ,
SPARK-14353,  Time Windowing API for Python  R  and SQL,  time windowing function  window  was added to Datasets  This JIRA is to track the status for the R  Python and SQL API ,
SPARK-13893,  SQLContext catalog  internal method ,  code can go through SessionState catalog  This brings two small benefits   1  Reduces internal dependency on SQLContext  2  Removes another public method in Java  Java does not obey package private visibility    More importantly  according to the design in SPARK 13485  we d need to claim this catalog function for the user facing public functions  rather than having an internal field     ,
SPARK-15178,  LazyFileRegion,LazyFileRegion was created so we didn t create a file descriptor before having to send the file  see https   issues apache org jira browse SPARK 4307   The change has been pushed back into Netty to support the same things under the DefaultFileRegion    https   github com netty netty issues 3129 https   github com netty netty commit a4450b76d9c786f8b7f76e230938b0200a7188be  It looks like that went into 4 0 25 Final   I believe at the time we created LazyFileRegion we were on 4 0 23 Final and we are now using 4 0 29 Final so we should be able to use the netty class directly ,
SPARK-10738,Refactoring  Instance  out from LOR and LIR  and also cleaning up some code,Refactoring  Instance  case class out from LOR and LIR  and also cleaning up some code ,
SPARK-13926,Automatically use Kryo serializer when shuffling RDDs with simple types,  ClassTags are available when constructing ShuffledRDD we can use  them to automatically use Kryo for shuffle serialization when the RDD s types are guaranteed to be compatible with Kryo  e g  RDDs whose key  value  and or combiner types are primitives  arrays of primitives  or strings   This is likely to result in a large performance gain for many RDD API workloads ,
SPARK-4056,  snappy java to 1 1 1 5,  should upgrade snappy java to 1 1 1 5 across all of our maintenance branches   This release improves error messages when attempting to deserialize empty inputs using SnappyInputStream  this operation is always an error  but the old error messages made it hard to distinguish failures due to empty streams from ones due to reading invalid   corrupted streams   see https   github com xerial snappy java issues 89 for more context   This should be a major help in the Snappy debugging work that I ve been doing ,
SPARK-13505,  API for MaxAbsScaler,  SPARK 13028  we should add Python API for MaxAbsScaler ,
SPARK-16691,  BucketSpec to catalyst module and use it in CatalogTable,,
SPARK-9285,  InternalRow s inheritance from Row,  is a big change  but it lets us use the type information to prevent accidentally passing internal types to external types  ,
SPARK-15472,  support for writing partitioned  csv    json    text  formats in Structured Streaming,  for partitioned  parquet  format in FileStreamSink was added in Spark 14716  now let s add support for partitioned  csv    json    text  format ,
SPARK-10045,  support for DataFrameStatFunctions in SparkR,  stat functions are defined in http   spark apache org docs latest api scala index html org apache spark sql DataFrameStatFunctions   Currently only crosstab   is supported  Functions to be supported include  corr  cov  freqItems,
SPARK-12420,  a built in CSV data source implementation,  is the most common data format in the  small data  world  It is often the first format people want to try when they see Spark on a single node  Making this built in for the most common source can provide a better experience for first time users    We should consider inlining https   github com databricks spark csv  ,
SPARK-12228,  in memory for execution hive s derby metastore,  from Hive 0 13  the derby metastore can use a in memory backend  Since our execution hive is a fake metastore  if we use in memory mode  we can reduce the time that is used on creating the execution hive ,
SPARK-3524,  workaround to pickle array of float for Pyrolite,  Pyrolite release a new version with PR https   github com irmen Pyrolite pull 11  we should remove the workaround introduced in PR https   github com apache spark pull 2365,
SPARK-9144,  DAGScheduler runLocallyWithinThread and spark localExecution enabled,  has an option called   spark localExecution enabled    according to the docs    quote  Enables Spark to run certain jobs  such as first   or take   on the driver  without sending tasks to the cluster  This can make certain jobs execute very quickly  but may require shipping a whole partition of data to the driver   quote   This feature ends up adding quite a bit of complexity to DAGScheduler  especially in the   runLocallyWithinThread   method  but as far as I know nobody uses this feature  I searched the mailing list and haven t seen any recent mentions of the configuration nor stacktraces including the runLocally method    As a step towards scheduler complexity reduction  I propose that we remove this feature and all code related to it for Spark 1 5  ,
SPARK-18973,  SortPartitions and RedistributeData,SortPartitions and RedistributeData logical operators are not actually used and can be removed  Note that we do have a Sort operator  with global flag false  that subsumed SortPartitions ,
SPARK-10592,  weights and use coefficients instead in ML models,  name  weights  becomes confusing as we are supporting weighted instanced  As discussed in https   github com apache spark pull 7884  we want to deprecate  weights  and use  coefficients  instead     Deprecate but do not remove  weights     Only make changes under  spark ml  ,
SPARK-16248,  the list of Hive fallback functions,  patch removes the blind fallback into Hive for functions  Instead  it creates a whitelist and adds only a small number of functions to the whitelist  i e  the ones we intend to support in the long run in Spark   ,
SPARK-6479,  external block store API,  be great to create APIs for external block stores  rather than doing a bunch of if statements everywhere   ,
SPARK-14581,  filter push down,  now  filter push down only works with Project  Aggregate  Generate and Join  they can t be pushed through many other plans ,
SPARK-17848,  LabelCol datatype cast into Predictor fit, Classifier getNumClasses  can not support Non Double types  and classification algos relying on it do not support non double labelCol  like  NavieBayes    As suggested by   sethah   it is not a reasonable way to do datatype cast everywhere  And we can make cast only happen in  Predictor      yanboliang   josephkb   srowen ,
SPARK-10097,  Evaluator should indicate if metric should be maximized or minimized,  Evaluator currently requires that metrics be maximized  bigger is better    That is counterintuitive for some metrics   Currently  we hackily negate some metrics in RegressionEvaluator  which is weird   Instead  we should    Return the metric as expected  e g    rmse  should return RMSE  not its negation     Provide an indicator of whether the metric should be maximized or minimized   Model selection algorithms can use the indicator as needed ,
SPARK-18592,  DT RF GBT Param setter methods to subclasses,  DT RF GBT Param setter methods to subclasses and deprecate these methods in the Model classes to make them more Java friendly  See discussion at https   github com apache spark pull 15913 discussion r89662469  ,
SPARK-12374,  performance of Range APIs via adding logical physical operators,  an actual logical physical operator for range for matching the performance of RDD Range APIs    Compared with the old Range API  the new version is 3 times faster than the old version     code  scala  val startTime   System currentTimeMillis  sqlContext oldRange 0  1000000000  1  15  count    val endTime   System currentTimeMillis  val start   new Timestamp startTime   val end   new Timestamp endTime   val elapsed    endTime   startTime   1000 0 startTime  Long   1450416394240                                                  endTime  Long   1450416421199 start  java sql Timestamp   2015 12 17 21 26 34 24 end  java sql Timestamp   2015 12 17 21 27 01 199 elapsed  Double   26 959  code    code  scala  val startTime   System currentTimeMillis  sqlContext range 0  1000000000  1  15  count    val endTime   System currentTimeMillis  val start   new Timestamp startTime   val end   new Timestamp endTime   val elapsed    endTime   startTime   1000 0 startTime  Long   1450416360107                                                  endTime  Long   1450416368590 start  java sql Timestamp   2015 12 17 21 26 00 107 end  java sql Timestamp   2015 12 17 21 26 08 59 elapsed  Double   8 483  code ,
SPARK-9572,  StreamingContext getActiveOrCreate   to python API,,
SPARK-14426,  ParserUtils and ParseUtils,  have ParserUtils and ParseUtils which are both utility collections for use during the parsing process   Those name and what they are used for is very similar so I think we can merge them   Also  the original unescapeSQLString method may have a fault  When   u0061  style character literals are passed to the method  it s not unescaped successfully ,
SPARK-11503,  API audit for Spark 1 6,  ticket to walk through all newly introduced APIs to make sure they are consistent  ,
SPARK-10982,  ExpressionAggregate    DeclarativeAggregate,  more closely with ImperativeAggregate  ,
SPARK-13980,Incrementally serialize blocks while unrolling them in MemoryStore,  a block is persisted in the MemoryStore at a serialized storage level  the current MemoryStore putIterator   code will unroll the entire iterator as Java objects in memory  then will turn around and serialize an iterator obtained from the unrolled array  This is inefficient and doubles our peak memory requirements  Instead  I think that we should incrementally serialize blocks while unrolling them  A downside to incremental serialization is the fact that we will need to deserialize the partially unrolled data in case there is not enough space to unroll the block and the block cannot be dropped to disk  However  I m hoping that the memory efficiency improvements will outweigh any performance losses as a result of extra serialization in that hopefully rare case ,
SPARK-10657,  legacy SCP based Jenkins log archiving code,  of https   issues apache org jira browse SPARK 7561  we no longer need to use our custom SCP based mechanism for archiving Jenkins logs on the master machine  this has been superseded by the use of a Jenkins plugin which archives the logs and provides public viewing of them   We should remove the legacy log syncing code  since this is a blocker to disabling Worker    Master SSH on Jenkins ,
SPARK-9773,  Python API for MultilayerPerceptronClassifier,  Python API for MultilayerPerceptronClassifier,
SPARK-16401,  Source APIs  Extending RelationProvider and CreatableRelationProvider Without SchemaRelationProvider,  users try to implement a data source API with extending only RelationProvider and CreatableRelationProvider  they will hit an error when resolving the relation   noformat  spark read  format  org apache spark sql test DefaultSourceWithoutUserSpecifiedSchema      load      write  format  org apache spark sql test DefaultSourceWithoutUserSpecifiedSchema      save    noformat   The error they hit is like  noformat  xyzDataSource does not allow user specified schemas   org apache spark sql AnalysisException  xyzDataSource does not allow user specified schemas   	at org apache spark sql execution datasources DataSource resolveRelation DataSource scala 319  	at org apache spark sql execution datasources DataSource write DataSource scala 494  	at org apache spark sql DataFrameWriter save DataFrameWriter scala 211   noformat ,
SPARK-16360,  up SQL query performance by removing redundant  executePlan  call in  Dataset ,   there are a few reports about Spark 2 0 query performance regression for large queries   This issue speeds up SQL query processing performance by removing redundant consecutive  executePlan  call in  Dataset ofRows  function and  Dataset  instantiation  Specifically  this issue aims to reduce the overhead of SQL query execution plan generation  not real query execution  So  we can not see the result in the Spark Web UI  Please use the following query script     Before    code  scala   pa    Entering paste mode  ctrl D to finish   val n   4000 val values    1 to n  map   toString  mkString       val columns    1 to n  map  column       mkString       val query     s          SELECT  columns       FROM VALUES   values  T  columns        WHERE 1 2 AND 1 IN   columns        GROUP BY  columns       ORDER BY  columns           stripMargin  def time R  block     R   R       val t0   System nanoTime     val result   block   println  Elapsed time        System nanoTime   t0    1e9     s     result    time sql query   time sql query       Exiting paste mode  now interpreting   Elapsed time  30 138142577s Elapsed time  25 787751452s  code     After    code  Elapsed time  17 500279659s     First query has a little overhead of initialization  Elapsed time  12 364812255s     This shows the real difference  The speed up is about 2 times   code   ,
SPARK-9268,  setDefault should not keep varargs annotation,   SPARK 7498    We added varargs  again    Though it is technically correct  it often requires that developers do clean assembly  rather than  not clean  assembly  which is a nuisance during development   This JIRA will remove it for now  pending a fix to the Scala compiler ,
SPARK-12465,  spark deploy mesos zookeeper dir and use spark deploy zookeeper dir,  spark deploy mesos zookeeper dir and use existing configuration spark deploy zookeeper dir for Mesos cluster mode ,
SPARK-12438,  SQLUserDefinedType support for encoder,  should add SQLUserDefinedType support for encoder ,
SPARK-14491,  object operator framework to make it easy to eliminate serializations,,
SPARK-14912,  data source options to Hadoop configurations,  currently have no way for users to propagate options to the underlying library that rely in Hadoop configurations to work  For example  there are various options in parquet mr that users might want to set  but the data source API does not expose a per job way to set it   This patch propagates the user specified options also into Hadoop Configuration  ,
SPARK-9759,  performance of Decimal times   and casting from integral,  discussion here  https   github com apache spark pull 8018 issuecomment 129044057,
SPARK-10891,  MessageHandler to KinesisUtils createStream similar to Direct Kafka,  is support for message handler in Direct Kafka Stream  which allows arbitrary T to be the output of the stream instead of Array Byte   This is a very useful function  therefore should exist in Kinesis as well ,
SPARK-14077,  weighted instances in naive Bayes,  naive Bayes  we expect inputs to be individual observations  In practice  people may have the frequency table instead  It is useful for us to support instance weights to handle this case ,
SPARK-14269,  unnecessary submitStage   call ,  a method   submitStage     for waiting stages is called on every iteration of the event loop in   DAGScheduler   to submit all waiting stages  but most of them are not necessary because they are not related to Stage status  The case we should try to submit waiting stages is only when their parent stages are successfully completed   This elimination can improve   DAGScheduler   performance  ,
SPARK-12315,  operator not pushed down for JDBC datasource ,  IsNotNull   filter is not being pushed down for JDBC datasource   It looks it is SQL standard according to SQL 92  SQL 1999  SQL 2003 and SQL 201x and I believe most databases support this ,
SPARK-7620,  calling size  length in while condition to avoid extra JVM call,,
SPARK-16358,  InsertIntoHiveTable From Logical Plan,LogicalPlan  InsertIntoHiveTable  is useless  Thus  we can remove it from the code base ,
SPARK-3445,  and later remove YARN alpha support,  will depend a bit on both user demand and the commitment level of maintainers  but I d like to propose the following timeline for yarn alpha support   Spark 1 2  Deprecate YARN alpha Spark 1 3  Remove YARN alpha  i e  require YARN stable   Since YARN alpha is clearly identified as an alpha API  it seems reasonable to drop support for it in a minor release  However  it does depend a bit whether anyone uses this outside of Yahoo   and that I m not sure of  In the past this API has been used and maintained by Yahoo  but they ll be migrating soon to the stable API s ,
SPARK-9810,  individual commit messages from the squash commit message,  took a look at the commit messages in git log    it looks like the individual commit messages are not that useful to include  but do make the commit messages more verbose  They are usually just a bunch of extremely concise descriptions of  bug fixes    merges   etc    code      cb3f12d  xxx  add whitespace     6d874a6  xxx  support pyspark for yarn client      89b01f5  yyy  Update the unit test to add more cases     275d252  yyy  Address the comments     7cc146d  yyy  Address the comments     2624723  yyy  Fix rebase conflict     45befaa  yyy  Update the unit test     bbc1c9c  yyy  Fix checkpointing doesn t retain driver port issue  code   See mailing list discussions  http   apache spark developers list 1001551 n3 nabble com discuss Removing individual commit messages from the squash commit message td13295 html ,
SPARK-9632,  InternalRow toSeq to make it accept data type info,,
SPARK-12204,  drop method for DataFrame in SparkR,,
SPARK-13928,  org apache spark Logging into org apache spark internal Logging,  was made private in Spark 2 0  If we move it  then users would be able to create a Logging trait themselves to avoid changing their own code  Alternatively  we can also provide in a compatibility package that adds logging  ,
SPARK-10997,  based RPC env should support a  client only  mode ,  new netty RPC still behaves too much like akka  it requires both client  e g  an executor  and server  e g  the driver  to listen for incoming connections   That is not necessary  since sockets are full duplex and RPCs should be able to flow either way on any connection  Also  because the semantics of the netty based RPC don t exactly match akka  you get weird issues like SPARK 10987   Supporting a client only mode also reduces the number of ports Spark apps need to use ,
SPARK-17173,  R mllib for easier ml implementations,,
SPARK-12044,  usage of isnan  isNaN,   Add  isNaN  to  Column  for SparkR  Column should has three related variable functions  isNaN  isNull  isNotNull  2  Replace  DataFrame isNaN  with  DataFrame isnan  at SparkR side  Because DataFrame isNaN has been deprecated and will be removed at Spark 2 0   3  Add  isnull  to  DataFrame  for SparkR  DataFrame should has two related functions  isnan  isnull  ,
SPARK-17190,  of HiveSharedState,   HiveClient  is used to interact with the Hive metastore  it should be hidden in  HiveExternalCatalog   After moving  HiveClient  into  HiveExternalCatalog    HiveSharedState  becomes a wrapper of  HiveExternalCatalog   Thus  removal of  HiveSharedState  becomes straightforward  After removal of  HiveSharedState   the reflection logic is directly applied on the choice of  ExternalCatalog  types  based on the configuration of  CATALOG IMPLEMENTATION      HiveClient  is also used invoked by the other entities besides HiveExternalCatalog  we defines the following two APIs   noformat             Return the existing   HiveClient   used to interact with the metastore          def getClient  HiveClient             Return a   HiveClient   as a new session         def getNewClient  HiveClient  noformat ,
SPARK-13465,  a task failure listener to TaskContext,TaskContext supports task completion callback  which gets called regardless of task failures  However  there is no way for the listener to know if there is an error  This ticket proposes adding a new listener that gets called when a task fails ,
SPARK-14267,  multiple Python UDFs in single batch, code  select udf1 a   udf2 b   udf3 a  b   code ,
SPARK-10117,  SQL data source API for reading LIBSVM data,  is convenient to implement data source API for LIBSVM format to have a better integration with DataFrames and ML pipeline API    code  import org apache spark ml source libsvm    val training   sqlContext read    format  libsvm      option  numFeatures    10000      load  path    code   This JIRA covers the following   1  Read LIBSVM data as a DataFrame with two columns  label  Double and features  Vector  2  Accept  numFeatures  as an option  3  The implementation should live under  org apache spark ml source libsvm  ,
SPARK-16374,  Alias from MetastoreRelation and SimpleCatalogRelation,  from the other leaf nodes   MetastoreRelation  and  SimpleCatalogRelation  have a pre defined  alias   which is used to change the qualifier of the node  However  based on the existing alias handling  alias should be put in  SubqueryAlias     This PR is to separate alias handling from  MetastoreRelation  and  SimpleCatalogRelation  to make it consistent with the other nodes    For example  below is an example query for  MetastoreRelation   which is converted to  LogicalRelation    noformat  SELECT tmp a   1 FROM test parquet ctas tmp WHERE tmp a   2  noformat   Before changes  the analyzed plan is  noformat     Analyzed Logical Plan     a   1   int Project   a 951   1  AS  a   1  952     Filter  a 951   2        SubqueryAlias tmp          Relation a 951  parquet  noformat  After changes  the analyzed plan becomes  noformat     Analyzed Logical Plan     a   1   int Project   a 951   1  AS  a   1  952     Filter  a 951   2        SubqueryAlias tmp          SubqueryAlias test parquet ctas             Relation a 951  parquet  noformat     Note  the optimized plans are the same     For  SimpleCatalogRelation   the existing code always generates two Subqueries  Thus  no change is needed  ,
SPARK-17260,  CreateTables to HiveStrategies,,
SPARK-13742,  non iterator interface to RandomSampler,RandomSampler sample currently accepts iterator as input and output another iterator  This makes it inappropriate to use in wholestage codegen of Sampler operator  We should add non iterator interface to RandomSampler ,
SPARK-12188, SQL  Code refactoring and comment correction in Dataset APIs,  Created a new private variable  boundTEncoder  that can be shared by multiple functions   RDD    select  and  collect      Replaced all the  queryExecution analyzed  by the function call  logicalPlan    A few API comments are using wrong class names  e g    DataFrame   or parameter names  e g    n     A few API descriptions are wrong   e g    mapPartitions  ,
SPARK-9692,  SqlNewHadoopRDD s generated Tuple2 and InterruptibleIterator,  small performance optimization    we don t need to generate a Tuple2 and then immediately discard the key  We also don t need an extra wrapper  ,
SPARK-11493,  Bitset in BytesToBytesMap,  we have 4 bytes as number of records in the beginning of a page  then the address can not be zero  so we do not need the bitset ,
SPARK-11236,  Tachyon dependency to 0 8 0,  the tachyon client dependency from 0 7 1 to 0 8 0  There are no new dependencies added or Spark facing APIs changed ,
SPARK-5440,  toLocalIterator to pyspark rdd,toLocalIterator is available in Java and Scala  If we add this functionality to Python  then we can also be able to use PySpark to iterate over a dataset partition by partition ,
SPARK-9334,  UnsafeRowConverter in favor of UnsafeProjection,,
SPARK-13695,  t cache MEMORY AND DISK blocks as bytes in memory store when reading spills,  a cached block is spilled to disk and read back in serialized form  i e  as bytes   the current BlockManager implementation will attempt to re insert the serialized block into the MemoryStore even if the block s storage level requests deserialized caching   This behavior adds some complexity to the MemoryStore but I don t think it offers many performance benefits and I d like to remove it in order to simplify a larger refactoring patch  Therefore  I propose to change the behavior such that disk store reads will only cache bytes in the memory store for blocks with serialized storage levels   There are two places where we request serialized bytes from the BlockStore   1  getLocalBytes    which is only called when reading local copies of TorrentBroadcast pieces  Broadcast pieces are always cached using a serialized storage level  so this won t lead to a mismatch in serialization forms if spilled bytes read from disk are cached as bytes in the memory store   2  the non shuffle block branch in getBlockData    which is only called by the NettyBlockRpcServer when responding to requests to read remote blocks  Caching the serialized bytes in memory will only benefit us if those cached bytes are read before they re evicted and the likelihood of that happening seems low since the frequency of remote reads of non broadcast cached blocks seems very low  Caching these bytes when they have a low probability of being read is bad if it risks the eviction of blocks which are cached in their expected serialized deserialized forms  since those blocks seem more likely to be read in local computation   Therefore  I think this is a safe change ,
SPARK-15364,  Python picklers for ml Vector and ml Matrix under spark ml python,  picklers for both new and old vectors are implemented under PythonMLlibAPI  To separate spark mllib from spark ml  we should implement them under  spark ml python  instead  I set the target to 2 1 since those are private APIs ,
SPARK-16043,  GenericArrayData implementation specialized for a primitive array,  is a ToDo of GenericArrayData class  which is to eliminate boxing unboxing for a primitive array  described  here https   github com apache spark blob master sql catalyst src main scala org apache spark sql catalyst util GenericArrayData scala L31    It would be good to prepare GenericArrayData implementation specialized for a primitive array to eliminate boxing unboxing from the view of runtime memory footprint and performance   ,
SPARK-16958,  subqueries within single query,  could be same subquery within a single query  we could reuse the result without running it multiple times ,
SPARK-13430,  ml summary function in PySpark for classification and regression models,  think model summary interface which is available in Spark s scala  Java and R interfaces should also be available in the python interface    Similar to  SPARK 11494   https   issues apache org jira browse SPARK 11494,
SPARK-12636,  API on UnsafeRowRecordReader to just run on files,  is beneficial just from a code testability point of view to be able to exercise individual components  Also makes it easy to benchmark it  It would be able to read data without need to create al the associate hadoop input split  etc components ,
SPARK-19411,  the metadata used to mark optional columns in merged Parquet schema for filter predicate pushdown,  is a metadata introduced before to mark the optional columns in merged Parquet schema for filter predicate pushdown  As we upgrade to Parquet 1 8 2 which includes the fix for the pushdown of optional columns  we don t need this metadata now ,
SPARK-12309,  sqlContext from MLlibTestSparkContext for spark ml test suites,  sqlContext from MLlibTestSparkContext rather than creating new one for spark ml test cases ,
SPARK-14303,  k means code in SparkRWrappers,  use a single object  SparkRWrappers   https   github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala  to wrap method calls to glm and kmeans in SparkR  This is quite hard to maintain  We should refactor them into separate wrappers  like  AFTSurvivalRegressionWrapper  and  NaiveBayesWrapper    The package name should be  spakr ml r  instead of  spark ml api r  ,
SPARK-11645,  OpenHashSet for the old aggregate ,,
SPARK-17365,  multiple executors together to reduce lock contention,  regulate pending and running executors we determine the executors which are eligible to kill and kill them iteratively rather than a loop  This does an RPC call and is synchronized leading to lock contention for SparkListenerBus    Side effect   listener bus is blocked while we iteratively remove executors    ,
SPARK-10745,  configs between shuffle and RPC,  6028 uses network module to implement RPC  However  there are some configurations named with  spark shuffle  prefix in the network module  We should refactor them and make sure the user can control them in shuffle and RPC separately ,
SPARK-12286,  UnsafeRow in all SparkPlan  if possible ,  are still some SparkPlan does not support UnsafeRow  or does not support well  ,
SPARK-11292,  API for text data source,  should add text to DataFrameReader and DataFrameWriter  ,
SPARK-18010,  unneeded heavy work performed by FsHistoryProvider for building up the application listing UI page,  are known complaints cribs about History Server s Application List not updating quickly enough when the event log files that need replay are huge  Currently  the FsHistoryProvider design causes the entire event log file to be replayed when building the initial application listing  refer the method mergeApplicationListing fileStatus  FileStatus     The process of replay involves     each line in the event log being read as a string     parsing the string to a Json structure    converting the Json to the corresponding Scala classes with nested structures  Particularly the part involving parsing string to Json and then to Scala classes is expensive  Tests show that majority of time spent in replay is in doing this work    When the replay is performed for building the application listing  the only two events that the code really cares for are  SparkListenerApplicationStart  and  SparkListenerApplicationEnd    since the only listener attached to the ReplayListenerBus at that point is the ApplicationEventListener  This means that when processing an event log file with a huge number  hundreds of thousands  can be more  of events  the work done to deserialize all of these event   and then replay them is not needed  Only two events are what we re interested in  and this can be used to ensure that when replay is performed for the purpose of building the application list  we only make the effort to replay these two events and not others    My tests show that this drastically improves application list load time  For a 150MB event log from a user  with over 100 000 events  the load time  local on my mac  comes down from about 16 secs to under 1 second using this approach  For customers that typically execute applications with large event logs  and thus have multiple large event logs present  this can speed up how soon the history server UI lists the apps considerably   I will be updating a pull request with take at fixing this ,
SPARK-11362,  Spark BitSet in BroadcastNestedLoopJoin,  use scala collection mutable BitSet in BroadcastNestedLoopJoin now  We should use Spark s BitSet ,
SPARK-11389,  support for off heap memory to MemoryManager,  order to lay the groundwork for proper off heap memory support in SQL   Tungsten  we need to extend our MemoryManager to perform bookkeeping for off heap memory ,
SPARK-11030,  should be shared by across sessions,  should share the SQLTab across sessions ,
SPARK-932,Consolidate local scheduler and cluster scheduler,  should consolidate LocalScheduler and ClusterScheduler  given most of the functionalities are duplicated in both    This can be done by removing the LocalScheduler  and create a LocalSchedulerBackend that connects directly to an Executor  ,
SPARK-15983,  FileFormat prepareRead  ,  method   FileFormat prepareRead     was added in  PR  12088 https   github com apache spark pull 12088  to handle a special case in the LibSVM data source   However  the semantics of this interface method isn t intuitive  it returns a modified version of the data source options map  Considering that the LibSVM case can be easily handled using schema metadata inside   inferSchema    we can remove this interface method to keep the   FileFormat   interface clean  ,
SPARK-11663,  Java API for trackStateByKey,,
SPARK-13944,  out local linear algebra as a standalone module without Spark dependency,  out linear algebra as a standalone module without Spark dependency to simplify production deployment  We can call the new module mllib local  which might contain local models in the future   The major issue is to remove dependencies on user defined types   The package name will be changed from mllib to ml  For example  Vector will be changed from  org apache spark mllib linalg Vector  to  org apache spark ml linalg Vector   The return vector type in the new ML pipeline will be the one in ML package  however  the existing mllib code will not be touched  As a result  this will potentially break the API  Also  when the vector is loaded from mllib vector by Spark SQL  the vector will automatically converted into the one in ml package  ,
SPARK-14614,   bround  function,  issue aims to add  bound  function  aka Banker s round  by extending current  round  implementation   Hive supports  bround  since 1 3 0   Language Manual https   cwiki apache org confluence display Hive LanguageManual UDF     code  hive  select round 2 5   bround 2 5   OK 3 0	2 0  code ,
SPARK-17949,  a JVM object based aggregate operator,  new Tungsten execution engine has very robust memory management and speed for simple data types  It does  however  suffer from the following     For user defined aggregates  Hive UDAFs  Dataset typed operators   it is fairly expensive to fit into the Tungsten internal format    For aggregate functions that require complex intermediate data structures  Unsafe  on raw bytes  is not a good programming abstraction due to the lack of structs   The idea here is to introduce a JVM object based hash aggregate operator that can support the aforementioned use cases  This operator  however  should limit its memory usage to avoid putting too much pressure on GC  e g  falling back to sort based aggregate as soon the number of objects exceeds a very low threshold   Internally at Databricks we prototyped a version of this for a customer POC and have observed substantial speed ups over existing Spark   ,
SPARK-4477,  numpy from RDDSampler of PySpark,  RDDSampler  it try use numpy to gain better performance for possion    but the number of call of random   is only  1 faction    N in the pure python implementation of possion    so there is no much performance gain from numpy    numpy is not a dependent of pyspark  so it maybe introduce some problem  such as there is no numpy installed in slaves  but only installed master  as reported in xxxx    It also complicate the code a lot  so we may should remove numpy from RDDSampler ,
SPARK-16192,  the type check of CollectSet in CheckAnalysis, CollectSet  cannot have map typed data because MapTypeData does not implement  equals   So  if we find map type in  CollectSet   queries fail ,
SPARK-17510,  Streaming MaxRate Independently For Multiple Streams,  use multiple DStreams coming from different Kafka topics in a Streaming application   Some settings like maxrate and backpressure enabled disabled would be better passed as config to KafkaUtils createStream and KafkaUtils createDirectStream  instead of setting them in SparkConf   Being able to set a different maxrate for different streams is an important requirement for us  we currently work around the problem by using one receiver based stream and one direct stream      We would like to be able to turn on backpressure for only one of the streams as well       ,
SPARK-15402,  ml evaluation should support save load,  ml evaluation has supported save load at Scala side  supporting it at Python side is very straightforward and easy  ,
SPARK-6122,  Tachyon dependency to 0 6 0,,
SPARK-16155,  package grouping in genjavadoc,  1 4 and earlier releases  we have package grouping in the generated Java API docs  See http   spark apache org docs 1 4 0 api java index html  However  this disappeared in 1 5 0  http   spark apache org docs 1 5 0 api java index html    Rather than fixing it  I d suggest removing grouping  Because it might take some time to fix and it is a manual process to update the grouping in SparkBuild scala  No one complained about missing groups since 1 5 0 ,
SPARK-14019,  noop SortOrder in Sort,  SortOrder does not contain any reference  it has no effect on the sorting  Remove the noop SortOrder in Optimizer  ,
SPARK-9704,  some ML APIs public  VectorUDT  Identifiable  ProbabilisticClassifier,  JIRA is for making several ML APIs public to make it easier for users to write their own Pipeline stages   Issue brought up by   eronwright    Descriptions below copied from  http   apache spark developers list 1001551 n3 nabble com Make ML Developer APIs public post 1 4 td13583 html    We plan to make these APIs public in Spark 1 5   However  they will be marked DeveloperApi and are  very likely  to be broken in the future    VectorUDT  To define a relation with a vector field  VectorUDT must be instantiated    Identifiable trait  The trait generates a unique identifier for the associated pipeline component   Nice to have a consistent format by reusing the trait    ProbabilisticClassifier   Third party components should leverage the complex logic around computing only selected columns   We will not yet make these public    SchemaUtils  Third party pipeline components have a need for checking column types and appending columns     This will probably be moved into Spark SQL   Users can copy the methods into their own code as needed    Shared Params  HasLabel  HasFeatures   This is covered in  SPARK 7146  but reiterating it here     We need to discuss whether these should be standardized public APIs   Users can copy the traits into their own code as needed  ,
SPARK-19139,  based authentication mechanism for Spark,  SPARK 13331  support for AES encryption was added to the Spark network library  But the authentication of different Spark processes is still performed using SASL s DIGEST MD5 mechanism  That means the authentication part is the weakest link  since the AES keys are currently encrypted using 3des  strongest cipher supported by SASL   Spark can t really claim to provide the full benefits of using AES for encryption   We should add a new auth protocol that doesn t need these disclaimers ,
SPARK-4596,Refactorize Normalizer to make code cleaner,  this refactoring  the performance is slightly increased by removing the overhead from breeze vector  The bottleneck is still in breeze norm which is implemented by activeIterator  This inefficiency of breeze norm will be addressed in next PR  At least  this PR makes the code more consistent in the codebase   ,
SPARK-11290,  trackStateByKey for improved state management,  is the first cut implementation of  trackStateByKey   new improvement state management method in Spark Streaming    See the epic jira for more details   https   issues apache org jira browse SPARK 2629 ,
SPARK-17509,  wrapping catalyst datatype to Hive data type avoid pattern matching,  a job  we saw that patten matching in wrap function of HiveInspector is consuming around 10  of the time which can be avoided   A similar change in the unwrap function was made in SPARK 15956  ,
SPARK-16751,  derby to 10 12 1 1 from 10 11 1 1,  JIRA is to upgrade the derby version from 10 11 1 1 to 10 12 1 1  Sean and I figured that we only use derby for tests and so the initial pull request was to not include it in the jars folder for Spark  I now believe it is required based on comments for the pull request and so this is only a dependency upgrade   The upgrade is due to an already disclosed vulnerability  CVE 2015 1832  in derby 10 11 1 1  We used https   www versioneye com search and will be checking for any other problems in a variety of libraries too  investigating if we can set up a Jenkins job to check our pom on a regular basis so we can stay ahead of the game for matters like this   This was raised on the mailing list at http   apache spark developers list 1001551 n3 nabble com VOTE Release Apache Spark 2 0 0 RC5 tp18367p18465 html by Stephen Hellberg and replied to by Sean Owen   I ve checked the impact to previous Spark releases and this particular version of derby is the only relatively recent and without vulnerabilities version  I checked up to the 1 3 branch  so ideally we d backport this for all impacted Spark releases   I ve marked this as critical and ticked the important checkbox as it s going to impact every user  there isn t a security component  should we add one   and hence the build tag ,
SPARK-18632,AggregateFunction should not ImplicitCastInputTypes,  AggregateFunction   currently implements   ImplicitCastInputTypes    which enables implicit input type casting   This can lead to unexpected results  and should only be enabled when it is suitable for the function at hand  ,
SPARK-17912,  code generation to get data for ColumnVector ColumnarBatch,  generation to get data from   ColumnVector   and   ColumnarBatch   is becoming pervasive  The code generation part can be reused by multiple components  e g  parquet reader  data cache  and so on   This JIRA refactors the code generation part as a trait for ease of reuse ,
SPARK-13833,  against race condition when re caching spilled bytes in memory,  reading data from the DiskStore and attempting to cache it back into the memory store  we should guard against race conditions where multiple readers are attempting to re cache the same block in memory ,
SPARK-14415,  functions should show usages by command  DESC FUNCTION ,   many functions do now show usages like the followings   code  scala  sql  desc function extended  sin    collect   foreach println   Function  sin   Class  org apache spark sql catalyst expressions Sin   Usage  To be added    Extended Usage  To be added    code   This PR adds descriptions for functions and adds a testcase prevent adding function without usage   code  scala   sql  desc function extended  sin    collect   foreach println    Function  sin   Class  org apache spark sql catalyst expressions Sin   Usage  sin x    Returns the sine of x    Extended Usage    SELECT sin 0    0 0   code   The only exceptions are  cube    grouping    grouping id    rollup    window  ,
SPARK-17315,  Kolmogorov Smirnov Test to SparkR,  Smirnov Test is a popular nonparametric test of equality of distributions  There is implementation in MLlib  It will be nice if we can expose that in SparkR  ,
SPARK-13235,  an Extra Distinct in Union,  Distinct has two Distinct that generate two Aggregation in the plan    code  sql  select   from t0 union select   from t0   explain true   code    code     Parsed Logical Plan     Project  unresolvedalias   None       Subquery u 2        Distinct           Project  unresolvedalias   None                Subquery u 1                 Distinct                    Union                       Project  unresolvedalias   None                            UnresolvedRelation  t0   None                       Project  unresolvedalias   None                            UnresolvedRelation  t0   None     Analyzed Logical Plan    id  bigint Project  id 16L     Subquery u 2       Distinct          Project  id 16L              Subquery u 1                Distinct                   Union                      Project  id 16L                          Subquery t0                            Relation id 16L  ParquetRelation                      Project  id 16L                          Subquery t0                            Relation id 16L  ParquetRelation     Optimized Logical Plan    Aggregate  id 16L    id 16L     Aggregate  id 16L    id 16L        Union          Project  id 16L              Relation id 16L  ParquetRelation          Project  id 16L              Relation id 16L  ParquetRelation  code ,
SPARK-7855,  hash style shuffle code out of ExternalSorter and into own file,ExternalSorter contains a bunch of code for handling the bypassMergeThreshold   hash style shuffle path   I think that it would significantly simplify the code to move this functionality out of ExternalSorter and into a separate class which shares a common interface  insertAll   writePartitionedFile      This is a stepping stone towards eventually removing this bypass path  see SPARK 6026 ,
SPARK-10810,  session management for SQL,   we try to support multiple sessions in SQL within a Spark Context  but it s broken and not complete   We should isolate these for each session    1  current database of Hive 2  SQLConf 3  UDF UDAF UDTF 4  temporary table  For added jar and cached tables  they should be accessible for all sessions   ,
SPARK-15860,  for codegen size and perf,  should expose codahale metrics for the codegen source text size and how long it takes to compile  The size is particularly interesting  since the JVM does have hard limits on how large methods can get ,
SPARK-12242,  transform function,  to Dataset transform  ,
SPARK-11423,  PrepareRDD ,  SPARK 10342 is resolved  MapPartitionWithPrepare is not needed anymore ,
SPARK-16813,  private sql  and private spark  from catalyst package,  catalyst package is meant to be internal  and as a result it does not make sense to mark things as private sql  or private spark   It simply makes debugging harder when Spark developers need to inspect the plans at runtime  ,
SPARK-14722,  upstreams      inputRDDs   in WholeStageCodegen,,
SPARK-14639,   bround  function in Python R ,  issue aims to expose Scala  bround  function in Python R API    bround  function is implemented in SPARK 14614 by extending current  round  function  We used the following semantics from  Hive https   github com apache hive blob master ql src java org apache hadoop hive ql udf generic RoundUtils java     code  public static double bround double input  int scale        if  Double isNaN input     Double isInfinite input           return input            return BigDecimal valueOf input  setScale scale  RoundingMode HALF EVEN  doubleValue       code ,
SPARK-13601,  task failure callbacks before calling outputstream close  ,  need to submit another PR against Spark to call the task failure callbacks before Spark calls the close function on various output streams  For example  we need to intercept an exception and call TaskContext markTaskFailed before calling close in the following code  in PairRDDFunctions scala     code        Utils tryWithSafeFinally           while  iter hasNext              val record   iter next             writer write record  1 asInstanceOf AnyRef   record  2 asInstanceOf AnyRef                 Update bytes written metric every few records           maybeUpdateOutputMetrics outputMetricsAndBytesWrittenCallback  recordsWritten            recordsWritten    1                             writer close            code   Changes to Spark should include unit tests to make sure this always work in the future ,
SPARK-11029,  computeCost to KMeansModel in spark ml,  should add a method analogous to spark mllib clustering KMeansModel computeCost to spark ml clustering KMeansModel   This will be a temp fix until we have proper evaluators defined for clustering ,
SPARK-4861,  command in spark sql,  a todo in spark sql   remove    Command    and use    RunnableCommand    instead  ,
SPARK-12130,  shuffleManagerClass with shortShuffleMgrNames in ExternalShuffleBlockResolver,,
SPARK-15030,  formula in spark kmeans in SparkR,  SparkR  spark kmeans take a DataFrame with double columns  This is different from other ML methods we implemented  which support R model formula  We should add support for that as well    code none  spark kmeans data   df  formula     lat   lon        code ,
SPARK-12464,  spark deploy mesos zookeeper url and use spark deploy zookeeper url,  spark deploy mesos zookeeper url and use existing configuration spark deploy zookeeper url for Mesos cluster mode ,
SPARK-10605,  list   and collect set   should accept struct types as argument,  already supports this according to https   issues apache org jira browse HIVE 10427  Currently Spark sql still supports only primitive types ,
SPARK-16546,  drop supported multi columns in spark api and should make python api also support it ,  drop supported multi columns in spark api and should make python api also support it ,
SPARK-17241,  spark glm should have configurable regularization parameter,  has configurable L2 regularization parameter for generalized linear regression  It is very important to have them in SparkR so that users can run ridge regression ,
SPARK-19441,  IN type coercion from PromoteStrings,  removed codes are not reachable  because  InConversion  already resolve the type coercion issues   ,
SPARK-11559,   runs  no effect in k means,  deprecated  runs  in Spark 1 6  SPARK 11358   In 2 0  we can either remove  runs  or make it no effect  with warning messages   So we can simplify the implementation  I prefer the latter for better binary compatibility ,
SPARK-14060,  StringToColumn implicit class into SQLImplicits,  were kept in SQLContext implicits object for binary backward compatibility  in the Spark 1 x series  It makes more sense for this API to be in SQLImplicits since that s the single class that defines all the SQL implicits ,
SPARK-12544,  window functions in SQLContext,,
SPARK-9876,  parquet mr to 1 8 1,  parquet mr   1 8 1 fixed several issues that affect Spark  For example PARQUET 201  SPARK 9407  ,
SPARK-9095,  old Parquet support code,  the new Parquet external data source matures  we should remove the old Parquet support now ,
SPARK-18287,  hash expressions from misc scala into hash scala,  scala was getting pretty long and it s not obvious that hash expressions belong there  Creating a hash scala to put all the hash expressions   ,
SPARK-11754,consolidate  ExpressionEncoder tuple  and  Encoders tuple ,,
SPARK-14899,  spark ml HashingTF hashingAlg option,   SPARK 10574  breaks behavior of HashingTF  we should try to enforce good practice by removing the  native  hashingAlg option in spark ml and pyspark ml   We can leave spark mllib and pyspark mllib alone ,
SPARK-12913,Reimplement stat functions as declarative function,  benchmarked and discussed here  https   github com apache spark pull 10786 files r50038294   Benefits from codegen  the declarative aggregate function could be much faster than imperative one   we should re implement all the builtin aggregate functions as declarative one   For skewness and kurtosis  we need to benchmark it to make sure that the declarative one is actually faster than imperative one ,
SPARK-14075,  MemoryStore to be testable independent of BlockManager,  would be nice to refactor the MemoryStore so that it can be unit tested without constructing a full BlockManager or needing to mock tons of things ,
SPARK-9934,  NIO ConnectionManager,  should deprecate ConnectionManager in 1 5 before removing it in 1 6  ,
SPARK-11914, SQL  Support coalesce and repartition in Dataset APIs,repartition  Returns a new   Dataset   that has exactly  numPartitions  partitions   coalesce  Returns a new   Dataset   that has exactly  numPartitions  partitions  Similar to coalesce defined on an   RDD    this operation results in a narrow dependency  e g  if you go from 1000 partitions to 100 partitions  there will not be a shuffle  instead each of the 100 new partitions will claim 10 of the current partitions ,
SPARK-12235,  mutate   to support replace existing columns,    in the dplyr package supports adding new columns and replacing existing columns  But currently the implementation of mutate   in SparkR supports adding new columns only   Also make the behavior of mutate more consistent with that in dplyr  1  Throw error message when there are duplicated column names in the DataFrame being mutated  2  when there are duplicated column names in specified columns by arguments  the last column of the same name takes effect ,
SPARK-12065,  Tachyon dependency to 0 8 2,  think that we should upgrade from Tachyon 0 8 1 to 0 8 2 in order to get the fix for https   tachyon atlassian net browse TACHYON 1254 ,
SPARK-16731,  StructType in CatalogTable and remove CatalogColumn,,
SPARK-15587,  2 0 QA  Scala APIs audit for feature,  containing JIRA for details   SPARK 14811 ,
SPARK-16935,Verification of Function related ExternalCatalog APIs,  related  HiveExternalCatalog  APIs do not have enough verification logics  After the PR   HiveExternalCatalog  and  InMemoryCatalog  become consistent in the error handling    For example  below is the exception we got when calling  renameFunction     noformat  15 13 40 369 WARN org apache hadoop hive metastore ObjectStore  Failed to get database db1  returning NoSuchObjectException 15 13 40 377 WARN org apache hadoop hive metastore ObjectStore  Failed to get database db2  returning NoSuchObjectException 15 13 40 739 ERROR DataNucleus Datastore Persist  Update of object  org apache hadoop hive metastore model MFunction 205629e9  using statement  UPDATE FUNCS SET FUNC NAME   WHERE FUNC ID    failed   org apache derby shared common error DerbySQLIntegrityConstraintViolationException  The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by  UNIQUEFUNCTION  defined on  FUNCS   	at org apache derby impl jdbc SQLExceptionFactory getSQLException Unknown Source  	at org apache derby impl jdbc Util generateCsSQLException Unknown Source  	at org apache derby impl jdbc TransactionResourceImpl wrapInSQLException Unknown Source  	at org apache derby impl jdbc TransactionResourceImpl handleException Unknown Source   noformat  ,
SPARK-18034,  to MiMa 0 1 11,  should upgrade to the latest release of MiMa  0 1 11  in order to include my fix for a bug which led to flakiness in the MiMa checks  https   github com typesafehub migration manager issues 115 ,
SPARK-10990,  the serialization multiple times during unrolling of complex types,  serialize   will be called by actualSize   and append    we should use UnsafeProjection before unrolling  ,
SPARK-10047,  the implementation of collect   on DataFrame in SparkR,  in SparkR  collect   on a DataFrame collects the data within the DataFrame into a local data frame  R users are used to using data frame   However  collect   currently can t collect data of nested types from a DataFrame because  1  The serializer in JVM backend does not support nested types  2  collect   in R side assumes each column is of simple atomic type that can be combinded into a atomic vector  ,
SPARK-15624,  0 python converage ml recommendation module,  parent task SPARK 14813 ,
SPARK-4643,  unneeded staging repositories from build,,
SPARK-18702,  file block start and input file block length function,  currently have function input file name to get the path of the input file  but don t have functions to get the block start offset and length  This patch introduces two functions   1  input file block start  returns the file block start offset  or  1 if not available   2  input file block length  returns the file block length  or  1 if not available ,
SPARK-14052,  BytesToBytesMap in HashedRelation,   for the key that can not fit within a long   we build a hash map for UnsafeHashedRelation  it s converted to BytesToBytesMap after serialization and deserialization   We should build a BytesToBytesMap directly to have better memory efficiency ,
SPARK-14500,  Dataset    instead of DataFrame in MLlib APIs,  Spark 2 0   DataFrame  is an alias of  Dataset Row    MLlib API actually works for other types of  Dataset   so we should accept  Dataset     instead  It maps to  Dataset     in Java  This is a source compatible change ,
SPARK-15626,  0 python converage ml regression module,  parent task SPARK 14813 ,
SPARK-14897,  Jetty to latest version of 8 9,  looks like the head master branch of Spark uses quite an old version of Jetty  8 1 14 v20131031  There have been some announcement of security vulnerabilities  notably in 2015 and there are versions of both 8 and 9 that address those  We recently left a web ui port open and had the server compromised within days  Albeit  this upgrade shouldn t be the only security improvement made  the current version is clearly vulnerable  as is  ,
SPARK-11644,  the option to turn off unsafe and codegen,  don t sufficiently test the code path with these settings off  It is better to just consolidate and focus on making one code path work well  ,
SPARK-9562,  spark ec2 from mesos to amplab,  http   apache spark developers list 1001551 n3 nabble com Re Should spark ec2 get its own repo td13151 html for more details,
SPARK-12288,  UnsafeRow in Coalesce Except Intersect,,
SPARK-12656,  Intersect phyiscal plan using semi join,  current Intersect physical operator simply delegates to RDD intersect  We should remove the Intersect physical operator and simply transform a logical intersect into a semi join  This way  we can take advantage of all the benefits of join implementations  e g  managed memory  code generation  broadcast joins   ,
SPARK-14049,  functionality in spark history sever API to query applications by end time ,   spark history server REST API provides functionality to query applications by application start time range based on minDate and maxDate query parameters  but it  lacks support to query applications by their end time  In this Jira we are proposing optional minEndDate and maxEndDate query parameters and filtering capability based on these parameters to spark history server REST API  This functionality can be used for following queries   1  Applications finished in last  x  minutes 2  Applications finished before  y  time 3  Applications finished between  x  time to  y  time 4  Applications started from  x  time and finished before  y  time   For backward compatibility  we can keep existing minDate and maxDate query parameters as they are and they can continue support filtering based on start time range ,
SPARK-14963,YarnShuffleService should use YARN getRecoveryPath   for leveldb location,  YarnShuffleService  currently just picks a directly in the yarn local dirs to store the leveldb file   YARN added an interface in hadoop 2 5 getRecoverPath   to get the location where it should be storing this   We should change to use getRecoveryPath    This does mean we will have to use reflection or similar to check for its existence though since it doesn t exist before hadoop 2 5,
SPARK-19239,  the lowerBound and upperBound whether equal None in jdbc API,  we use the   jdbc   in pyspark  if we check the lowerBound and upperBound  we can give a more friendly suggestion ,
SPARK-11611,  API for bisecting k means,  Python API for bisecting k means ,
SPARK-5933,  deprecated configs in SparkConf,  configs are currently all strewn across the code base  It would be good to simplify the handling of the deprecated configs in a central location to avoid duplicating the deprecation logic everywhere ,
SPARK-18566,  OverwriteOptions,,
SPARK-13739,  Push Down Through Window Operator,  down the predicate through the Window operator   In this JIRA  predicates are pushed through Window if and only if the following conditions are satisfied     Predicate involves one and only one column that is part of window partitioning key   Window partitioning key is just a sequence of attributeReferences   i e   none of them is an expression    Predicate must be deterministic,
SPARK-17751,  spark sql eagerAnalysis,  always does eager analysis now  Thus  spark sql eagerAnalysis is not used any more  Thus  we need to remove it ,
SPARK-8190,ExpressionEvalHelper checkEvaluation should also run the optimizer version,  should remove the existing ExpressionOptimizationSuite  and update checkEvaluation to also run the optimizer version  ,
SPARK-9918,  runs from KMeans under the pipeline API,  requires some discussion  I m not sure whether  runs  is a useful parameter  It certainly complicates the implementation  We might want to optimize the k means implementation with block matrix operations  In this case  having  runs  may not be worth the trade offs ,
SPARK-14972,  performance of JSON schema inference s inferField step,  schema inference spends a lot of time in inferField and there are a number of techniques to speed it up  including eliminating unnecessary sorting and the use of inefficient collections ,
SPARK-18729,  should not call DataFrame collect when holding a lock,   other threads cannot query the content in MemorySink when  DataFrame collect  takes long time to finish ,
SPARK-13118,  for classes defined in package objects,  you define a class inside of a package object  the name ends up being something like   org mycompany project package MyClass     However  when reflect on this we try and load   org mycompany project MyClass   ,
SPARK-12700,SortMergeJoin and BroadcastHashJoin should support condition,  now  We use a Filter follow SortMergeJoin or BroadcastHashJoin for conditions  the result projection of join could be very expensive if they generate lots of rows  could be reduce mostly by condition  ,
SPARK-17062,    conf to mesos dispatcher process,  we simply need to add a property in Spark Config for the Mesos Dispatcher  The only option right now is to created a property file,
SPARK-17845,  window function frame boundary API in DataFrame,  SQL uses the following to specify the frame boundaries for window functions    code  ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING ROWS BETWEEN UNBOUNDED PRECEDING AND 3 PRECEDING ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ROWS BETWEEN CURRENT ROW AND UNBOUNDED PRECEDING ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING  code   In Spark s DataFrame API  we use integer values to indicate relative position    0 means  CURRENT ROW     1 means  1 PRECEDING    Long MinValue means  UNBOUNDED PRECEDING    Long MaxValue to indicate  UNBOUNDED FOLLOWING    code     ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING Window rowsBetween  3   3      ROWS BETWEEN UNBOUNDED PRECEDING AND 3 PRECEDING Window rowsBetween Long MinValue   3      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW Window rowsBetween Long MinValue  0      ROWS BETWEEN CURRENT ROW AND UNBOUNDED PRECEDING Window rowsBetween 0  Long MaxValue      ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING Window rowsBetween Long MinValue  Long MaxValue   code   I think using numeric values to indicate relative positions is actually a good idea  but the reliance on Long MinValue and Long MaxValue to indicate unbounded ends is pretty confusing   1  The API is not self evident  There is no way for a new user to figure out how to indicate an unbounded frame by looking at just the API  The user has to read the doc to figure this out  2  It is weird Long MinValue or Long MaxValue has some special meaning  3  Different languages have different min max values  e g  in Python we use  sys maxsize and  sys maxsize    To make this API less confusing  we have a few options    Option 1  Add the following  additional  methods    code     ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING Window rowsBetween  3   3      this one exists already     ROWS BETWEEN UNBOUNDED PRECEDING AND 3 PRECEDING Window rowsBetweenUnboundedPrecedingAnd  3      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW Window rowsBetweenUnboundedPrecedingAndCurrentRow       ROWS BETWEEN CURRENT ROW AND UNBOUNDED PRECEDING Window rowsBetweenCurrentRowAndUnboundedFollowing       ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING Window rowsBetweenUnboundedPrecedingAndUnboundedFollowing    code   This is obviously very verbose  but is very similar to how these functions are done in SQL  and is perhaps the most obvious to end users  especially if they come from SQL background    Option 2  Decouple the specification for frame begin and frame end into two functions  Assume the boundary is unlimited unless specified    code     ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING Window rowsFrom  3  rowsTo 3      ROWS BETWEEN UNBOUNDED PRECEDING AND 3 PRECEDING Window rowsTo  3      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW Window rowsToCurrent   or Window rowsTo 0      ROWS BETWEEN CURRENT ROW AND UNBOUNDED PRECEDING Window rowsFromCurrent   or Window rowsFrom 0      ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING    no need to specify  code   If we go with option 2  we should throw exceptions if users specify multiple from s or to s  A variant of option 2 is to require explicitly specification of begin end even in the case of unbounded boundary  e g     code  Window rowsFromBeginning   rowsTo  3  or Window rowsFromUnboundedPreceding   rowsTo  3   code   ,
SPARK-12877,TrainValidationSplit is missing in pyspark ml tuning,  was investingating progress in SPARK 10759 and I noticed that there is no TrainValidationSplit class in pyspark ml tuning module  Java Scala s examples SPARK 10759 use org apache spark ml tuning TrainValidationSplit that is not available from Python and this blocks SPARK 10759   Does the class have different name in PySpark  maybe  Also  I couldn t find any JIRA task to saying it need to be implemented  Is it by design that the TrainValidationSplit estimator is not ported to PySpark  If not  that is if the estimator needs porting then I would like to contribute ,
SPARK-14594,  error messages for RDD API,  you have an error in your R code using the RDD API  you always get as error message       Error in if  returnStatus    0      argument is of length zero  This is not very useful and I think it might be better to catch the R exception and show it instead ,
SPARK-13904,  support for pluggable cluster manager,  Spark allows only a few cluster managers viz Yarn  Mesos and Standalone  But  as Spark is now being used in newer and different use cases  there is a need for allowing other cluster managers to manage spark components  One such use case is   embedding spark components like executor and driver inside another process which may be a datastore  This allows colocation of data and processing  Another requirement that stems from such a use case is that the executors driver should not take the parent process down when they go down and the components can be relaunched inside the same process again    So  this JIRA requests two functionalities  1  Support for external cluster managers 2  Allow a cluster manager to clean up the tasks without taking the parent process down  ,
SPARK-19409,  Parquet to 1 8 2,  Parquet 1 8 2 is released officially last week on 26 Jan  This issue aims to bump Parquet version to 1 8 2 since it includes many fixes   https   lists apache org thread html af0c813f1419899289a336d96ec02b3bbeecaea23aa6ef69f435c142  3Cdev parquet apache org 3E,
SPARK-13449,  Bayes wrapper in SparkR,  SPARK 13011  we can add a wrapper for naive Bayes in SparkR  R s naive Bayes implementation is from package e1071 with signature    code     S3 method for class  formula  naiveBayes formula  data  laplace   0       subset  na action   na pass     Default S3 method  which we don t want to support   naiveBayes x  y  laplace   0            S3 method for class  naiveBayes  predict object  newdata    type   c  class    raw    threshold   0 001  eps   0        code   It should be easy for us to match the parameters ,
SPARK-12354,  unhandledFilter interface,  discussed here https   github com apache spark pull 10221  it might need to implement   unhandledFilter   to remove duplicated Spark side filtering   ,
SPARK-14275,Reimplement TypedAggregateExpression to DeclarativeAggregate,,
SPARK-13696,  BlockStore interface to more cleanly reflect different memory and disk store responsibilities,   both the MemoryStore and DiskStore implement a common BlockStore API  but I feel that this API is inappropriate because it abstracts away important distinctions between the behavior of these two stores   For instance  the disk store doesn t have a notion of storing deserialized objects  so it s confusing for it to expose object based APIs like putIterator   and getValues   instead of only exposing binary APIs and pushing the responsibilities of serialization and deserialization to the client   As part of a larger BlockManager interface cleanup  I d like to remove the BlockStore API and refine the MemoryStore and DiskStore interfaces to reflect more narrow sets of responsibilities for those components ,
SPARK-12817,  CacheManager and replace it with new BlockManager getOrElseUpdate method,CacheManager directly calls MemoryStore unrollSafely   and has its own logic for handling graceful fallback to disk when cached data does not fit in memory  However  this logic also exists inside of the MemoryStore itself  so this appears to be unnecessary duplication   Thanks to the addition of block level read write locks  we can refactor the code to remove the CacheManager and replace it with an atomic getOrElseUpdate BlockManager method  ,
SPARK-4632,  MQTT dependency to use mqtt client 1 0 1,  client 0 4 0 was removed from the Eclipse Paho repository  and hence is breaking Spark build ,
SPARK-13466,  redundant project in colum pruning rule,  column pruning rule in optimizer  we will introduce redundant project for some cases  We should prevent it ,
SPARK-11324,  to close Write Ahead Log after writing,  the Write Ahead Log in Spark Streaming flushes data as writes need to be made  S3 does not support flushing of data  data is written once the stream is actually closed    In case of failure  the data for the last minute  default rolling interval  will not be properly written  Therefore we need a flag to close the stream after the write  so that we achieve read after write consistency ,
SPARK-12025,  some window rank function names for SparkR,  cumeDist    cume dist  denseRank    dense rank  percentRank    percent rank  rowNumber    row number  There are two reasons that we should make this change    We should follow the naming convention rule of R  http   www inside r org node 230645     Spark DataFrame has deprecated the old convention  such as cumeDist  and will remove it in Spark 2 0   It s better to fix this issue before 1 6 release  otherwise we will make breaking API change ,
SPARK-18467,  StaticInvoke  Invoke and NewInstance ,    StaticInvoke      Invoke   and   NewInstance   as     Introduce   InvokeLike   to extract common logic from   StaticInvoke      Invoke   and   NewInstance   to prepare arguments    Remove unneeded null checking and fix nullability of   NewInstance      Modify to short circuit if arguments have   null   when   propageteNull    true    ,
SPARK-14324,  GLMs code in SparkRWrappers,  use a single object  SparkRWrappers   https   github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala  to wrap method calls to glm and kmeans in SparkR  This is quite hard to maintain  We should refactor them into separate wrappers  like  AFTSurvivalRegressionWrapper  and  NaiveBayesWrapper    The package name should be  spakr ml r  instead of  spark ml api r  ,
SPARK-13763,  Project when its projectList is Empty,  are using  SELECT 1  as a dummy table  when the table is used for SQL statements in which a table reference is required  but the contents of the table are not important  For example    code  SELECT pageid  adid FROM  SELECT 1  dummyTable LATERAL VIEW explode adid list  adTable AS adid   code   In this case  we will see a useless Project whose projectList is empty after executing ColumnPruning rule  ,
SPARK-10446,  to specify join type when calling join with usingColumns,  the method join right  DataFrame  usingColumns  Seq String   only supports inner join  It is more convenient to have it support other join types ,
SPARK-1996,  use of special Maven repo for Akka,  to http   doc akka io docs akka 2 3 3 intro getting started html Akka is now published to Maven Central  so our documentation and POM files don t need to use the old Akka repo  It will be one less step for users to worry about ,
SPARK-16812,  up SparkILoop getAddedJars,  getAddedJars is a useful method to use so we can programmatically get the list of jars added   ,
SPARK-16964,  private sql  and private spark  from sql execution package,  execution package is meant to be internal  and as a result it does not make sense to mark things as private sql  or private spark   It simply makes debugging harder when Spark developers need to inspect the plans at runtime  ,
SPARK-14277,Significant amount of CPU is being consumed in SnappyNative arrayCopy method,  running a Spark job which is spilling a lot of data in reduce phase  we see that significant amount of CPU is being consumed in native Snappy ArrayCopy method  Please see the stack trace below     Stack trace    org xerial snappy SnappyNative   YJP  arrayCopy Native Method  org xerial snappy SnappyNative arrayCopy SnappyNative java  org xerial snappy Snappy arrayCopy Snappy java 85  org xerial snappy SnappyInputStream rawRead SnappyInputStream java 190  org xerial snappy SnappyInputStream read SnappyInputStream java 163  java io DataInputStream readFully DataInputStream java 195  java io DataInputStream readLong DataInputStream java 416  org apache spark util collection unsafe sort UnsafeSorterSpillReader loadNext UnsafeSorterSpillReader java 71  org apache spark util collection unsafe sort UnsafeSorterSpillMerger 2 loadNext UnsafeSorterSpillMerger java 79  org apache spark sql execution UnsafeExternalRowSorter 1 next UnsafeExternalRowSorter java 136  org apache spark sql execution UnsafeExternalRowSorter 1 next UnsafeExternalRowSorter java 123   The reason for that is the SpillReader does a lot of small reads from the underlying snappy compressed stream and SnappyInputStream invokes native jni ArrayCopy method to copy the data  which is expensive  We should fix Snappy  java to  use with non JNI based System arrayCopy method in this case    ,
SPARK-12301,  final from classes in spark ml trees and ensembles where possible,  have been continuing requests  e g    SPARK 7131   for allowing users to extend and modify MLlib models and algorithms   If you are a user who needs these changes  please comment here about what specifically needs to be modified for your use case ,
SPARK-10691,  Logistic  Linear Regression Model evaluate   method public,  following method in   LogisticRegressionModel   is marked as   private    which prevents users from creating a summary on any given data set  Check  here https   github com feynmanliang spark blob d219fa4c216e8f35b71a26921561104d15cd6055 mllib src main scala org apache spark ml regression LinearRegression scala L272     code     TODO  decide on a good name before exposing to public API private classification  def evaluate dataset  DataFrame    LogisticRegressionSummary         new BinaryLogisticRegressionSummary          this transform dataset              probabilityCol              labelCol      code   This method is definitely necessary to test model performance  By the way  the name   evaluate   is already pretty good for me     mengxr  Could you check this   Thx,
SPARK-10028,  Python API for PrefixSpan,  Python API for mllib fpm PrefixSpan,
SPARK-13414,  support for launching multiple Mesos dispatchers,  the sbin  start stop  mesos dispatcher scripts only assume there is one mesos dispatcher launched  but potentially users that like to run multi tenant dispatcher might want to launch multiples  It also helps local development to have the ability to launch multiple ones ,
SPARK-16584,  regexp unit tests to RegexpExpressionsSuite,,
SPARK-16012,  gapplyCollect   for SparkDataFrame,  a new API method called gapplyCollect   for SparkDataFrame  It does gapply on a SparkDataFrame and collect the result back to R  Compared to gapply     collect    gapplyCollect   offers performance optimization as well as programming convenience  as no schema is needed to be provided   This is similar to dapplyCollect   ,
SPARK-710,  dependency on Twitter4J repository,  maven repository is blocked in China  We should get rid of that dependency so people in China can compile Spark  ,
SPARK-11462,  JavaStreamingListener,  Java friendly API for StreamingListener,
SPARK-11743,  UserDefinedType support to RowEncoder,  doesn t support UserDefinedType now  We should add the support for it ,
SPARK-18138,  officially deprecate support for Python 2 6  Java 7  and Scala 2 10,     Mark it very explicit in Spark 2 1 0 that support for the aforementioned environments are deprecated    Remove support it Spark 2 2 0   Also see mailing list discussion  http   apache spark developers list 1001551 n3 nabble com Straw poll dropping support for things like Scala 2 10 tp19553p19577 html,
SPARK-16159,  RDD creation logic from FileSourceStrategy apply,  embed partitioning logic in FileSourceStrategy apply  making the function very long  This is a small refactoring to move it into its own functions  Eventually we would be able to move the partitioning functions into a physical operator  rather than doing it in physical planning   ,
SPARK-13664,  and Speedup HadoopFSRelation,  majority of Spark SQL queries likely run though   HadoopFSRelation    however there are currently several complexity and performance problems with this code path     The class mixes the concerns of file management  schema reconciliation  scan building  bucketing  partitioning  and writing data     For very large tables  we are broadcasting the entire list of files to every executor   SPARK 11441     For partitioned tables  we always do an extra projection   This results not only in a copy  but undoes much of the performance gains that we are going to get from vectorized reads   This is an umbrella ticket to track a set of improvements to this codepath ,
SPARK-13797,  Unnecessary Window,  the Window does not have any window expression  it is useless  It might happen after column pruning,
SPARK-13957,  group by ordinal in SQL,  is to support order by position in SQL  e g    noformat  select c1  c2  c3  sum    from tbl group by by 1  3  c4  noformat   should be equivalent to   noformat  select c1  c2  c3  sum    from tbl order by c1  c3  c4  noformat   We only convert integer literals  not foldable expressions    For positions that are aggregate functions  an analysis exception should be thrown  e g  in postgres    noformat   rxin   select  one    two   count    from r1 group by 1  3  ERROR   aggregate functions are not allowed in GROUP BY LINE 1  select  one    two   count    from r1 group by 1  3                                  noformat   This should be controlled by config option spark sql groupByOrdinal  ,
SPARK-9553,  the createCode and createStructCode  and replace the usage of them by createStructCode,,
SPARK-16828,  MaxOf and MinOf,,
SPARK-18951,  com thoughtworks paranamer paranamer to 2 6,  recently hit a bug of com thoughtworks paranamer paranamer  which causes jackson fail to handle byte array defined in a case class  Then I find https   github com FasterXML jackson module scala issues 48  which suggests that it is caused by a bug in paranamer  Let s upgrade paranamer    Since we are using jackson 2 6 5 and jackson module paranamer 2 6 5 use com thoughtworks paranamer paranamer 2 6  I suggests that we upgrade paranamer to 2 6  ,
SPARK-14008,  Extend the Vectorized Parquet Reader,    nongli  s suggestions   We should do these things  1  Remove the non vectorized parquet reader code  2  Support the remaining types  just big decimals   3  Move the logic to determine if our parquet reader can be used to planning  Only complex types should fall back to the parquet mr reader ,
SPARK-15623,  0 python coverage ml feature,  parent task SPARK 14813    bryanc  did this component ,
SPARK-14069,  SparkStatusTracker to also track executor information,,
SPARK-17215,   SQLContext parseDataType dataTypeString  String   could be removed ,   SQLContext parseDataType dataTypeString  String   could be removed  we should use  SparkSession parseDataType dataTypeString  String   instead  This require updating PySpark ,
SPARK-18991,  ContextCleaner referenceBuffer to ConcurrentHashMap to make it faster,  now  ContextCleaner referenceBuffer  is ConcurrentLinkedQueue and the time complexity of the  remove  action is O   n    It can be changed to use ConcurrentHashMap whose  remove  is O 1  ,
SPARK-11235,  streaming data using network library,  part of the work to implement SPARK 11140  it would be nice to have the network library efficiently stream data over a connection   Currently all it has is the shuffle data protocol  which is not very efficient for large files  it requires the whole file to be buffered on the receiver side before the receiver can do anything  For large files  that comes at a huge cost in memory  You can chunk large files but that requires the client to ask for each chunk separately   Instead  a similar approach but allowing the data to be processed as it arrives would be a lot more efficient  and make it easier to implement the file server in the referenced bug ,
SPARK-16967,  Mesos support code into a module profile,    mgummelt    tnachen    skonto   I think this is fairly easy and would be beneficial as more work goes into Mesos  It should separate into a module like YARN does  just on principle really  but because it also means anyone that doesn t need Mesos support can build without it   I m entirely willing to take a shot at this ,
SPARK-15625,  0 python converage ml classification module,  parent task SPARK 14813 ,
SPARK-18992,  spark sql hive thriftServer singleSession to SQLConf,    spark sql hive thriftServer singleSession   is a configuration of SQL component  this conf can be moved from   SparkConf   to   StaticSQLConf      When we introduced   spark sql hive thriftServer singleSession    all the SQL configuration can be modified in different sessions  Later  static SQL configuration is added  It is a perfect fit for   spark sql hive thriftServer singleSession    Previously  we did the same move for   spark sql warehouse dir   from   SparkConf   to   StaticSQLConf  ,
SPARK-13215,  fallback in codegen,  newMutableProjection  it will fallback to InterpretedMutableProjection if failed to compile   Since we remove the configuration for codegen  we are heavily reply on codegen  also TungstenAggregate require the generated MutableProjection to update UnsafeRow   should remove the fallback  which could make user confusing  see the discussion in SPARK 13116 ,
SPARK-9700,  default page size more intelligently,   we use 64MB as the default page size  which was way too big for a lot of Spark applications  especially for single node    This patch changes it so that the default page size  if unset by the user  is determined by the number of cores available and the total execution memory available ,
SPARK-6817,  UDFs in R,  depends on some internal interface of Spark SQL  should be done after merging into Spark  ,
SPARK-10447,  pyspark to use py4j 0 9,  was recently released  and it has many improvements  especially the following    quote  Python side  IDEs and interactive interpreters such as IPython can now get help text autocompletion for Java classes  objects  and members  This makes Py4J an ideal tool to explore complex Java APIs  e g   the Eclipse API   Thanks to  jonahkichwacoders  quote   Normally we wrap all the APIs in spark  but for the ones that aren t  this would make it easier to offroad by using the java proxy objects ,
SPARK-18766,  Down Filter Through BatchEvalPython,   when users use Python UDF in Filter    BatchEvalPython   is always generated below   FilterExec    However  not all the predicates need to be evaluated after Python UDF execution  Thus  we can push down the predicates through   BatchEvalPython       noformat      df   spark createDataFrame   1   1     2   2     1   2     1   2       key    value        from pyspark sql functions import udf  col     from pyspark sql types import BooleanType     my filter   udf lambda a  a   2  BooleanType        sel   df select col  key    col  value    filter  my filter col  key        df value    2        sel explain True   noformat    noformat     Physical Plan     Project  key 0L  value 1      Filter   isnotnull value 1     pythonUDF0 9      value 1   2         BatchEvalPython   lambda  key 0L     key 0L  value 1  pythonUDF0 9           Scan ExistingRDD key 0L value 1   noformat  ,
SPARK-10106,   ifelse  Column function to SparkR,  a column function on a DataFrame like  ifelse  in R to SparkR  I guess we could implement it with a combination with   when   and   otherwise     h3  Example  If   df x   0   is TRUE  then return 0  otherwise return 1   noformat  ifelse df x   0  0  1   noformat ,
SPARK-13010,  analysis in SparkR,  a simple wrapper of AFTSurvivalRegression in SparkR to support survival analysis ,
SPARK-9642,LinearRegression should supported weighted data,  many modeling application  data points are not necessarily sampled with equal probabilities  Linear regression should support weighting which account the over or under sampling   ,
SPARK-2397,  rid of LocalHiveContext,HiveLocalContext is nearly completely redundant with HiveContext   We should consider deprecating it and removing all uses ,
SPARK-14733,  custom timing control in microbenchmarks,  current benchmark framework runs a code block for several iterations and reports statistics  However there is no way to exclude per iteration setup time from the overall results ,
SPARK-1110,  up and clarify use of SPARK HOME,  the spirit of SPARK 929 we should clean up the use of SPARK HOME and  if possible  remove it entirely   We need to look through what this is used for  One use was allowing applications to run different versions of Spark in standalone mode  For instance  someone could submit an application with a custom SPARK HOME and the Worker would launch an Executor using a different path for Spark  This use case is not widely used and maybe should just be removed   The existing constructors that take SPARK HOME for this purpose should be deprecated and we should explain that SPARK HOME is no longer used for this purpose   If there are other legitimate reasons for SPARK HOME  we can keep it around    we need to audit the uses of it  ,
SPARK-11358,   runs  in k means, runs  introduces extra complexity and overhead in MLlib s k means implementation  I haven t seen much usage with  runs  not equal to  1   We can deprecate this method in 1 6  and remove or void it in 1 7  It helps us simplify the implementation ,
SPARK-1057,  Fastutil,  simple request  to upgrade fastutil to 6 5 x  The current version  6 4 x  has some minor API s in the Object2xxOpenHashMap structures which is used in many places in Spark  and has been marked deprecated  Plus there is a conflict with another library we are using  saddle    http   saddle github io   which uses a newer version of fastutil   I d be happy to send a PR   I guess a bigger question is do you want to keep using fastutil  SPARK 681  but Spark uses more than just hashmaps  so that probably requires another discussion ,
SPARK-11536,  the internal implicit conversion from Expression to Column in functions scala,,
SPARK-10464,  WeibullGenerator for RandomDataGenerator,  8518 need to generate random data which follow Weibull distribution  ,
SPARK-17868,  not use bitmasks during parsing and analysis of CUBE ROLLUP GROUPING SETS,  generate bitmasks for grouping sets during the parsing process  and use these during analysis  These bitmasks are difficult to work with in practice and have lead to numerous bugs  I suggest that we remove these and use actual sets instead  however we would need to generate these offsets for the grouping id ,
SPARK-8879,  EmptyRow class,  now InternalRow is megamorphic because it has many different implementations  We should work towards having only one or at most two InternalRow implementations ,
SPARK-11488,GroupedData should only keep common first order statistics,  should remove methods for variance  stddev  skewness      ,
SPARK-15406,  streaming support for consuming from Kafka,  is the parent JIRA to track all the work for the building a Kafka source for Structured Streaming  Here is the design doc for an initial version of the Kafka Source  https   docs google com document d 19t2rWe51x7tq2e5AOfrsM9qb8 m7BRuv9fel9i0PqR8 edit usp sharing                      Old description                           Structured streaming doesn t have support for kafka yet   I personally feel like time based indexing would make for a much better interface  but it s been pushed back to kafka 0 10 1  https   cwiki apache org confluence display KAFKA KIP 33   Add a time based log index,
SPARK-9922,  StringIndexerInverse to IndexToString,  StringIndexerInverse does is not strictly associated with StringIndexer  and the name is not super clear ,
SPARK-10371,  sequential projections,  ML pipelines  each transformer estimator appends new columns to the input DataFrame  For example  it might produce DataFrames like the following columns  a  b  c  d  where a is from raw input  b   udf b a   c   udf c b   and d   udf d c   Some UDFs could be expensive  However  if we materialize c and d  udf b  and udf c are triggered twice  i e   value c is not re used   It would be nice to detect this pattern and re use intermediate values    code  val input   sqlContext range 10  val output   input withColumn  x   col  id     1  withColumn  y   col  x     2  output explain true      Parsed Logical Plan     Project      x   2  AS y 254   Project  id 252L  id 252L   cast 1 as bigint   AS x 253L    LogicalRDD  id 252L   MapPartitionsRDD 458  at range at  console  30     Analyzed Logical Plan    id  bigint  x  bigint  y  bigint Project  id 252L x 253L  x 253L   cast 2 as bigint   AS y 254L   Project  id 252L  id 252L   cast 1 as bigint   AS x 253L    LogicalRDD  id 252L   MapPartitionsRDD 458  at range at  console  30     Optimized Logical Plan    Project  id 252L  id 252L   1  AS x 253L   id 252L   1    2  AS y 254L   LogicalRDD  id 252L   MapPartitionsRDD 458  at range at  console  30     Physical Plan    TungstenProject  id 252L  id 252L   1  AS x 253L   id 252L   1    2  AS y 254L   Scan PhysicalRDD id 252L   Code Generation  true input  org apache spark sql DataFrame    id  bigint  output  org apache spark sql DataFrame    id  bigint  x  bigint  y  bigint   code ,
SPARK-15644,  SQLContext with SparkSession in MLlib,  the latest Sparksession to replace the existing SQLContext in MLlib,
SPARK-16461,  partition batch pruning with        EqualNullSafe  predicate in InMemoryTableScanExec,  seems  EqualNullSafe  filter was missed for batch pruneing partitions in cached tables   Supporting this improve the performance roughly  75   it will vary    Running the codes below    code  test  Null safe equal comparison       val N   20000000   val df   spark range N  repartition 20    val benchmark   new Benchmark  Null safe equal comparison   N    df createOrReplaceTempView  t     spark catalog cacheTable  t     sql  select id from t where id     1   collect      benchmark addCase  Null safe equal comparison   10             sql  select id from t where id     1   collect         benchmark run      code    produces the results below   Before    code  Running benchmark  Null safe equal comparison   Running case  Null safe equal comparison   Stopped after 10 iterations  2098 ms  Java HotSpot TM  64 Bit Server VM 1 8 0 45 b14 on Mac OS X 10 11 5 Intel R  Core TM  i7 4850HQ CPU   2 30GHz  Null safe equal comparison               Best Avg Time ms     Rate M s    Per Row ns    Relative                                                                                                  Null safe equal comparison                     204    210         98 1          10 2       1 0X  code   After   code  Running benchmark  Null safe equal comparison   Running case  Null safe equal comparison   Stopped after 10 iterations  478 ms  Java HotSpot TM  64 Bit Server VM 1 8 0 45 b14 on Mac OS X 10 11 5 Intel R  Core TM  i7 4850HQ CPU   2 30GHz  Null safe equal comparison               Best Avg Time ms     Rate M s    Per Row ns    Relative                                                                                                  Null safe equal comparison                      42     48        474 1           2 1       1 0X  code ,
SPARK-10491,  RowMatrix dspr to BLAS,  implemented dspr with sparse vector support in  RowMatrix   This method is also used in WeightedLeastSquares and other places  It would be useful to move it to  linalg BLAS  ,
SPARK-17798,  redundant Experimental annotations in sql streaming package,,
SPARK-5540,  ALS solveLeastSquares ,  method survived the code review and it has been there since v1 1 0  It exposes jblas types  Let s remove it from the public API  I expect that no one calls it directly ,
SPARK-16934,  LogisticCostAggregator serialization code to make it consistent with LinearRegression,  LogisticCostAggregator serialization code to make it consistent with LinearRegression,
SPARK-12287,  UnsafeRow in MapPartitions MapGroups CoGroup,,
SPARK-10076,  MultilayerPerceptronClassifier layers and weights public ,  MultilayerPerceptronClassifier layers and weights public ,
SPARK-12475,  Zinc from 0 3 5 3 to 0 3 9,  should update to the latest version of Zinc in order to match our SBT version ,
SPARK-12599,  the use of the deprecated callUDF in MLlib,  s Transformer uses the deprecated callUDF API ,
SPARK-13322,AFTSurvivalRegression should support feature standardization,  bug is reported by Stuti Awasthi  https   www mail archive com user spark apache org msg45643 html The lossSum has possibility of infinity because we do not standardize the feature before fitting model  we should support feature standardization  Another benefit is that standardization will improve the convergence rate  ,
SPARK-11766,  serialization of Vectors,  want to support JSON serialization of vectors in order to support SPARK 11764 ,
SPARK-12293,  UnsafeRow in LocalTableScan,,
SPARK-18124,  delay based event time watermarks,  we aggregate data by event time  we want to consider data is late and out of order in terms of its event time  Since we keep aggregate keyed by the time as state  the state will grow unbounded if we keep around all old aggregates in an attempt consider arbitrarily late data  Since the state is a store in memory  we have to prevent building up of this unbounded state  Hence  we need a watermarking mechanism by which we will mark data that is older beyond a threshold as  too late   and stop updating the aggregates with them  This would allow us to remove old aggregates that are never going to be updated  thus bounding the size of the state   Here is the design doc   https   docs google com document d 1z Pazs5v4rA31azvmYhu4I5xwqaNQl6ZLIS03xhkfCQ edit usp sharing,
SPARK-17317,  package vignette to SparkR,  publishing SparkR to CRAN  it would be nice to have a vignette as a user guide that   describes the big picture   introduces the use of various methods  This is important for new users because they may not even know which method to look up ,
SPARK-15171,  registerTempTable and add dataset createTempView,  current dataset registerTempTable does not actually materialize data  So  it should be considered as creating a temp view  We can deprecate it and create a new method called dataset createTempView replaceIfExists  Boolean   The default value of replaceIfExists should be false  For registerTempTable  it will call dataset createTempView replaceIfExists   true  ,
SPARK-12337,  dropDuplicates   method of DataFrame in SparkR,    and unique   drop duplicated rows on all columns  While dropDuplicates   can drop duplicated rows on selected columns ,
SPARK-16516,  for pushing down filters for decimal and timestamp types in ORC,  filters for   TimestampType   and   DecimalType   are not being pushed down in ORC data source although ORC filters support both  ,
SPARK-11926,  GetStructField and GetInternalRowField,,
SPARK-11773,  collection functions in SparkR,  functions documented at http   spark apache org docs latest api scala index html org apache spark sql functions  are size    explode    array contains   and sort array    size    explode   are already implemented  array contains   and sort array   are to be implemented ,
SPARK-13921,  serialized blocks as multiple chunks in MemoryStore,  of storing serialized blocks in individual ByteBuffers  the BlockManager should be capable of storing a serialized block in multiple chunks  each occupying a separate ByteBuffer   This change will help to improve the efficiency of memory allocation and the accuracy of memory accounting when serializing blocks  Our current serialization code uses a   ByteBufferOutputStream    which doubles and re allocates its backing byte array  this increases the peak memory requirements during serialization  since we need to hold extra memory while expanding the array   In addition  we currently don t account for the extra wasted space at the end of the ByteBuffer s backing array  so a 129 megabyte serialized block may actually consume 256 megabytes of memory  After switching to storing blocks in multiple chunks  we ll be able to efficiently trim the backing buffers so that no space is wasted   This change is also a prerequisite to being able to cache blocks which are larger than 2GB  although full support for that depends on several other changes which have not bee implemented yet  ,
SPARK-16675,  per record type dispatch in JDBC when writing,  is similar with https   issues apache org jira browse SPARK 16351   Currently    JdbcUtils savePartition   is doing type based dispatch for each row to write appropriate values   So  appropriate writers can be created first according to the schema  and then apply them to each row  This approach is similar with   CatalystWriteSupport   ,
SPARK-16259,  options for DataFrame reader API in Python,  are some duplicated code for options  we should simplify them ,
SPARK-18862,  SparkR mllib R into multiple files,  mllib R is getting bigger as we add more ML wrappers  I d like to split it into multiple files to make us easy to maintain    mllibClassification R   mllibRegression R   mllibClustering R   mllibFeature R  or     mllib classification R   mllib regression R   mllib clustering R   mllib features R  For R convention  it s more prefer the first way  And I m not sure whether R supports the second organized way  will check later   Please let me know your preference  I think the start of a new release cycle is a good opportunity to do this  since it will involves less conflicts  If this proposal was approved  I can work on it   cc   felixcheung    josephkb    mengxr  ,
SPARK-15603,  SQLContext with SparkSession in ML MLLib,  issue replaces all deprecated  SQLContext  occurrences with  SparkSession  in  ML MLLib  module except the following two classes  These two classes use  SQLContext  as their function arguments     ReadWrite scala   TreeModels scala,
SPARK-16031,  debug only socket source in Structured Streaming,  is a debug only version of SPARK 15842  for tutorials and debugging of streaming apps  it would be nice to have a text based socket source similar to the one in Spark Streaming  It will clearly be marked as debug only so that users don t try to run it in production applications  because this type of source cannot provide HA without storing a lot of state in Spark ,
SPARK-17351,  JDBCRDD to expose JDBC    SparkSQL conversion functionality,  would be useful if more of JDBCRDD s JDBC    Spark SQL functionality was usable from outside of JDBCRDD  this would make it easier to write test harnesses comparing Spark output against other JDBC databases  ,
