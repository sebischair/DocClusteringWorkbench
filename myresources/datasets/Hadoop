HADOOP-11698,  DistCpV1 and Logalyzer,  is pretty much unsupported  we should just remove it ,
HADOOP-11139,  user to choose JVM for container execution,  currently supports one JVM defined through JAVA HOME   Since multiple JVMs  Java 6 7 8 9  are active  it will be helpful if there is an user configuration to choose the custom but supported JVM for her job   In other words  user will be able to choose her expected JVM only for her container execution while Hadoop services may be running on different JVM ,
HADOOP-12579,  WriteableRPCEngine,    WriteableRPCEninge   depends on Java s serialization mechanisms for RPC requests  Without proper checks  it has be shown that it can lead to security vulnerabilities such as remote code execution  e g   COLLECTIONS 580  HADOOP 12577    The current implementation has migrated from   WriteableRPCEngine   to   ProtobufRPCEngine   now  This jira proposes to deprecate   WriteableRPCEngine   in branch 2 and to remove it in trunk  ,
HADOOP-12651,  dev support with wrappers to Yetus,  that Yetus has had a release  we should rip out the components that make it up from dev support and replace them with wrappers   The wrappers should    default to a sane version   allow for version overrides via an env var   download into patchprocess   execute with the given parameters  Marking this as an incompatible change  since we should also remove the filename extensions and move these into a bin directory for better maintenance towards the future ,
HADOOP-3228,  Text hashcode with a better hash function for non ascii strings,  we change the hash function for Text to something that handles non ascii characters better   http   bailey svn sourceforge net viewvc bailey trunk src java org apache bailey util Hash java view markup,
HADOOP-1831,  should kill long running tests,  should kill long running tests   I believe it is supposed to but doesn t quite seem to do the job if the test is really hung up    It would be nice if  when the timer goes off  Hudson did a  code kill  QUIT code   to try to get a thread dump  and then followed that with a  code kill  9 code    See the section  Killing a hung test  at http   wiki apache org lucene hadoop HudsonBuildServer  ,
HADOOP-2082,randomwriter should complain if there are too many arguments,  user was moving from 0 13 to 0 14 and was invoking randomwriter with a config on the command line like   bin hadoop jar hadoop   examples jar randomwriter output conf xml  which worked in 0 13  but in 0 14 it ignores the conf xml without complaining  The equivalent is   bin hadoop jar hadoop   examples jar randomwriter  conf conf xml output ,
HADOOP-9737,  getJar should delete the jar file upon destruction of the JVM,    JarFinder getJar     is invoked by a client app  it would be really useful to destroy the generated JAR after the JVM is destroyed by setting   tempJar deleteOnExit      In order to preserve backwards compatibility a configuration setting could be implemented  e g    test build dir purge on exit    ,
HADOOP-12985,  MetricsSource interface for DecayRpcScheduler Metrics,  allows metrics collector such as AMS to collect it with MetricsSink  The per user RPC call counts  schedule decisions and per priority response time will be useful to detect and trouble shoot Hadoop RPC server such as Namenode overload issues  ,
HADOOP-12782,  LDAP group name resolution with ActiveDirectory,  typical LDAP group name resolution works well under typical scenarios  However  we have seen cases where a user is mapped to many groups  in an extreme case  a user is mapped to more than 100 groups   The way it s being implemented now makes this case super slow resolving groups from ActiveDirectory   The current LDAP group resolution implementation sends two queries to a ActiveDirectory server  The first query returns a user object  which contains DN  distinguished name   The second query looks for groups where the user DN is a member  If a user is mapped to many groups  the second query returns all group objects associated with the user  and is thus very slow   After studying a user object in ActiveDirectory  I found a user object actually contains a  memberOf  field  which is the DN of all group objects where the user belongs to  Assuming that an organization has no recursive group relation  that is  a user A is a member of group G1  and group G1 is a member of group G2   we can use this properties to avoid the second query  which can potentially run very slow   I propose that we add a configuration to only enable this feature for users who want to reduce group resolution time and who does not have recursive groups  so that existing behavior will not be broken ,
HADOOP-6470,  Context for Metrics,  way metrics are currently exposed to the JMX in the NameNode is not helpful  since only the current counters in the record can be fetched and without any context those number mean little  For example the number of files created equal to 150 only means that in the last period there were 150 files created but when the new period will end is unknown so fetching 150 again will either mean another 150 files or we are fetching the same time period   One of the solutions for this problem will be to have a JMX context that will accumulate the data  being child class of AbstractMetricsContext  and expose different records to the JMX through custom MBeans  This way the information fetched from the JMX will represent the state of things in a more meaningful way ,
HADOOP-12562,  hadoop dockerfile usable by Yetus,  would be better if Hadoop s dockerfile could be used by Yetus so that external dependencies are owned by the project ,
HADOOP-12059,S3Credentials should support use of CredentialProvider,  now S3Credentials only works with cleartext passwords in configs  as a secret access key or the URI   The non URI version should use credential providers with a fallback to the clear text option ,
HADOOP-13681,  Kafka dependencies in hadoop kafka module,  newly added Kafka module defines the Kafka dependency as   noformat       groupId org apache kafka  groupId   artifactId kafka 2 10  artifactId   version   kafka version   version       noformat   This is unfavorable because its using the server dependency  which transitively has the client jars  The server dependency includes all of the server code and some larger transitive dependencies like Scala and Zookeeper   Instead the pom file should be changed to only depend on the clients jar which is a much smaller footprint   noformat       groupId org apache kafka  groupId   artifactId kafka clients  artifactId   version   kafka version   version       noformat    ,
HADOOP-7209,  to FsShell,  project  Pig  exposes FsShell functionality to our end users through a shell command  We want to use this command with no modifications to make sure that whether you work with HDFS through Hadoop or Pig you get identical semantics   The main concern that has been recently raised by our users is that there is no way to ignore certain failures that they consider to be benign  for instance  removing a non existent directory   We have 2 asks related to this issue    1  Meaningful error code returned from FsShell  we use java class  so that we can take different actions on different errors  2  Unix like ways to tell the command to ignore certain behavior  Here are the commands that we would like to be expanded implemented       rm  f      rmdir    ignore fail on non empty      mkdir  p ,
HADOOP-12366,  calculated paths,  would be useful for 3rd party apps to know the locations of things when hadoop is running without explicit path env vars set ,
HADOOP-12767,  apache httpclient version to 4 5 2  httpcore to 4 4 4,  SSL security fixes are needed   See   CVE 2012 6153  CVE 2011 4461  CVE 2014 3577  CVE 2015 5262 ,
HADOOP-8147,  patch should run tests with  fn to avoid masking test failures,  there are known failures  test patch will bail out as soon as it sees them   This causes the precommit builds to potentially not find real issues with a patch  because the tests that would fail might come after a known failure   We should add  fn to just the mvn test command in test patch to get the full list of failures ,
HADOOP-10776,  up already widely used APIs for delegation token fetching   renewal to ecosystem projects,  would like to be able to fetch delegation tokens and forward them on to running topologies so that they can access HDFS  STORM 346    But to do so we need to open up access to some of APIs    Most notably FileSystem addDelegationTokens    Token renew  Credentials getAllTokens  and UserGroupInformation but there may be others   At a minimum adding in storm to the list of allowed API users  But ideally making them public  Restricting access to such important functionality to just MR really makes secure HDFS inaccessible to anything except MR  or tools that reuse MR input formats ,
HADOOP-13782,  MutableRates metrics thread local write  aggregate on read,  the   MutableRates   metrics class serializes all writes to metrics it contains because of its use of   MetricsRegistry add      i e   even two increments of unrelated metrics contained within the same   MutableRates   object will serialize w r t  each other   This class is used by   RpcDetailedMetrics    which may have many hundreds of threads contending to modify these metrics  Instead we should allow updates to unrelated metrics objects to happen concurrently  To do so we can let each thread locally collect metrics  and on a   snapshot    aggregate the metrics from all of the threads    I have collected some benchmark performance numbers in HADOOP 13747  https   issues apache org jira secure attachment 12835043 benchmark results  which indicate that this can bring significantly higher performance in high contention situations  ,
HADOOP-1713,  a utility to convert binary  sequence and compressed  files to strings ,  would be nice to have a utility that looked at the first N bytes of a file and picked a decoder for it into Strings  It could then be hooked up to  bin hadoop fs cat  and the web ui to textify sequence and compressed files ,
HADOOP-10787,  remove non HADOOP    etc from the shell scripts,  should make an effort to clean up the shell env var name space by removing unsafe variables   See comments for list ,
HADOOP-13737,  DiskChecker interface,  DiskChecker class has a few unused public methods  We can remove them ,
HADOOP-10949,  sink plugin for Apache Kafka,  a metrics2 sink plugin for Hadoop to send metrics directly to Apache Kafka in addition to the current  Graphite   Hadoop 9704 https   issues apache org jira browse HADOOP 9704    Ganglia and File sinks ,
HADOOP-12866,  a subcommand for gridmix,  shouldn t require a raw java command line to run ,
HADOOP-6964,  compact property description in xml,  should allow users to use the more compact form of xml elements  For example  we could allow   noformat   property name  mapred local dir  value   disk 0 mapred  disk 1 mapred     noformat  The old format would also be supported ,
HADOOP-6629,  of dependencies should be specified in a single place,  the Maven POM file is generated from a template file that includes the versions of all the libraries we depend on   The versions of these libraries are also present in ivy libraries properties  so that  when a library is updated  it must be updated in two places  which is error prone   We should instead only specify library versions in a single place ,
HADOOP-7224,  command factory to FsShell,  FsShell has many chains if then else chains for instantiating and running commands   A dynamic mechanism is needed for registering commands such that FsShell requires no changes when adding new commands ,
HADOOP-7624,  things up for a top level hadoop tools module,  this thread  http   markmail org thread cxtz3i6lvztfgfxn  We need to get things up and running for a top level hadoop tools module  DistCpV2 will be the first resident of this new home  Things we need     The module itself and a top level pom with appropriate dependencies    Integration with the patch builds for the new module    Integration with the post commit and nightly builds for the new module ,
HADOOP-13226,  async call retry and failover,  current Async DFS implementation  file system calls are invoked and returns Future    immediately to clients  Clients call Future get to retrieve final results  Future get internally invokes a chain of callbacks residing in ClientNamenodeProtocolTranslatorPB  ProtobufRpcEngine and ipc Client  The callback path bypasses the original retry layer logic designed for synchronous DFS  This proposes refactoring to make retry also works for Async DFS ,
HADOOP-8885,  to add fs shim to use QFS,  has released QFS 1 0  http   quantcast github com qfs   a C   distributed filesystem based on Kosmos File System KFS   QFS comes with various feature  performance  and stability improvements over KFS   A hadoop  fs  shim needs be added to support QFS through  qfs     URIs ,
HADOOP-13220,  on fixups after upgraded mini kdc using Kerby,  is a  follow up jira from HADOOP 12911  1  Now with the findbug warning   noformat  org apache hadoop minikdc MiniKdc stop   calls Thread sleep   with a lock held At MiniKdc java lock held At MiniKdc java  line 345    noformat  As discussed in HADOOP 12911  bq  Why was this committed with a findbugs errors rather than adding the necessary plumbing in pom xml to make it go away  we will add the findbugsExcludeFile xml and will get rid of this given kerby 1 0 0 rc3 release  2  Add the kerby version hadoop project pom xml bq  hadoop project pom xml contains the dependencies of all libraries used in all modules of hadoop  under dependencyManagement  Only here version will be mentioned  All other Hadoop Modules will inherit hadoop project  so all submodules will use the same version  In submodule  version need not be mentioned in pom xml  This will make version management easier ,
HADOOP-13709,  to clean up subprocesses spawned by Shell when the process exits,  runCommand code in Shell java can get into a situation where it will ignore InterruptedExceptions and refuse to shutdown due to being in I O waiting for the return value of the subprocess that was spawned  We need to allow for the subprocess to be interrupted and killed when the shell process gets killed  Currently the JVM will shutdown and all of the subprocesses will be orphaned and not killed ,
HADOOP-5102,  build script for building core  hdfs and mapred separately,,
HADOOP-13660,  commons configuration version to 2 1,  re currently pulling in version 1 6   I think we should upgrade to the latest 1 10  ,
HADOOP-7266,  metrics v1,,
HADOOP-12930, Umbrella  Dynamic subcommands for hadoop shell scripts,  for converting hadoop  hdfs  mapred  and yarn to allow for dynamic subcommands  See first comment for more details ,
HADOOP-12185,NetworkTopology is not efficient adding getting removing nodes,NetworkToplogy uses nodes with a list of children  The access to these children is slow as it s a linear search ,
HADOOP-6080,  of  Trash with quota,  with quota turned on  user cannot call   rmr  on large directory that causes over quota    noformat   knoguchi src   hadoop dfs  rmr  tmp net2 rmr  Failed to move to trash  hdfs   abc def com tmp net2  knoguchi src   hadoop dfs  mv  tmp net2  user knoguchi  Trash Current mv  org apache hadoop hdfs protocol QuotaExceededException  The quota of  user knoguchi is exceeded  namespace quota 37500 file count 37757  diskspace quota  1 diskspace 1991250043353  noformat   Besides from error message being unfriendly  how should this be handled  ,
HADOOP-7166,DaemonFactory should be moved from HDFS to common,DaemonFactory class is defined in hdfs util  common would be a better place for this class ,
HADOOP-13272,ViewFileSystem should support storage policy related API,    ViewFileSystem   does not support storage policy related API  it will throw   UnsupportedOperationException    ,
HADOOP-11346,  sls rumen to use new shell framework,,
HADOOP-12625,  a config to disable the  logs endpoints,  should add a config to disable the  logs endpoint in HttpServer2  Listing a directory like this can be dangerous from a security perspective   We can keep it enabled by default for compatibility though ,
HADOOP-11515,  jsch lib to jsch 0 1 51 to avoid problems running on java7,  had an application sitting on top of Hadoop and got problems using jsch once we switched to java 7  Got this exception   noformat   com jcraft jsch JSchException  verify  false 	at com jcraft jsch Session connect Session java 330  	at com jcraft jsch Session connect Session java 183   noformat   Upgrading to jsch 0 1 51 from jsch 0 1 49 fixed the issue for us  but then it got in conflict with hadoop s jsch version  we fixed this for us by jarjar ing our jsch version    So i think jsch got introduce by namenode HA  HDFS 1623   So you guys should check if the ssh part is properly working for java7 or preventively upgrade the jsch lib to jsch 0 1 51   Some references to problems reported    http   sourceforge net p jsch mailman jsch users thread loom 20131009T211650 749 post gmane org    https   issues apache org bugzilla show bug cgi id 53437,
HADOOP-13673,  scripts to be smarter when running with privilege,  work continues on HADOOP 13397  it s become evident that we need better hooks to start daemons as specifically configured users   Via the  command   subcommand  USER environment variables in 3 x  we actually have a standardized way to do that   This in turn means we can make the sbin scripts super functional with a bit of updating     Consolidate start dfs sh and start secure dns sh into one script   Make start    sh and stop    sh know how to switch users when run as root   Undeprecate start stop all sh so that it could be used as root for production purposes and as a single user for non production users ,
HADOOP-11090, Umbrella  Support Java 8 in Hadoop,  8 is coming quickly to various clusters  Making sure Hadoop seamlessly works  with Java 8 is important for the Apache community     This JIRA is to track  the issues experiences encountered during Java 8 migration  If you find a potential bug   please create a separate JIRA either as a sub task or linked into this JIRA  If you find a Hadoop or JVM configuration tuning  you can create a JIRA as well  Or you can add  a comment  here  ,
HADOOP-7295,  a test patch script for Hudson,  should create a script that Hudson uses to execute test patch that is in source control so modifications to test patch sh arguments can be done w o updating Hudson   The script would execute the following  and take just the password as an argument    noformat    ANT HOME  bin ant            Dpatch file foobar            Dscratch dir   WORKSPACE  patchprocess            Dsupport dir  homes hudson buildSupport             Dps cmd  bin ps            Dwget cmd  usr bin wget            Djiracli cmd  homes hudson tools jiracli 1 5 jira            Dsvn cmd  usr bin svn            Dgrep cmd  bin grep            Dpatch cmd  usr bin patch            Dfindbugs home  homes hudson tools findbugs latest            Dforrest home  homes hudson tools forrest latest            Declipse home  homes hudson tools eclipse latest            Dpython home  homes hudson tools python latest            Djira passwd                     Dcurl cmd  usr bin curl            Ddefect HADOOP   ISSUE NUM            hudson test patch  noformat ,
HADOOP-11203,  ditscp to accept bandwitdh in fraction MegaBytes,  uses ThrottleInputStream  which  provides a bandwidth throttling on a specified stream  Currently  Distcp allows the max bandwidth value in Mega Bytes  which does not accept fractional values  It would be better if it accepts the Max Bandwitdh in fractional MegaBytes   Due to this we are not able to throttle the bandwidth in KBs in our prod setup ,
HADOOP-2987,  two generations of fsimage,  to verify the fsimage each time it creates the new one ,
HADOOP-12957,  the number of outstanding async calls,  async RPC  if the callers don t read replies fast enough  the buffer storing replies could be used up  This is to propose limiting the number of outstanding async calls to eliminate the issue ,
HADOOP-4463,  classes for Archive and ChukwaRecord files,  some utility classes to dump both Archive and ChukwaRecords files,
HADOOP-13590,  until TGT expires even if the UGI renewal thread encountered exception,  UGI has a background thread to renew the tgt  On exception  it  terminates itself https   github com apache hadoop blob bee9f57f5ca9f037ade932c6fd01b0dad47a1296 hadoop common project hadoop common src main java org apache hadoop security UserGroupInformation java L1013 L1014   If something temporarily goes wrong that results in an IOE  even if it recovered no renewal will be done and client will eventually fail to authenticate  We should retry with our best effort  until tgt expires  in the hope that the error recovers before that  ,
HADOOP-12453,  decoding KMS Delegation Token with its own Identifier,  dt currently does not have its own token identifier class to decode it properly as shown in the HDFS logs below  This JIRA is opened to add support for that    code  2015 09 30 22 36 14 379 beaver machine INFO 5619 140004068153152 MainThread 15 09 30 22 36 14 WARN token Token  Cannot find class for token kind kms dt 2015 09 30 22 36 14 380 beaver machine INFO 5619 140004068153152 MainThread 15 09 30 22 36 14 INFO security TokenCache  Got dt for hdfs   tde hdfs 3 novalocal 8020  Kind  kms dt  Service  172 22 64 179 9292  Ident  00 06 68 72 74 5f 71 61 02 72 6d 00 8a 01 50 20 66 1c a3 8a 01 50 44 72 a0 a3 0f 03  code ,
HADOOP-12691,  CSRF Filter for REST APIs to Hadoop Common,  prevention for REST APIs can be provided through a common servlet filter  This filter would check for the existence of an expected  configurable  HTTP header   such as X XSRF Header   The fact that CSRF attacks are entirely browser based means that the above approach can ensure that requests are coming from either  applications served by the same origin as the REST API or that there is explicit policy configuration that allows the setting of a header on XmlHttpRequest from another origin ,
HADOOP-7937,  port SequenceFile syncFs and friends from Hadoop 1 x,  200 added a new public API SequenceFile syncFs  we need to forward port this for compatibility  Looks like it might have introduced other APIs that need forward porting as well  eg LocaltedBlocks setFileLength  and DataNode getBlockInfo  ,
HADOOP-12668,  excluding weak Ciphers in HttpServer2 through ssl server xml ,  Embeded jetty Server used across all hadoop services is configured through ssl server xml file from their respective configuration section  However  the SSL TLS protocol being used for this jetty servers can be downgraded to weak cipher suites  This code changes aims to add following functionality  1  Add logic in hadoop common  HttpServer2 java and associated interfaces  to spawn jetty servers with ability to exclude weak cipher suites  I propose we make this though ssl server xml and hence each service can choose to disable specific ciphers  2  Modify DFSUtil java used by HDFS code to supply new parameter ssl server exclude cipher list for hadoop common code  so it can exclude the ciphers supplied through this key ,
HADOOP-12666,  Microsoft Azure Data Lake   as a file system in Hadoop,   Description This JIRA describes a new file system implementation for accessing Microsoft Azure Data Lake Store  ADL  from within Hadoop  This would enable existing Hadoop applications such has MR  HIVE  Hbase etc     to use ADL store as input or output    ADL is ultra high capacity  Optimized for massive throughput with rich management and security features  More details available at https   azure microsoft com en us services data lake store  ,
HADOOP-12635,  Append API support for WASB,  the WASB implementation of the HDFS interface does not support Append API  This JIRA is added to design and implement the Append API support to WASB  The intended support for Append would only support a single writer   ,
HADOOP-11013,  handling should be consolidated  debuggable,  part of HADOOP 9902  java execution across many different shell bits were consolidated down to  effectively  two routines   Prior to calling those two routines  the CLASSPATH is exported   This export should really be getting handled in the exec function and not in the individual shell bits   Additionally  it would be good if there was    code  echo   CLASSPATH     dev null  code   so that bash  x would show the content of the classpath or even a    debug classpath  option that would echo the classpath to the screen prior to java exec to help with debugging  ,
HADOOP-13964,  vestigal templates directories creation,  share hadoop  component  template directories are from when RPM and such were built as part of the build system   that no longer happens and now those files cause more harm than good since they are in the classpath   let s remove them ,
HADOOP-6312,Configuration sends too much data to log4j,Configuration objects send a DEBUG level log message every time they re instantiated  which include a full stack trace  This is more appropriate for TRACE level logging  as it renders other debug logs very hard to read ,
HADOOP-5118,ChukwaAgent controller should retry to register for a longer period but not as frequent as now ,  is watching for ChukwaAgent only once every 5 minutes  so there s no point in retrying more than once every 5 mins   In practice  if the watchdog is not able to automatically restart the agent  it will take more than 20 minutes to get Ops to restart it  Also Ops want us to limit the number of communications between Hadoop and Chukwa  that s why 30 minutes ,
HADOOP-13606,  FS to add a service load metadata file,  a metadata file giving the FS impl of swift  remove the entry from core default xml,
HADOOP-11717,  Redirecting WebSSO behavior with JWT Token in Hadoop Auth,  AltKerberosAuthenticationHandler to provide WebSSO flow for UIs   The actual authentication is done by some external service that the handler will redirect to when there is no hadoop auth cookie and no JWT token found in the incoming request   Using JWT provides a number of benefits     It is not tied to any specific authentication mechanism   so buys us many SSO integrations   It is cryptographically verifiable for determining whether it can be trusted   Checking for expiration allows for a limited lifetime and window for compromised use  This will introduce the use of nimbus jose jwt library for processing  validating and parsing JWT tokens   ,
HADOOP-12873,  MRv1 terms from HttpAuthentication md, noformat HttpAuthentication md  By default Hadoop HTTP web consoles  JobTracker  NameNode  TaskTrackers and DataNodes  allow access without any form of authentication   noformat  We should use ResourceManager and NodeManager instead of JobTracker and TaskTracker ,
HADOOP-1823,  InputFormat for bzip2 files,  gzip  the bzip file format supports splitting   Compression is by blocks  900k by default  and blocks are separated by a synchronization marker  a 48 bit approximation of Pi    This would permit very large compressed files to be split into multiple map tasks  which is not currently possible unless using a Hadoop specific file format ,
HADOOP-7378,  dfs  ls   Do not expand directories  was HDFS 1475 ,  a nutshell  ls needs the ability to list a directory but not its contents   W o  d  it is impossible to list the root directory s owner  permissions  etc   See the original hdfs bug for details ,
HADOOP-8279,  HA  Allow manual failover to be invoked from zkfc ,  8247 introduces a configure flag to prevent potential status inconsistency between zkfc and namenode  by making auto and manual failover mutually exclusive   However  as described in 2 7 2 section of design doc at HDFS 2185  we should allow manual and auto failover co exist  by    adding some rpc interfaces at zkfc   manual failover shall be triggered by haadmin  and handled by zkfc if auto failover is enabled   ,
HADOOP-4581,  test framework,  to test endToEnd chukwa pipeline   From log file to dataSink   From dataSink to demux output,
HADOOP-12776,  getaclstatus call for non acl commands in getfacl ,  getaclstatus call for non acl commands in getfacl ,
HADOOP-13583,Incorporate checkcompatibility script which runs Java API Compliance Checker,  on discussion at YETUS 445  this code can t go there  but it s still very useful for release managers  A similar variant of this script has been used for a while by Apache HBase and Apache Kudu  and IMO JACC output is easier to understand than JDiff ,
HADOOP-11246,  jenkins to Java 7,  hadoop 2 7 will drop the support of Java 6  the jenkins slaves should be compiling code using Java 7 ,
HADOOP-5158,  HDFS space quotas to 0 18,  18 already has quotas for HDFS namespace  HADOOP 3187   HADOOP 3938 implements similar quotas for disk space on HDFS in 0 19  This jira proposes to port HADOOP 3938 to 0 18 4  ,
HADOOP-13161,  JDK7 from Dockerfile,  should slim down the Docker image by removing JDK7 now that trunk no longer supports it ,
HADOOP-11792,  all of the CHANGES txt files,  the commit of HADOOP 11731  the CHANGES txt files are now EOLed   We should remove them ,
HADOOP-13716,  LambdaTestUtils class for tests  fix eventual consistency problem in contract test setup,  make our tests robust against timing problems and eventual consistent stores  we need to do more spin   wait for state   We have some code in   GenericTestUtils waitFor   to await a condition being met  but the predicate it calls doesn t throw exceptions  there s no way for a probe to throw an exception  and all you get is the eventual  timed out  message    We can do better  and in closure ready languages  scala   scalatest  groovy and some slider code  we ve examples to follow  Some of that work has been reimplemented slightly in   S3ATestUtils eventually    I propose adding a class in the test tree    Eventually   to be a successor replacement for these     has an eventually waitfor operation taking a predicate that throws an exception   has an  evaluate  exception which tries to evaluate an answer until the operation stops raising an exception   again  from scalatest    plugin backoff strategies  from Scalatest  lets you do exponential as well as linear    option of adding a special handler to generate the failure exception  e g  run more detailed diagnostics for the exception text  etc     be Java 8 lambda expression friendly   be testable and tested itself     ,
HADOOP-12194,  for incremental generation in the protoc plugin,  protoc maven plugin currently generates new Java classes every time  which means Maven always picks up changed files in the build  It would be better if the protoc plugin only generated new Java classes when the source protoc files change ,
HADOOP-11731,  the changelog and releasenotes,  current way we generate these build artifacts is awful   Plus they are ugly and  in the case of release notes  very hard to pick out what is important  ,
HADOOP-12548,  s3a creds from a Credential Provider,  would be good if we could read s3 creds from a source other than via a java property Hadoop configuration option,
HADOOP-8212,  ActiveStandbyElector s behavior when session expires,  when the ZK session expires  it results in a fatal error being sent to the application callback  This is not the best behavior    for example  in the case of HA  if ZK goes down  we would like the current state to be maintained  rather than causing either NN to abort  When the ZK clients are able to reconnect  they should sort out the correct leader based on the normal locking schemes ,
HADOOP-13142,  project version from 3 0 0 to 3 0 0 alpha1,  want to rename 3 0 0 to 3 0 0 alpha1 for the first alpha release  However  the version number is also encoded outside of the pom xml s  so we need to update these too ,
HADOOP-12249,  argument parsing into a function,  order to enable significantly better unit testing as well as enhanced functionality  large portions of   config sh should be pulled into functions   See first comment for more ,
HADOOP-11970,  uses of ThreadLocal Random  with JDK7 ThreadLocalRandom,ThreadLocalRandom should be used when available in place of ThreadLocal Random   For JDK7 the difference is minimal  but JDK8 starts including optimizations for ThreadLocalRandom ,
HADOOP-2921,  map splits on sorted files with key boundaries, this is something that we have implemented in the application layer   may be useful to have in hadoop itself    long term log storage systems often keep data sorted  by some sort key   future computations on such files can often benefit from this sort order  if the job requires grouping by the sort key   then it should be possible to do reduction in the map stage itself   this is not natively supported by hadoop  except in the degenerate case of 1 map file per task  since splits can span the sort key  however aligning the data read by the map task  to sort key boundaries is straightforward   and this would be a useful capability to have in hadoop   the definition of the sort key should be left up to the application  it s not necessarily the key field in a Sequencefile  through a generic interface   but otherwise   the sequencefile and text file readers can use the extracted sort key to align map task data with key boundaries ,
HADOOP-12334,  Mode Of Copy Operation of HBase WAL Archiving to bypass Azure Storage Throttling after retries,  11693 mitigated the problem of HMaster aborting regionserver due to Azure Storage Throttling event during HBase WAL archival  The way this was achieved was by applying an intensive exponential retry when throttling occurred   As a second level of mitigation we will change the mode of copy operation if the operation fails even after all retries  i e  we will do a client side copy of the blob and then copy it back to destination  This operation will not be subject to throttling and hence should provide a stronger mitigation  However it is more expensive  hence we do it only in the case we fail after all retries,
HADOOP-7323,  capability to resolve compression codec based on codec name,  setting up a compression codec in an MR job the full class name of the codec must be used   To ease usability  compression codecs should be resolved by their codec name  ie  gzip    deflate    zlib    bzip2   instead their full codec class name   Besides easy of use for Hadoop users who would use the codec alias instead the full codec class name  it could simplify how HBase resolves loads the codecs ,
HADOOP-7346,  back nicer error to clients using outdated IPC version,  an older Hadoop version tries to contact a newer Hadoop version across an IPC protocol version bump  the client currently just gets a non useful error message like  EOFException    Instead  the IPC server code can speak just enough of prior IPC protocols to send back a  fatal  message indicating the version mismatch ,
HADOOP-10048,LocalDirAllocator should avoid holding locks while accessing the filesystem,  noted in MAPREDUCE 5584 and HADOOP 7016  LocalDirAllocator can be a bottleneck for multithreaded setups like the ShuffleHandler   We should consider moving to a lockless design or minimizing the critical sections to a very small amount of time that does not involve I O operations ,
HADOOP-13994,  declare the commons lang3 dependency as 3 4,  people aren t seeing this  yet    but unless you explicitly exclude v 3 4 of commons lang3 from the azure build  which HADOOP 13660 does   then the dependency declaration of commons lang3 v 3 3 2 is creating a resolution conflict  That s a dependency only needed for the local dynamodb   tests   I propose to fix this in s3guard by explicitly declaring the version used in the tests to be that of the azure storage one  excluding that you get for free  It doesn t impact anything shipped in production  but puts the hadoop build in control of what versions of commons lang are coming in everywhere by way of the commons config version declared in hadoop common,
HADOOP-7347,  Wire Compatibility,,
HADOOP-9018,  invalid Windows URIs,  JIRA is to make handling of improperly constructed file URIs for Windows local paths more rigorous  e g  reject  file    c   Windows    Valid file URI syntax explained at http   blogs msdn com b ie archive 2006 12 06 file uris in windows aspx   Also see https   issues apache org jira browse HADOOP 8953,
HADOOP-11554,  HadoopKerberosName as a hadoop subcommand,HadoopKerberosName has been around as a  secret hack  for quite a while   We should clean up the output and make it official by exposing it via the hadoop command ,
HADOOP-11460,  shell vars,  is a very common shell pattern in 3 x to effectively replace sub project specific vars with generics   We should have a function that does this replacement and provides a warning to the end user that the old shell var is deprecated   Additionally  we should use this shell function to deprecate the shell vars that are holdovers already  ,
HADOOP-2786,  hbase out of hadoop core,  hbase out of hadoop core   Move its JIRA issues and move it in svn from https   svn apache org repos asf hadoop core trunk src contrib hbase to https   svn apache org repos asf hadoop hbase trunk,
HADOOP-12384,    direct  flag option for fs copy so that user can choose not to create    COPYING   file,  CLI is using CommandWithDestination java which add    COPYING   to the tail of file name when it does the copy  For blobstore like S3 and Swift  to create    COPYING   file and rename it is expensive    direct  flag can allow user to avoiding the    COPYING   file ,
HADOOP-14003,  additional KMS tomcat settings configurable,  some Tomcat performance tuning on a loaded cluster  we found that   acceptCount      acceptorThreadCount    and   protocol   can be useful  Let s make these configurable in the kms startup script   Since the KMS is Jetty in 3 x  this is targeted at just branch 2 ,
HADOOP-7571,  config sh needs to be updated post mavenization,  common src main bin hadoop config sh needs to be updated post mavenization  eg it still refers to build classes etc  ,
HADOOP-13175,  hadoop ant from hadoop tools,  hadoop ant code is an ancient kludge unlikely to have any users  still  We can delete it from trunk as a  scream test  for 3 x ,
HADOOP-12563,  utility to create modify token files,  fetchdt is missing some critical features and is geared almost exclusively towards HDFS operations   Additionally  the token files that are created use Java serializations which are hard impossible to deal with in other languages  It should be replaced with a better utility in common that can read write protobuf based token files  has enough flexibility to be used with other services  and offers key functionality such as append and rename  The old version file format should still be supported for backward compatibility  but will be effectively deprecated   A follow on JIRA will deprecrate fetchdt ,
HADOOP-13953,  FTPFileSystem s data connection mode and transfer mode configurable,  FTP transfer mode used by FTPFileSystem is BLOCK TRANSFER MODE  FTP Data connection mode used by FTPFileSystem is ACTIVE LOCAL DATA CONNECTION MODE  This jira makes them configurable ,
HADOOP-13827,  reencryptEncryptedKey interface to KMS,  is the KMS part  Please refer to HDFS 10899 for the design doc ,
HADOOP-10035,  TestFilterFileSystem,  TestFilterFileSystem only checks for FileSystem methods that must be implemented in FilterFileSystem with a list of methods that are exception to this rule  This jira wants to make this check stricter by adding a test for ensuring the methods in exception rule list must not be implemented by the FilterFileSystem   This also cleans up the current class that has methods from exception rule list to interface to avoid having to provide dummy implementation of the methods ,
HADOOP-5988,  a command to   FsShell stat   to get a file s block location information,  an option to   FsShell stat   to get a file s block location information will be very useful  we can print the block location information in this format  blockID XXXXX  byte range YYYY ZZZZ  location dn1 dn2  blockID XXXXX  byte range YYYY ZZZZ  location dn1 dn2 ,
HADOOP-7775,  Layer improvements to support protocol compatibility,,
HADOOP-13193,  to Apache Yetus 0 3 0,  yetus wrapper to be 0 3 0 now that it has passed vote ,
HADOOP-13702,  a new instrumented read write lock,  a new instrumented read write lock in hadoop common  so that the HDFS 9668 can use this to improve the locking in FsDatasetImpl,
HADOOP-11470,  some uses of obsolete guava APIs from the hadoop codebase,  the same vein as HADOOP 11286  there are now several remaining usages of guava APIs that are now incompatible with a more recent version  e g  16    This JIRA proposes eliminating those usages  With this  the hadoop code base should run compile cleanly even if guava 16 is used for example   This JIRA doesn t propose upgrading the guava dependency version however  just making the codebase compatible with guava 16   ,
HADOOP-12321,  JvmPauseMonitor an AbstractService,  new JVM pause monitor has been written with its own start stop lifecycle which has already proven brittle to both ordering of operations and  even after HADOOP 12313  is not thread safe  both start and stop are potentially re entrant    It also requires every class which supports the monitor to add another field and perform the lifecycle operations in its own lifecycle  which  for all Yarn services  is the YARN app lifecycle  as implemented in Hadoop common   Making the  monitor a subclass of   AbstractService   and moving the init start   stop operations in   serviceInit        serviceStart         serviceStop     methods will fix the concurrency and state model issues  and make it trivial to add as a child to any YARN service which subclasses   CompositeService    most the NM and RM apps  will be able to hook up the monitor simply by creating one in the ctor and adding it as a child ,
HADOOP-13263,  cached groups in background after expiry,  HADOOP 11238 the Guava cache was introduced to allow refreshes on the Namenode group cache to run in the background  avoiding many slow group lookups  Even with this change  I have seen quite a few clusters with issues due to slow group lookups  The problem is most prevalent in HA clusters  where a slow group lookup on the hdfs user can fail to return for over 45 seconds causing the Failover Controller to kill it   The way the current Guava cache implementation works is approximately   1  On initial load  the first thread to request groups for a given user blocks until it returns  Any subsequent threads requesting that user block until that first thread populates the cache   2  When the key expires  the first thread to hit the cache after expiry blocks  While it is blocked  other threads will return the old value   I feel it is this blocking thread that still gives the Namenode issues on slow group lookups  If the call from the FC is the one that blocks and lookups are slow  if can cause the NN to be killed   Guava has the ability to refresh expired keys completely in the background  where the first thread that hits an expired key schedules a background cache reload  but still returns the old value  Then the cache is eventually updated  This patch introduces this background reload feature  There are two new parameters   1  hadoop security groups cache background reload   default false to keep the current behaviour  Set to true to enable a small thread pool and background refresh for expired keys  2  hadoop security groups cache background reload threads   only relevant if the above is set to true  Controls how many threads are in the background refresh pool  Default is 1  which is likely to be enough ,
HADOOP-12850,  shell code out of hadoop dist,  s pull the shell code out of the hadoop dist pom xml,
HADOOP-7710,  a script to setup application in order to create root directories for application such hbase  hcat  hive etc,,
HADOOP-13687,  a unified dependency artifact that transitively includes the cloud storage modules shipped with Hadoop ,   downstream projects that want to integrate with different Hadoop compatible file systems like WASB and S3A need to list dependencies on each one   This creates an ongoing maintenance burden for those projects  because they need to update their build whenever a new Hadoop compatible file system is introduced   This issue proposes adding a new artifact that transitively includes all Hadoop compatible file systems   Similar to hadoop client  this new artifact will consist of just a pom xml listing the individual dependencies   Downstream users can depend on this artifact to sweep in everything  and picking up a new file system in a future version will be just a matter of updating the Hadoop dependency version ,
HADOOP-8124,  the deprecated Syncable sync   method,  Syncable sync   was deprecated in 0 21   We should remove it ,
HADOOP-11590,  sbin commands and documentation to use new   slaves option,  HADOOP 11565 now committed  we need to remove usages of yarn daemons sh and hadoop daemons sh from the start and stop scripts  converting them to use the new   slaves option   Additionally  the documentation should be updated to reflect these new command options ,
HADOOP-13457,  hardcoded absolute path for shell executable,  java has a hardcoded path to  bin bash which is not correct on all platforms    Pointed out by   aw  while reviewing HADOOP 13434 ,
HADOOP-11226,  a configuration to set ipc Client s traffic class with IPTOS LOWDELAY IPTOS RELIABILITY,  heavy shuffle  packet loss for IPC packets was observed from a machine   Avoid packet loss and speed up transfer by using 0x14 QOS bits for the packets ,
HADOOP-5271,  option for minimum progress threshold before reducers are assigned,  specific sub case of the general priority inversion problem noted in HADOOP 4557 is when many lower priority jobs are submitted and are waiting for mappers to free up   Even though they haven t actually done any work  they will be assigned any free reducers   If a higher priority job is submitted  priority inversion results not just due to the lower priority tasks that are in the midst of completing  but also due to the ones that haven t yet started but have claimed all the free reducers   A simple workaround is to require a job to complete some useful work before assigning it a reducer   This can be done in a tunable and backwards compatible manner by adding a  minimum map progress percentage before assigning a reducer  option to the JobConf   Setting this to 0 would eliminate the common case above  and setting it to 100 would technically eliminate the inversion of HADOOP 4557  though likely at an unacceptably high cost ,
HADOOP-12950,ShutdownHookManager should have a timeout for each of the Registered shutdown hook,  8325 added a ShutdownHookManager to be used by different components instead of the JVM shutdownhook  For each of the shutdown hook registered  we currently don t have an upper bound for its execution time  We have seen namenode failed to shutdown completely  waiting for shutdown hook to finish after failover  for a long period of time  which breaks the namenode high availability scenarios  This ticket is opened to allow specifying a timeout value for the registered shutdown hook  ,
HADOOP-6728,  metrics framework,  discussions with Arun  Chris  Hong and Rajiv  et al  we concluded that the current metrics framework needs an overhaul to     Allow multiple plugins for different monitoring systems simultaneously  see also  HADOOP 6508     Refresh metrics plugin config without server restart     Including filtering of metrics per plugin    Support metrics schema for plugins   The jira will be resolved when core hadoop components  hdfs  mapreduce  are updated to use the new framework   Updates to external components that use the existing metrics framework will be tracked by different issues    The current design wiki http   wiki apache org hadoop HADOOP 6728 MetricsV2  ,
HADOOP-3876,  Core should support source filesfor multiple schedulers,  the default JT scheduling algorithm  there is work going on with at least two more schedulers  HADOOP 3445  HADOOP 3746   HADOOP 3412 makes it easier to plug in new schedulers into the JT  Where do we place the source files for various schedulers so that it s easy for users to choose their scheduler of choice during deployment  and easy for developers to add in more schedulers into the framework  without inundating it   ,
HADOOP-12747,  wildcard in libjars argument,  is a problem when a user job adds too many dependency jars in their command line  The HADOOP CLASSPATH part can be addressed  including using wildcards       But the same cannot be done with the  libjars argument  Today it takes only fully specified file paths   We may want to consider supporting wildcards as a way to help users in this situation  The idea is to handle it the same way the JVM does it     expands to the list of jars in that directory  It does not traverse into any child directory   Also  it probably would be a good idea to do it only for libjars  i e  don t do it for  files and  archives  ,
HADOOP-6418,  should have mkdir and create file apis which do not create parent path ,  should have mkdir and create file apis which do not create parent path  The usecase is illustrated at  this https   issues apache org jira browse HDFS 98 focusedCommentId 12595989 page com atlassian jira plugin system issuetabpanels 3Acomment tabpanel action 12595989 ,
HADOOP-11661,  FileUtil copyMerge, FileUtil copyMerge is currently unused in the Hadoop source tree  In branch 1  it had been part of the implementation of the hadoop fs  getmerge shell command  In branch 2  the code for that shell command was rewritten in a way that no longer requires this method   Please check more details here    https   issues apache org jira browse HADOOP 11392 focusedCommentId 14339336 page com atlassian jira plugin system issuetabpanels comment tabpanel comment 14339336,
HADOOP-13354,  WASB driver to use the latest version  4 2 0  of SDK for Microsoft Azure Storage Clients,  WASB driver to use the latest version  4 2 0  of SDK for Microsoft Azure Storage Clients  We are currently using version 2 2 0 of the SDK  Version 4 2 0 brings some breaking changes   Need to fix code to resolve all these breaking changes and certify that everything works properly ,
HADOOP-1849,  server max queue size should be configurable,  max queue size for IPC server is set to  100   handlers   Usually when RPC failures are observed  e g  HADOOP 1763   we increase number of handlers and the problem goes away  I think a big part of such a fix is increase in max queue size  I think we should make maxQsize per handler configurable  with a bigger default than 100   There are other improvements also  HADOOP 1841    Server keeps reading RPC requests from clients  When the number in flight RPCs is larger than maxQsize  the earliest RPCs are deleted  This is the main feedback Server has for the client  I have often heard from users that Hadoop doesn t handle bursty traffic   Say handler count is 10  default  and Server can handle 1000 RPCs a sec  quite conservative low for a typical server   it implies that an RPC can wait for only for 1 sec before it is dropped  If there 3000 clients and all of them send RPCs around the same time  not very rare  with heartbeats etc   2000 will be dropped  In stead of dropping the earliest RPCs  if the server delays reading new RPCs  the feedback to clients would be much smoother  I will file another jira regd queue management   For this jira I propose to make queue size per handler configurable  with a larger default  may be 500   ,
HADOOP-12050,  MaxInactiveInterval for hadoop http auth token,  http authentication  a cookie which contains the authentication token is dropped  The expiry time of the authentication token can be configured via hadoop http authentication token validity  The default value is 10 hours   For clusters which require enhanced security  it is desirable to have a configurable MaxInActiveInterval for the authentication token  If there is no activity during MaxInActiveInterval  the authentication token will be invalidated    The MaxInActiveInterval will be less than hadoop http authentication token validity  The default value will be 30 minutes  ,
HADOOP-3315,  binary file format,SequenceFile s block compression format is too complex and requires 4 codecs to compress or decompress  It would be good to have a file format that only needs ,
HADOOP-13227,AsyncCallHandler should use an event driven architecture to handle async calls,  JIRA is to address  Jing s comments https   issues apache org jira browse HADOOP 13226 focusedCommentId 15308630 page com atlassian jira plugin system issuetabpanels comment tabpanel comment 15308630  in HADOOP 13226 ,
HADOOP-11827,  up distcp buildListing   using threadpool,  very large source trees on s3 distcp is taking long time to build file listing  client code  before starting mappers   For a dataset I used  1 5M files  50K dirs  it was taking 65 minutes before my fix in HADOOP 11785 and 36 minutes after the fix    ,
HADOOP-5962,  tests should not be placed in hdfs ,  following tests are under the org apache hadoop fs package but were moved to hdfs sub directory by HADOOP 5135   noformat    org apache hadoop fs ftp TestFTPFileSystem java   org apache hadoop fs loadGenerator TestLoadGenerator java   org apache hadoop fs permission TestStickyBit java   org apache hadoop fs TestGlobPaths java   org apache hadoop fs TestUrlStreamHandler java  noformat    Some of them are not related to hdfs  e g  TestFTPFileSystem  These files should be moved out from hdfs and should not use hdfs codes    Some of them are testing hdfs features  e g  TestStickyBit  They should be defined under org apache hadoop hdfs package  ,
HADOOP-8847,  untar to use Java API on Windows instead of spawning tar process,  FileUtil unTar   spawns tar utility to do the work  Tar may not be present on all platforms by default eg  Windows  So changing this to use JAVA API s would help make it more cross platform  FileUtil unZip   uses the same approach ,
HADOOP-4460,  Chukwa Demux process,  simplify Parsers implementation   add map and reduce side to the demux   add dynamic link between RecordType and Parsers using configuration file and alias   encapsulate data files creation location and naming convention inside the core demux classes   sort all data by TimePartition Machine Timestamp by default ,
HADOOP-6526,  mapping from long principal names to local OS user names,  need a configurable mapping from full user names  eg  omalley APACHE ORG  to local user names  eg  omalley   For many organizations it is sufficient to just use the prefix  however  in the case of shared clusters there may be duplicated prefixes  A configurable mapping will let administrators resolve the issue ,
HADOOP-12864,  bin rcc script,  o a h record was moved  bin rcc was never updated to pull those classes from the streaming jar ,
HADOOP-13341,  HADOOP SERVERNAME OPTS  replace with  command   subcommand  OPTS,  features like YARN 2928 demonstrate that even senior level Hadoop developers forget that daemons need a custom  OPTS env var   We can replace all of the custom vars with generic handling just like we do for the username check   For example  with generic handling in place      Old Var    New Var      HADOOP NAMENODE OPTS   HDFS NAMENODE OPTS     YARN RESOURCEMANAGER OPTS   YARN RESOURCEMANAGER OPTS     n a   YARN TIMELINEREADER OPTS     n a   HADOOP DISTCP OPTS     n a   MAPRED DISTCP OPTS     HADOOP DN SECURE EXTRA OPTS   HDFS DATANODE SECURE EXTRA OPTS     HADOOP NFS3 SECURE EXTRA OPTS   HDFS NFS3 SECURE EXTRA OPTS     HADOOP JOB HISTORYSERVER OPTS   MAPRED HISTORYSERVER OPTS     This makes it   a  consistent across the entire project b  consistent for every subcommand c  eliminates almost all of the custom appending in the case statements  It s worth pointing out that subcommands like distcp that sometimes need a higher than normal client side heapsize or custom options are a huge win   Combined with  hadooprc and or dynamic subcommands  it means users can easily do customizations based upon their needs without a lot of weirdo shell aliasing or one line shell scripts off to the side  ,
HADOOP-4462,  mechanism for demux output,  order to reduce the number of file on HDFS we need to have a rolling mechanism for the demux output   avoid immediate merging if there s already file for the same time range  create a spill file instead   merge all raw files every hours   merge all hourly files every days ,
HADOOP-13812,  Tomcat to 6 0 48,  and HttpFS currently uses Tomcat 6 0 44  propose to upgrade to the latest version is 6 0 48  ,
HADOOP-12811,  kms server port number which conflicts with HMaster port number,  HBase s HMaster port number conflicts with Hadoop kms port number  Both uses 16000   There might be use cases user need kms and HBase present on the same cluster  The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories   Users would have to manually override the default port of either application on their cluster  It would be nice to have different default ports so kms and HBase could naturally coexist   ,
HADOOP-12817,  TLS v1 1 and 1 2,  7 supports TLSv1 1 and TLSv1 2  which are more secure than TLSv1  which was all that was supported in Java 6   so we should add those to the default list for   hadoop ssl enabled protocols   ,
HADOOP-12672,  timeout should not override IPC ping interval,  if the value of ipc client rpc timeout ms is greater than 0  the timeout overrides the ipc ping interval and client will throw exception instead of sending ping when the interval is passed  RPC timeout should work without effectively disabling IPC ping ,
HADOOP-6918,  metrics naming consistent,  working HADOOP 6728  I noticed that our metrics naming style is all over the place     Capitalized camel case  e g    FilesCreated  in namenode metrics and some rpc metrics   uncapitalized camel case  e g   threadsBlocked  in jvm metrics and some rpc metrics   lowercased underscored  e g    bytes written  in datanode metrics and mapreduce metrics  Let s make them consistent  How about uncapitalized camel case  My main reason for the camel case  some backends have limits on the name length and underscore is wasteful   Once we have a consistent naming style we can do   Metric  Number of INodes created   MutableCounterLong filesCreated   instead of the more redundant   Metric   FilesCreated    Number of INodes created    MutableCounterLong filesCreated ,
HADOOP-13465,  Server Call to be extensible for unified call queue,  RPC layer supports QoS but other protocols  ex  webhdfs  are completely unconstrained   Generalizing   Server Call   to be extensible with simple changes to the handlers will enable unifying the call queue for multiple protocols ,
HADOOP-4461,  Chukwa parsers,  Update Chukwa parsers,
HADOOP-11596,  smart apply patch sh to add new files in binary git patches,  a new file is added  the source is  dev null  rather than the root of the tree  which would mean a a b prefix   Allow for this,
HADOOP-7570,  config sh needs to be updated post MR2,  common src main bin hadoop config sh needs to be updated post MR2  eg the layout of mapred home has changed  ,
HADOOP-1899,  metrics system in the job tracker is running too often,  metrics system in the JobTracker is defaulting to every 5 seconds computing all of the counters for all of the jobs  This work is a substantial amount of work showing up as running in 20  of the snapshots that I ve seen  I d like to lower the default interval to once every 60 seconds and make it a low priority thread ,
HADOOP-13900,  snapshot version of SDK dependency from Azure Data Lake Store File System,  Data Lake Store File System dependent Azure Data Lake Store SDK is released and has not need for further snapshot version dependency  This JIRA removes the SDK snapshot dependency to released SDK candidate  There is not functional change in the SDK and no impact to live contract test  ,
HADOOP-13337,  maven enforcer plugin version to 1 4 1,,
HADOOP-11485,  shell integration,  would be useful to provide a way for core and non core Hadoop components to plug into the shell infrastructure   This would allow us to pull the HDFS  MapReduce  and YARN shell functions out of hadoop functions sh   Additionally  it should let 3rd parties such as HBase influence things like classpaths at runtime ,
HADOOP-11687,  x   and response headers when copying an Amazon S3 object,  EMC ViPR ECS object storage platform uses proprietary headers starting by x emc    like Amazon does with x amz     Headers starting by x emc   should be included in the signature computation  but it s not done by the Amazon S3 Java SDK  it s done by the EMC S3 SDK    When s3a copy an object it copies all the headers  but when the object includes x emc   headers  it generates a signature mismatch   Removing the x emc   headers from the copy would allow s3a to be compatible with the EMC ViPR ECS object storage platform   Removing the x   which aren t x amz   headers from the copy would allow s3a to be compatible with any object storage platform which is using proprietary headers,
HADOOP-8521,  StreamInputFormat to new Map Reduce API,  of now   hadoop streaming uses old Hadoop M R API  This JIRA ports it to the new M R API ,
HADOOP-12859,  hiding field style checks in class setters,  discussed in mailing list  this will disable style checks in class setters like the following   noformat  void setBlockLocations LocatedBlocks blockLocations    42   blockLocations  hides a field  void setTimeout int timeout    25   timeout  hides a field  void setLocatedBlocks List LocatedBlock  locatedBlocks    46   locatedBlocks  hides a field  void setRemaining long remaining    28   remaining  hides a field  void setBytesPerCRC int bytesPerCRC    29   bytesPerCRC  hides a field  void setCrcType DataChecksum Type crcType    39   crcType  hides a field  void setCrcPerBlock long crcPerBlock    30   crcPerBlock  hides a field  void setRefetchBlocks boolean refetchBlocks    35   refetchBlocks  hides a field  void setLastRetriedIndex int lastRetriedIndex    34   lastRetriedIndex  hides a field   noformat ,
HADOOP-13688,  bundling HTML source code in javadoc JARs,  generate source code with line numbers for inclusion in the javadoc JARs  Given that there s github and other online viewers  this doesn t seem so useful these days   Disabling the  linkSource  option saves us 40MB for the hadoop common javadoc jar    noformat   rw r  r   1 andrew andrew 98M Oct  5 14 44 hadoop common 3 0 0 alpha2 SNAPSHOT javadoc jar  rw r  r   1 andrew andrew 58M Oct  5 15 00   hadoop common project hadoop common target hadoop common 3 0 0 alpha2 SNAPSHOT javadoc jar  noformat ,
HADOOP-516,  based GUI  DFS explorer and basic Map Reduce job launcher,  increase productivity in our current project  which makes a heavy use of Hadoop   we wrote a small Eclipse based GUI application which basically consists in 2 views         a HDFS explorer adapted from Eclipse filesystem explorer example       For now  it includes the following features           o classical tree based browsing interface  with directory            content being detailed in a 3 columns table  file name  file            size  file type           o refresh button          o delete file or directory  with confirm dialog   select files            in the tree or table and click the  Delete  button          o rename file or directory  simple click on the file in the            table  type the new name and validate          o open file with system editor  select the file in the table            and click  Open  button  works on Windows  not on Linux           o internal drag   drop          o external drag   drop from the local filesystem to the HDFS             the opposite doesn t work        a MapReduce  very  simple job launcher           o select the job XML configuration file          o run the job          o kill the job          o visualize map and reduce progress with progress bars          o open a browser on the Hadoop job tracker web interface   INSTALLATION NOTES     Eclipse 3 2    JDK 1 5    import the archive in Eclipse    copy your hadoop conf file  hadoop default xml in  src  folder     this step should be moved in the GUI later    right click on the project and Run As    Eclipse Application    enjoy    ,
HADOOP-4839,  Hadoop native library to java library path  compression ,,
HADOOP-11960,  Azure Storage Client Side logging ,AzureStorageExceptions currently are logged as part of the WAB code which often is not too informative  AzureStorage SDK supports client side logging that can be enabled that logs relevant information w r t request made from the Storage client    This JIRA is created to enable Azure Storage Client Side logging at the Job submission level  User should be able to configure Client Side logging on a Per Job bases  ,
HADOOP-12759,RollingFileSystemSink should eagerly rotate directories,  RollingFileSystemSink only rolls over to a new directory if a new metrics record comes in   The issue is that HDFS does not update the file size until it s closed  HDFS 5478   and if no new metrics record comes in  then the file size will never be updated   This JIRA is to add a background thread to the sink that will eagerly close the file at the top of the hour ,
HADOOP-13168,  Future get with timeout in ipc async calls,   the Future returned by ipc async call only support Future get   but not Future get timeout  unit    We should support the latter as well ,
HADOOP-13008,  XFS Filter for UIs to Hadoop Common,  Frame Scripting  XFS  prevention for UIs can be provided through a common servlet filter  This filter will set the X Frame Options HTTP header to DENY unless configured to another valid setting   There are a number of UIs that could just add this to their filters as well as the Yarn webapp proxy which could add it for all it s proxied UIs   if appropriate ,
HADOOP-5726,  pre emption from the capacity scheduler code base,  an effort to simplify the code base  we would like to remove the pre emption related code in the capacity scheduler  We would reintroduce this  possibly with some revisions to the original design  after a while  This will be an incompatible change  Any objections  ,
HADOOP-11496,ServiceLevelAuth still references hadoop dfsadmin mradmin,  should be hdfs dfsadmin and yarn rmadmin,
HADOOP-12909,  ipc Client to support asynchronous calls,  ipc Client  the underlying mechanism is already supporting asynchronous calls    the calls shares a connection  the call requests are sent using a thread pool and the responses can be out of order   Indeed  synchronous call is implemented by invoking wait   in the caller thread in order to wait for the server response   In this JIRA  we change ipc Client to support asynchronous mode   In asynchronous mode  it return once the request has been sent out but not wait for the response from the server ,
HADOOP-11565,    slaves shell option,  a   slaves shell option to hadoop config sh to trigger the given command on slave nodes   This is required to deprecate hadoop daemons sh and yarn daemons sh ,
HADOOP-12943,   w  r options in dfs  test command,  the dfs  test command only supports      d   e   f   s   z  options  It would be helpful if we add      w   r   to verify permission of r w before actual read or write  This will help script programming ,
HADOOP-13427,  needless uses of FileSystem  exists    isFile    isDirectory   ,  re cleaning up Hive and Spark s use of FileSystem exists  because it is often the case we see code of exists open  exists delete  when the exists probe is needless  Against object stores  expensive needless   Hadoop can set an example here by stripping them out  It will also show where there are opportunities to optimise things better and or improve reporting ,
HADOOP-12358,   safely flag to rm to prompt when deleting many files,  have seen many cases with customers deleting data inadvertently with  skipTrash  The FSShell should prompt user if the size of the data or the number of files being deleted is bigger than a threshold even though  skipTrash is being used ,
HADOOP-2864,  the Scalability and Robustness of IPC,  jira is intended to enhance IPC s scalability and robustness    Currently an IPC server can easily hung due to a disk failure or garbage collection  during which it cannot respond to the clients promptly  This has caused a lot of dropped calls and delayed responses thus many running applications fail on timeout  On the other side if busy clients send a lot of requests to the server in a short period of time or too many clients communicate with the server simultaneously  the server may be swarmed by requests and cannot work responsively    The proposed changes aim to    provide a better client server coordination    Server should be able to throttle client during burst of requests     A slow client should not affect server from serving other clients     A temporary hanging server should not cause catastrophic failures to clients    Client server should detect remote side failures  Examples of failures include   1  the remote host is crashed   2  the remote host is crashed and then rebooted   3  the remote process is crashed or shut down by an operator    Fairness  Each client should be able to make progress  ,
HADOOP-5029,  utilities to load chukwa sequence file to database,  data need to be reprocessed in the database  there is currently no manual method to reload the chukwa sequence files into database  A few minor tweaks to MetricsDataLoader should be possible to create a command line utility to do this ,
HADOOP-13522,   A and  a formats for fs  stat command to print permissions,  patch adds to fs shell Stat java the missing options of  a and  A    FileStatus already contains the getPermission   method required for returning symbolic permissions  FsPermission contains the method to return the binary short  but nothing to present in standard Octal format    Most UNIX admins base their work on such standard octal permissions  Hence  this patch also introduces one tiny method to translate the toShort   return into octal   Build has already passed unit tests and javadoc ,
HADOOP-12758,  CSRF Filter with UserAgent Checks,  protect against CSRF attacks  HADOOP 12691 introduces a CSRF filter that will require a specific HTTP header to be sent with every REST API call  This will affect all API consumers from web apps to CLIs and curl    Since CSRF is primarily a browser based attack we can try and minimize the impact on non browser clients   This enhancement will provide additional configuration for identifying non browser useragents and skipping the enforcement of the header requirement for anything identified as a non browser  This will largely limit the impact to browser based PUT and POST calls when configured appropriately ,
HADOOP-13209,  slaves with workers,  sh and the slaves file should get replace with workers sh and a workers file ,
HADOOP-8357,  security in Hadoop 0 22 branch,  is to track changes for restoring security in 0 22 branch ,
HADOOP-10075,  jetty dependency to version 9,  is no longer maintained   Update the dependency to jetty9 ,
HADOOP-1381,  distance between sync blocks in SequenceFiles should be configurable,  SequenceFiles put in sync blocks every 2000 bytes  It would be much better if it was configurable with a much higher default  1mb or so   ,
HADOOP-6786,  patch needs to verify Herriot integrity,  a new patch is submitted for verification   test patch   process has to make sure that none of Herriot bindings were broken ,
HADOOP-14004,  hadoop cloud storage project module in pom xml, code     module hadoop cloud storage project  module   code  is missing in pom xml  That way   mvn versions set   does not work for the project ,
HADOOP-9837,  Token Command,  JIRA is to define commands for Hadoop token  The scope of this task is highlighted as following    	Token init  authenticate and request an identity token  then persist the token in token cache for later reuse   	Token display  show the existing token with its info and attributes in the token cache   	Token revoke  revoke a token so that the token will no longer be valid and cannot be used later   	Token renew  extend the lifecycle of a token before it s expired ,
HADOOP-12916,  RPC scheduler callqueue backoff using response times,  back off policy from HADOOP 10597 is hard coded to base on whether call queue is full  This ticket is open to allow flexible back off policies such as moving average of response time in RPC calls of different priorities    ,
HADOOP-13425,  layer optimizations,  jira for y  optimizations to reduce object allocations  more efficiently use protobuf APIs  unified ipc and webhdfs callq to enable QoS  etc ,
HADOOP-8125,  hadoop client set of curated jars available in a distribution tarball,  thing that the original patch for HADOOP 8082 didn t address is the need for those curated jars to be visible in the final tarball ,
HADOOP-9091,  daemon startup when at least 1  or configurable  disk is in an OK state ,  given example is if datanode disk definitions but should be applicable to all configuration where a list of disks are provided   I have defined multiple local disks defined for a datanode   property   name dfs data dir  name   value  data 01 dfs dn  data 02 dfs dn  data 03 dfs dn  data 04 dfs dn  data 05 dfs dn  data 06 dfs dn  value   final true  final    property   When one of those disks breaks and is unmounted then the mountpoint  such as  data 03 in this example  becomes a regular directory which doesn t have the valid permissions and possible directory structure Hadoop is expecting  When this situation happens  the datanode fails to restart because of this while actually we have enough disks in an OK state to proceed   The only way around this is to alter the configuration and omit that specific disk configuration   To my opinion  It would be more practical to let Hadoop daemons start when at least 1 disks partition in the provided list is in a usable state   This prevents having to roll out custom configurations for systems which have temporarily a disk  and therefor directory layout  missing   This might also be configurable that at least X partitions out of he available ones are in OK state       ,
HADOOP-13348,  spurious  master  branch,  now the git repo has a branch named  master  in addition to our  trunk  branch  Since  master  is the common place name of the  most recent  branch in git repositories  this is misleading to new folks   It looks like the branch is from  11 months ago  We should remove it ,
HADOOP-2154,  interleaved checksums would optimize block transfers ,  when a block is transfered to a data node the client interleaves data chunks with the respective checksums   This requires creating an extra copy of the original data in a new buffer interleaved with the crcs  We can avoid extra copying if the data and the crc are fed to the socket one after another ,
HADOOP-11627,  io native lib available,  to the discussion in HADOOP 8642  we should remove   io native lib available   from trunk  and always use native libraries if they exist ,
HADOOP-10485,  dead classes in hadoop streaming,  streaming no longer requires many classes in o a h record  This jira removes the dead code ,
HADOOP-12541,  re2j dependency consistent,  the re2j dependency consistent with other parts of Hadoop   Seeing some weird rare failures with older versions of maven that appear to be related to this ,
HADOOP-8078,  capability to turn on security in unit tests ,  should be able to start a kdc server for unit tests  so that security could be turned on  This will greatly improve the coverage of unit tests ,
HADOOP-12967,  FileUtil copyMerge,  per  comment https   issues apache org jira browse HADOOP 11661 focusedCommentId 15083887 page com atlassian jira plugin system issuetabpanels comment tabpanel comment 15083887  from    wheat9  in HADOOP 11661  Need to remove FileUtil copyMerge   CC  to   wheat9 ,
HADOOP-13065,  a new interface for retrieving FS and FC Statistics,  FileSystem Statistics exposes the following statistics  BytesRead BytesWritten ReadOps LargeReadOps WriteOps  These are in turn exposed as job counters by MapReduce and other frameworks  There is logic within DfsClient to map operations to these counters that can be confusing  for instance  mkdirs counts as a writeOp   Proposed enhancement  Add a statistic for each DfsClient operation including create  append  createSymlink  delete  exists  mkdirs  rename and expose them as new properties on the Statistics object  The operation specific counters can be used for analyzing the load imposed by a particular job on HDFS   For example  we can use them to identify jobs that end up creating a large number of files   Once this information is available in the Statistics object  the app frameworks like MapReduce can expose them as additional counters to be aggregated and recorded as part of job summary ,
HADOOP-8215,  support for ZK Failover controller,  keep the initial patches manageable  kerberos security is not currently supported in the ZKFC implementation  This JIRA is to support the following important pieces for security    integrate with ZK authentication  kerberos or password based    allow the user to configure ACLs for the relevant znodes   add keytab configuration and login to the ZKFC daemons   ensure that the RPCs made by the health monitor and failover controller properly authenticate to the target daemons,
HADOOP-11948,  patch s issue matching regex should be configurable ,  have the issue matching regex configurable via altering the default  add in a cli arg to update it on invocation ,
HADOOP-6311,  support for unix domain sockets to JNI libs,  HDFS 347 we need to use unix domain sockets  This JIRA is to include a library in common which adds a o a h net unix package based on the code from Android  apache 2 license ,
HADOOP-4998,  a native OS runtime for Hadoop,  would be useful to implement a JNI based runtime for Hadoop to get access to the native OS runtime  This would allow us to stop relying on exec ing bash to get access to information such as user groups  process limits etc  and for features such as chown chgrp  org apache hadoop util Shell  ,
HADOOP-7352,  listStatus should throw IOE upon access error,  HADOOP 6201 and HDFS 538 it was agreed that FileSystem  listStatus should throw FileNotFoundException instead of returning null  when the target directory did not exist   However  in LocalFileSystem implementation today  FileSystem  listStatus still may return null  when the target directory exists but does not grant read permission   This causes NPE in many callers  for all the reasons cited in HADOOP 6201 and HDFS 538   See HADOOP 7327 and its linked issues for examples  ,
HADOOP-6999,  implementation for new FileSystem  FileContext  API,  file system API  HADOOP 4952  should implement security features currently provided by  FileSystem APIs This is a critical requirement for MapReduce components to  migrate and use new APIs for internal  filesystem operations  MAPREDUCE 2020 ,
HADOOP-12111, Umbrella  Split test patch off into its own TLP,  test patch s tendency to get forked into a variety of different projects  it makes a lot of sense to make an Apache TLP so that everyone can benefit from a common code base ,
HADOOP-12713,  spurious checkstyle checks,  of the checkstyle checks are not realistic  like the line length   leading to spurious  1 in precommit  Let s disable ,
HADOOP-12911,  Hadoop MiniKDC with Kerby,  discussed in the mailing list  we d like to introduce Apache Kerby into Hadoop  Initially it s good to start with upgrading Hadoop MiniKDC with Kerby offerings  Apache Kerby  https   github com apache directory kerby   as an Apache Directory sub project  is a Java Kerberos binding  It provides a SimpleKDC server that borrowed ideas from MiniKDC and implemented all the facilities existing in MiniKDC  Currently MiniKDC depends on the old Kerberos implementation in Directory Server project  but the implementation is stopped being maintained  Directory community has a plan to replace the implementation using Kerby  MiniKDC can use Kerby SimpleKDC directly to avoid depending on the full of Directory project  Kerby also provides nice identity backends such as the lightweight memory based one and the very simple json one for easy development and test environments ,
HADOOP-13537,  external calls in the RPC call queue,  HADOOP 13465 will allow non rpc calls to be added to the call queue   This is intended to support routing webhdfs calls through the call queue to provide a unified and protocol independent QoS ,
HADOOP-7328,  a serializer class is missing  return null  not throw an NPE ,  you have a key value class that s non Writable and you forget to attach io serializers for the same  an NPE is thrown by the tasks with no information on why or what s missing and what led to it  I think a better exception can be thrown by SerializationFactory instead of an NPE when a class is not found accepted by any of the loaded ones ,
HADOOP-3013,  to show  checksum  corrupted files,   only way to find  files with all replica being corrupt is when we read those files   Instead  can we have fsck  report those   Using the corrupted blocks found by the periodic verification       ,
HADOOP-12904,  Yetus to 0 2 0,  Yetus to 0 2 0,
HADOOP-11713,ViewFileSystem should support snapshot methods ,     ViewFileSystem   does not dispatch snapshot methods through the mount table   All snapshot methods throw   UnsupportedOperationException    even though the underlying mount points could be HDFS instances that support snapshots   We need to update   ViewFileSystem   to implement the snapshot methods ,
HADOOP-4334,  on top of TFile,     We need to have Object  Serialization Deserialization   support for TFile   ,
HADOOP-12504,  metrics v1,  HADOOP 7266  we should remove metrics v1 from trunk ,
HADOOP-10474,  o a h record to hadoop streaming,  classes in o a h record have been deprecated for more than a year and a half  They should be removed  As the first step  the jira moves all these classes into the hadoop streaming project  which is the only user of these classes ,
HADOOP-13863,   Add a new SAS key mode for WASB ,  implementation of WASB  only supports Azure storage keys and SAS key being provided via org apache hadoop conf Configuration  which results in these secrets residing in the same address space as the WASB process and providing complete access to the Azure storage account and its containers  Added to the fact that WASB does not inherently support ACL s  WASB is its current implementation cannot be securely used for environments like secure hadoop cluster  This JIRA is created to add a new mode in WASB  which operates on Azure Storage SAS keys  which can provide fine grained timed access to containers and blobs  providing a segway into supporting WASB for secure hadoop cluster   More details about the issue and the proposal are provided in the design proposal document ,
HADOOP-13578,  Codec for ZStandard Compression,   https   github com facebook zstd has been used in production for 6 months by facebook now   v1 0 was recently released   Create a codec for this library   ,
HADOOP-6581,  authenticated TokenIdentifiers to UGI so that they can be used for authorization,  token is used for authentication over RPC  information other than username may be needed for access authorization  This information is typically specified in TokenIdentifier  This is especially true for block tokens used for client to datanode accesses  where authorization is based on access permissions specified in TokenIdentifier  and not on username  Block tokens used to be called access tokens and one can think of them as capability tokens  See HADOOP 4359 for more info ,
HADOOP-12325,  Metrics   Add the ability track and log slow RPCs,  JIRA proposes to add a counter called RpcSlowCalls and also a configuration setting that allows users to log really slow RPCs   Slow RPCs are RPCs that fall at 99th percentile  This is useful to troubleshoot why certain services like name node freezes under heavy load  ,
HADOOP-13933,  haadmin  getAllServiceState option to get the HA state of all the NameNodes ResourceManagers,  we have one command to get state of namenode    code    hdfs haadmin  getServiceState  serviceId   code   It will be good to have command which will give state of all the namenodes ,
HADOOP-6894,  foundation for Hadoop client tools,  Hadoop widespreads and matures the number of tools and utilities for users keeps growing   Some of them are bundled with Hadoop core  some with Hadoop contrib  some on their own  some are full fledged servers on their own  For example  just to name a few  distcp  streaming  pipes  har  pig  hive  oozie   Today there is no standard mechanism for making these tools available to users  Neither there is a standard mechanism for these tools to integrate and distributed them with each other   The lack of a common foundation creates issues for developers and users  ,
HADOOP-11353,  support for  hadooprc,  system should be able to read in user defined env vars from    hadooprc ,
HADOOP-12180,  ResourceCalculatorPlugin from YARN to Common,  of the monitoring functions could be moved from YARN to Common for easier sharing,
HADOOP-4522,  Scheduler needs to re read its configuration,  external application  an Ops script  or some CLI based tool  can change the configuration of the Capacity Scheduler  change the capacities of various queues  for example  by updating its config file  This application then needs to tell the Capacity Scheduler that its config has changed  which causes the Scheduler to re read its configuration  It s possible that the Capacity Scheduler may need to interact with external applications in other similar ways  ,
HADOOP-13962,  ADLS SDK to 2 1 4,  has multiple upgrades since the version 2 0 11 we are using  2 1 1  2 1 2  and 2 1 4  Change list  https   github com Azure azure data lake store java blob master CHANGES md ,
HADOOP-12082,  multiple authentication schemes via AuthenticationFilter,  requirement is to support LDAP based authentication scheme via Hadoop AuthenticationFilter  HADOOP 9054 added a support to plug in custom authentication scheme  in addition to Kerberos  via AltKerberosAuthenticationHandler class  But it is based on selecting the authentication mechanism based on User Agent HTTP header which does not conform to HTTP protocol semantics   As per  RFC 2616 http   www w3 org Protocols rfc2616 rfc2616 html     HTTP protocol provides a simple challenge response authentication mechanism that can be used by a server to challenge a client request and by a client to provide the necessary authentication information     This mechanism is initiated by server sending the 401  Authenticate  response with  WWW Authenticate  header which includes at least one challenge that indicates the authentication scheme s  and parameters applicable to the Request URI     In case server supports multiple authentication schemes  it may return multiple challenges with a 401  Authenticate  response  and each challenge may use a different auth scheme     A user agent MUST choose to use the strongest auth scheme it understands and request credentials from the user based upon that challenge   The existing Hadoop authentication filter implementation supports Kerberos authentication scheme and uses  Negotiate  as the challenge as part of  WWW Authenticate  response header  As per the following documentation   Negotiate  challenge scheme is only applicable to Kerberos  and Windows NTLM  authentication schemes   SPNEGO based Kerberos and NTLM HTTP Authentication http   tools ietf org html rfc4559   Understanding HTTP Authentication https   msdn microsoft com en us library ms789031 28v vs 110 29 aspx   On the other hand for LDAP authentication  typically  Basic  authentication scheme is used  Note TLS is mandatory with Basic authentication scheme   http   httpd apache org docs trunk mod mod authnz ldap html  Hence for this feature  the idea would be to provide a custom implementation of Hadoop AuthenticationHandler and Authenticator interfaces which would support both schemes   Kerberos  via Negotiate auth challenge  and LDAP  via Basic auth challenge   During the authentication phase  it would send both the challenges and let client pick the appropriate one  If client responds with an  Authorization  header tagged with  Negotiate    it will use Kerberos authentication  If client responds with an  Authorization  header tagged with  Basic    it will use LDAP authentication   Note   some HTTP clients  e g  curl or Apache Http Java client  need to be configured to use one scheme over the other e g    curl tool supports option to use either Kerberos  via   negotiate flag  or username password based authentication  via   basic and  u flags      Apache HttpClient library can be configured to use specific authentication scheme  http   hc apache org httpcomponents client ga tutorial html authentication html  Typically web browsers automatically choose an authentication scheme based on a notion of  strength  of security  e g  take a look at the  design of Chrome browser for HTTP authentication https   www chromium org developers design documents http authentication     ,
HADOOP-13037,  Azure Data Lake Store as an independent FileSystem,  jira proposes an improvement over HADOOP 12666 to remove webhdfs dependencies from the ADL file system client and build out a standalone client  At a high level  this approach would extend the Hadoop file system class to provide an implementation for accessing Azure Data Lake  The scheme used for accessing the file system will continue to be   adl    accountname  azuredatalake net path to file    The Azure Data Lake Cloud Store will continue to provide a webHDFS rest interface  The client will  access the ADLS store using WebHDFS Rest APIs provided by the ADLS store    ,
HADOOP-13956,  ADLS credentials from Credential Provider,  ADLS credentials using Hadoop CredentialProvider API  See https   hadoop apache org docs current hadoop project dist hadoop common CredentialProviderAPI html ,
HADOOP-13661,  HTrace version,  re currently pulling in version 4 0 1 incubating   I think we should upgrade to the latest 4 1 0 incubating  ,
HADOOP-6423,  unit tests framework  Mockito ,  tests are functional tests or end to end  It makes sense to have Mockito framework for the convenience of true unit tests development   ,
HADOOP-11025,  daemons sh should just call hdfs directly,  is little to no reason for it to call hadoop daemon sh anymore ,
HADOOP-12520,  XInclude in hadoop azure test configuration to isolate Azure Storage account keys for service integration tests ,  hadoop azure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account   The configuration works by overwriting the src test resources azure test xml file   This can be an error prone process   The azure test xml file is checked into revision control to show an example   There is a risk that the tester could overwrite azure test xml containing the keys and then accidentally commit the keys to revision control   This would leak the keys to the world for potential use by an attacker   This issue proposes to use XInclude to isolate the keys into a separate file  ignored by git  which will never be committed to revision control   This is very similar to the setup already used by hadoop aws for integration testing ,
HADOOP-13705,  HADOOP 13534 Remove unused TrashPolicy getInstance and initialize code,  discussion on HADOOP 13700  I d like to revert HADOOP 13534  It removes a deprecated API  but the 2 x line does not have a release with the new replacement API  This places a burden on downstream applications ,
HADOOP-11939,  DistCpV1 and Logalyzer,  DistCpV1 and Logalyzer  which are no longer used  ,
HADOOP-7947,  XMLs if a relevant tool is available  when using scripts,  that we are locked down to using only XML for configuration and most of the administrators need to manage it by themselves  unless a tool that manages for you is used   it would be good to also validate the provided config XML    site xml  files with a tool like   xmllint   or maybe Xerces somehow  when running a command or  at least  when starting up daemons   We should use this only if a relevant tool is available  and optionally be silent if the env  requests ,
HADOOP-13466,  an AutoCloseableLock class,  an AutoCloseableLock class that is a thin wrapper over RentrantLock  It allows using RentrantLock with try with resources syntax   The wrapper functions perform no expensive operations in the lock acquire release path ,
HADOOP-5105,  optimize hudsonBuildHadoopNightly sh script,,
HADOOP-12626,  ISA L libraries should be added to the Dockerfile,  11887 added a compile and runtime dependence on the Intel ISA L library but didn t add it to the Dockerfile so that it could be part of the Docker based build environment  start build env sh    This needs to be fixed ,
HADOOP-3188,  utility for directories,  will collapse the contents of a directory into a small number of files  ,
HADOOP-12600,FileContext and AbstractFileSystem should be annotated as a Stable interface ,    FileContext   class currently is annotated as   Evolving     However  at this point we really need to treat it as a   Stable   interface ,
HADOOP-13396,  pluggable audit loggers in KMS,   KMS audit log is using log4j  to write a text format log  We should refactor this  so that people can easily add new format audit logs  The current text format log should be the default  and all of its behavior should remain compatible   ,
HADOOP-12055,  usage of NativeIO link,  our min version is now JDK7  there s hardlink support via   Files    This means we can deprecate the JNI implementation and discontinue usage ,
HADOOP-12857,  hadoop tools,  hadoop tools grows bigger and bigger  it s becoming evident that having a single directory that gets sucked in is starting to become a big burden as the number of tools grows   Let s rework this to be smarter ,
HADOOP-6282,  patch should verify that FindBugs version used for verification is correct one,  s a possibility that contributors and commiters might be checking their patches against a version of FindBugs which differs from one installed on Hudson   test patch script has to verify if the version of FindBugs is correct ,
HADOOP-13797,  hardcoded absolute path for ls,  java has a hardcoded path to  bin ls which is not correct on all platforms  eg  not on NixOS    see HADOOP 13457 for a similar issue ,
HADOOP-11843,  setting up the build environment easier,  As discussed with   aw    In AVRO 1537 a docker based solution was created to setup all the tools for doing a full build  This enables much easier reproduction of any issues and getting up and running for new developers   This issue is to  copy port  that setup into the hadoop project in preparation for the bug squash ,
HADOOP-7930,  relogin interval in UserGroupInformation should be configurable,  the check done in the  hasSufficientTimeElapsed    method is hardcoded to 10 mins wait   The wait time should be driven by configuration and its default value  for clients should be 1 min  ,
HADOOP-10814,  Tomcat version used by HttpFS and KMS to latest 6 x version,  and HttpFS are using Tomcat 6 0 37  we should move it to 6 0 41 to get bug fixes and security fixes   We should add a property with the tomcat version in the hadoop project POM and use that property from KMS and HttpFS   ,
HADOOP-13885,  getLinkTarget for ViewFileSystem,ViewFileSystem doesn t override FileSystem getLinkTarget    So  when view filesystem is used to resolve the symbolic links  the default FileSystem implementation throws UnsupportedOperationException   The proposal is to define getLinkTarget   for ViewFileSystem and invoke the target FileSystem for resolving the symbolic links  Path thus returned is preferred to be a viewfs qualified path  so that it can be used again on the ViewFileSystem handle  ,
HADOOP-13145,  DistCp  prevent unnecessary getFileStatus call when not preserving metadata ,  DistCp copies a file  it calls   getFileStatus   to get the   FileStatus   from the destination so that it can compare to the source and update metadata if necessary   If the DistCp command was run without the option to preserve metadata attributes  then this additional   getFileStatus   call is wasteful ,
HADOOP-10950,   heap management  vars,  HADOOP 9902  we need to rework how heap is configured for small footprint machines  deprecate some options  introduce new ones for greater flexibility ,
HADOOP-12064, JDK8  Update guice version to 4 0,  3 0 doesn t work with lambda statement  https   github com google guice issues 757  We should upgrade it to 4 0 which includes the fix ,
HADOOP-8101,  Control support for Non secure deployment of Hadoop on Windows,,
HADOOP-12702,  an HDFS metrics sink,  need a metrics2 sink that can write metrics to HDFS  The sink should accept as configuration a  directory prefix  and do the following in   putMetrics        Get yyyyMMddHH from current timestamp    If HDFS dir  dir prefix    yyyyMMddHH doesn t exist  create it  Close any currently open file and create a new file called  hostname  log in the new directory    Write metrics to the current log file ,
HADOOP-9136,  per server IPC configuration,  different IPC servers in Hadoop use the same config variables names starting with  ipc server   This makes it difficult and confusing to maintain configuration for different IPC servers ,
HADOOP-13742,   NumOpenConnectionsPerUser  as a metric,  track user level connections  How many connections for each user  in busy cluster where so many connections to server ,
HADOOP-601,  need some rpc retry framework,  need some mechanism for RPC calls that get exceptions to automatically retry the call under certain circumstances  In particular  we often end up with calls to rpcs being wrapped with retry loops for timeouts  We should be able to make a retrying proxy that will call the rpc and retry in some circumstances ,
HADOOP-6596,  add version to the serialization of DelegationToken,  that we are adding the serialized form of delegation tokens into the http interfaces  we should include some version information ,
HADOOP-11356,  deprecated o a h fs permission AccessControlException,    o a h fs permission AccessControlException   has been deprecated for last major releases and it should be removed ,
HADOOP-8619,WritableComparator must implement no arg constructor,  of reasons listed here  http   findbugs sourceforge net bugDescriptions html SE COMPARATOR SHOULD BE SERIALIZABLE  comparators should be serializable  To make deserialization work  it is required that all superclasses have no arg constructor  http   findbugs sourceforge net bugDescriptions html SE NO SUITABLE CONSTRUCTOR  Simply add no arg constructor to  WritableComparator ,
HADOOP-4901,  to JUnit 4,  other things  JUnit 4 has better support for class wide set up and tear down  via  BeforeClass and  AfterClass annotations   and more flexible assertions  http   junit sourceforge net doc ReleaseNotes4 4 html   It would be nice to be able to take advantage of these features in tests we write   JUnit 4 can run tests written for JUnit 3 8 1 without any changes ,
HADOOP-12756,Incorporate Aliyun OSS file system implementation,  OSS is widely used among China s cloud users  but currently it is not easy to access data laid on OSS storage from user s Hadoop Spark application  because of no original support for OSS in Hadoop   This work aims to integrate Aliyun OSS with Hadoop  By simple configuration  Spark Hadoop applications can read write data from OSS without any code change  Narrowing the gap between user s APP and data storage  like what have been done for S3 in Hadoop  ,
HADOOP-12291,  support for nested groups in LdapGroupsMapping,  using   LdapGroupsMapping   with Hadoop  nested groups are not supported  So for example if user   jdoe   is part of group A which is a member of group B  the group mapping currently returns only group A   Currently this facility is available with   ShellBasedUnixGroupsMapping   and SSSD  or similar tools  but would be good to have this feature as part of   LdapGroupsMapping   directly ,
HADOOP-10788,  kms to use new shell framework,  was not rewritten to use the new shell framework   It should be reworked to take advantage of it ,
HADOOP-13105,  timeouts in LDAP queries in LdapGroupsMapping ,  LdapGroupsMapping   currently does not set timeouts on the LDAP queries   This can create a risk of a very long infinite wait on a connection ,
HADOOP-11552,  handoff on the server side for RPC requests,  RPC server handler thread is tied up for each incoming RPC request  This isn t ideal  since this essentially implies that RPC operations should be short lived  and most operations which could take time end up falling back to a polling mechanism   Some use cases where this is useful    YARN submitApplication   which currently submits  followed by a poll to check if the application is accepted while the submit operation is written out to storage  This can be collapsed into a single call    YARN allocate   requests and allocations use the same protocol  New allocations are received via polling  The allocate protocol could be split into a request heartbeat along with a  awaitResponse   The request heartbeat is sent only when there s a request or on a much longer heartbeat interval  awaitResponse is always left active with the RM   and returns the moment something is available  MapReduce Tez task to AM communication is another example of this pattern   The same pattern of splitting calls can be used for other protocols as well  This should serve to improve latency  as well as reduce network traffic since the keep alive heartbeat can be sent less frequently   I believe there s some cases in HDFS as well  where the DN gets told to perform some operations when they heartbeat into the NN ,
HADOOP-13738,DiskChecker should perform some disk IO,DiskChecker can fail to detect total disk controller failures indefinitely  We have seen this in real clusters  DiskChecker performs simple permissions based checks on directories which do not guarantee that any disk IO will be attempted   A simple improvement is to write some data and flush it to the disk ,
HADOOP-4893,  the FileTailingAdaptor is unable to read a file it should take action instead of trying 300 times,   the FileTailingAdaptor  if when trying to read a file  a  File does not exist     Permission denied  exception is throws then that file should be log and  removed,
HADOOP-11858, JDK8  Set minimum version of Hadoop 3 to JDK 8,  minimum version of trunk to JDK 8,
HADOOP-6580,  should contain authentication method ,  UserGroupInformation should contain authentication method in its subject  This will be used in HDFS to issue delegation tokens only to kerberos authenticated clients ,
HADOOP-13382,  unneeded commons httpclient dependencies from POM files in Hadoop and sub projects,  branch 2 8 and later  the patches for various child and related bugs listed in HADOOP 10105  most recently including HADOOP 11613  HADOOP 12710  HADOOP 12711  HADOOP 12552  and HDFS 10623  eliminate all use of  commons httpclient  from Hadoop and its sub projects  except for hadoop tools hadoop openstack  see HADOOP 11614    However  after incorporating these patches   commons httpclient  is still listed as a dependency in these POM files    hadoop project pom xml   hadoop yarn project hadoop yarn hadoop yarn registry pom xml  We wish to remove these  but since commons httpclient is still used in many files in hadoop tools hadoop openstack  we ll need to  add  the dependency to   hadoop tools hadoop openstack pom xml  We ll add a note to HADOOP 11614 to undo this when commons httpclient is removed from hadoop openstack   In 2 8  this was mostly done by HADOOP 12552  but the version info formerly inherited from hadoop project pom xml also needs to be added  so that is in the branch 2 8 version of the patch   Other projects with undeclared transitive dependencies on commons httpclient  previously provided via hadoop common or hadoop client  may find this to be an incompatible change   Of course that also means such project is exposed to the commons httpclient CVE  and needs to be fixed for that reason as well  ,
HADOOP-12446,Undeprecate createNonRecursive  ,  createNonRecursive   is deprecated  However  there is no DistributedFileSystem create   implementation which throws exception if parent directory doesn t exist  This limits clients  migration away from the deprecated method   For HBase  IO fencing relies on the behavior of FileSystem createNonRecursive    Variant of create   method should be added which throws exception if parent directory doesn t exist ,
